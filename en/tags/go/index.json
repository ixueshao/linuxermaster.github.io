[{"content":"背景研究 分享一个通过 nginx 搭建一个静态资源的 web 服务器，实践中将会涉及到三个部分的内容：\n 静态文件服务器的配置 Nginx模块之gzip 自定义访问日志的格式: Embedded Variables  静态文件服务器 简单的就通过一个自定义的index.html文件当做一个静态页面Demo，下面准备一下静态文件资源存储的位置。\n1 2  mkdir /webserver/nginx18/ops/ echo \u0026#34;云原生生态圈\u0026#34; \u0026gt; /webserver/nginx18/ops/index.html   增加nginx配置文件，在/webserver/nginx18/conf/nginx.conf里增加一个 server 配置段，然后通过 alias 映射到虚拟目录。注意访问时 location 中的/对应的是 ops 后面的/\nserver { listen 80; server_name web.devopsman.cn; location / { alias ops/; } } 测试访问:\n☸️ k8sdev🔥 default ~  🐳 👉 curl web.devopsman.cn 静态网站demo ☸️ k8sdev🔥 default ~  🐳 👉 curl -I web.devopsman.cn HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Sat, 11 Jul 2020 14:11:46 GMT Content-Type: text/html Content-Length: 17 Last-Modified: Sat, 11 Jul 2020 14:11:13 GMT Connection: keep-alive ETag: \u0026quot;5f09c881-11\u0026quot; Accept-Ranges: bytes 开启静态网站压缩 nginx可以通过一种浏览器支持且压缩不受数据损失的gzip模块将样式文件、文本文件、图片等压缩之后在用户的请求之间传输，但是对于jpg、jpeg、png这类本身存在压缩性质上的图片来说，压缩效果不是很明显，主要体现在文本文件的压缩上，因此对于服务的提供者来说，减少了不小的流量带宽的使用，同时也提高访问速度。\n1 2 3 4  gzip on; gzip_min_length 1; # 表示小于1k不压缩 gzip_comp_level 2; # 设置压缩的级别 gzip_types text_plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png; # 启用压缩类型的文件   gzip_comp_level的压缩比设置区间为0-9,设置的压缩比越高，文件越小，但是对服务器CPU的消耗也就越多，一般出于这种选择压缩比为6。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  root@master:/webserver/nginx18/conf# curl -I http://web.devopsman.cn/test.html HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Sun, 12 Jul 2020 13:45:22 GMT Content-Type: text/html Content-Length: 20480 Last-Modified: Sun, 12 Jul 2020 13:34:14 GMT Connection: keep-alive ETag: \u0026#34;5f0b1156-5000\u0026#34; Accept-Ranges: bytes root@master:/webserver/nginx18/conf# curl -H \u0026#34;Accept-Encoding: gzip\u0026#34; -I http://web.devopsman.cn/test.html HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Sun, 12 Jul 2020 13:45:26 GMT Content-Type: text/html Last-Modified: Sun, 12 Jul 2020 13:34:14 GMT Connection: keep-alive ETag: W/\u0026#34;5f0b1156-5000\u0026#34; Content-Encoding: gzip   通过以上测试的方式，既可以访问gzip_types中设定的类型的资源的时候，除了图片之外，应该能成功的找到Content-Encoding: gzip。\n配置访问日志格式 nginx 的日志格式，是通过 nginx 支持的环境变量将所需要的日志信息给组装在一起，在 nginx 里，除了内置的环境变量之外，还包括第三方模块的环境变量信息，比如，nginx 核心模块的支持的环境变量可以在Embedded Variables找到，第三方支持哪些环境变量可以在对应的模块文档中找到，比如realip模块支持的环境变量Embedded Variables，下面是是一个定义的案例，定义一个main类型的日志格式。\nlog_format main '{' '\u0026quot;@timestamp\u0026quot;:\u0026quot;$time_iso8601\u0026quot;,' '\u0026quot;connection\u0026quot;:\u0026quot;$connection\u0026quot;,' '\u0026quot;connection_requests\u0026quot;:\u0026quot;$connection_requests\u0026quot;,' '\u0026quot;request\u0026quot;:\u0026quot;$request\u0026quot;,' '\u0026quot;request_time\u0026quot;:\u0026quot;$request_time\u0026quot;,' '\u0026quot;request_length\u0026quot;:\u0026quot;$request_length\u0026quot;,' '\u0026quot;request_uri\u0026quot;:\u0026quot;$request_uri\u0026quot;,' '\u0026quot;request_method\u0026quot;:\u0026quot;$request_method\u0026quot;,' '\u0026quot;server_name\u0026quot;:\u0026quot;$server_name\u0026quot;,' '\u0026quot;status\u0026quot;:\u0026quot;$status\u0026quot;,' '\u0026quot;remote_user\u0026quot;:\u0026quot;$remote_user\u0026quot;,' '\u0026quot;remote_addr\u0026quot;:\u0026quot;$remote_addr\u0026quot;,' '\u0026quot;time_local\u0026quot;:\u0026quot;$time_local\u0026quot;,' '\u0026quot;body_bytes_sent\u0026quot;:\u0026quot;$body_bytes_sent\u0026quot;,' '\u0026quot;bytes_sent\u0026quot;:\u0026quot;$bytes_sent\u0026quot;,' '\u0026quot;msec\u0026quot;:\u0026quot;$msec\u0026quot;,' '\u0026quot;upstream_status\u0026quot;:\u0026quot;$upstream_status\u0026quot;,' '\u0026quot;upstream_addr\u0026quot;:\u0026quot;$upstream_addr\u0026quot;,' '\u0026quot;upstream_response_time\u0026quot;:\u0026quot;$upstream_response_time\u0026quot;,' '\u0026quot;http_sessionKey\u0026quot;:\u0026quot;$http_sessionKey\u0026quot;,' '\u0026quot;cookie_sk\u0026quot;:\u0026quot;$cookie_sk\u0026quot;,' '\u0026quot;http_referer\u0026quot;:\u0026quot;$http_referer\u0026quot;,' '\u0026quot;http_user_agent\u0026quot;:\u0026quot;$http_user_agent\u0026quot;,' '\u0026quot;http_x_forwarded_for\u0026quot;:\u0026quot;$http_x_forwarded_for\u0026quot;' '}'; 在定义好日志格式之后，通过access_log指令配置日志的存储路径，在配置完日志之后，reload一下就可自动创建日志文件，以上的环境变量含义都可以在官网上找到准确的释义:\n1  access_log logs/web.devopsman.cn.access.log main;   此时，发送一个 http 的请求，就会按照日志的格式记录日志写入到日志文件中:\n1 2  root@master:/webserver/nginx18/logs# tail -f web.devopsman.cn.access.log {\u0026#34;@timestamp\u0026#34;:\u0026#34;2020-07-11T10:26:55-04:00\u0026#34;,\u0026#34;connection\u0026#34;:\u0026#34;17\u0026#34;,\u0026#34;connection_requests\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;request\u0026#34;:\u0026#34;HEAD / HTTP/1.1\u0026#34;,\u0026#34;request_time\u0026#34;:\u0026#34;0.000\u0026#34;,\u0026#34;request_length\u0026#34;:\u0026#34;81\u0026#34;,\u0026#34;request_uri\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;request_method\u0026#34;:\u0026#34;HEAD\u0026#34;,\u0026#34;server_name\u0026#34;:\u0026#34;web.devopsman.cn\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;200\u0026#34;,\u0026#34;remote_user\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;remote_addr\u0026#34;:\u0026#34;192.168.99.1\u0026#34;,\u0026#34;time_local\u0026#34;:\u0026#34;11/Jul/2020:10:26:55 -0400\u0026#34;,\u0026#34;body_bytes_sent\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;bytes_sent\u0026#34;:\u0026#34;236\u0026#34;,\u0026#34;msec\u0026#34;:\u0026#34;1594477615.080\u0026#34;,\u0026#34;upstream_status\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;upstream_addr\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;upstream_response_time\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;http_sessionKey\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;cookie_sk\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;http_referer\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;http_user_agent\u0026#34;:\u0026#34;curl/7.64.1\u0026#34;,\u0026#34;http_x_forwarded_for\u0026#34;:\u0026#34;-\u0026#34;}   ","description":"通过规范使用Gzip压缩、日志格式来配置Nginx静态服务器","id":2,"section":"posts","tags":["Nginx","gzip","日志规范"],"title":"Nginx专辑-02 Nginx静态服务器规范化配置","uri":"https://linuxermaster.github.io/en/posts/20200712-nginx%E4%B8%93%E8%BE%91-02-nginx%E9%9D%99%E6%80%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A7%84%E8%8C%83%E5%8C%96%E9%85%8D%E7%BD%AE/"},{"content":"Nginx 编译安装的前期准备 Nginx 的编译安装一般用于需要加入一些扩展的第三方模块或者自研发的扩展功能包，所以在编译之前需要提前下载准备好这些扩展的包文件，比如我们编译的时候增加需要使用jemalloc优化 nginx 内存，使用openssl实现 nginx 的 https 数据加密，使用zlib库扩展 nginx 压缩的功能等，下面我们下载一些我们需要的基础依赖库的包文件:\n1 2 3 4 5 6 7 8 9 10 11 12 13  cd /usr/local/src wget http://nginx.org/download/nginx-1.14.2.tar.gz wget http://nginx.org/download/nginx-1.10.3.tar.gz wget https://www.zlib.net/fossils/zlib-1.2.8.tar.gz wget https://openssl.org/source/openssl-1.0.2d.tar.gz wget https://github.com/simplresty/ngx_devel_kit/archive/v0.3.0.tar.gz -O ngx_devel_kit.tar.gz wget https://github.com/jemalloc/jemalloc/releases/download/4.5.0/jemalloc-4.5.0.tar.bz2 wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz -O nginx_upstream_check_module.tar.gz wget https://github.com/openresty/lua-nginx-module/archive/v0.10.10.tar.gz -O lua-nginx-module.tar.gz tar xf jemalloc-4.5.0.tar.bz2 cd jemalloc-4.5.0 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install apt-get install -y lua5.1-0-dev sudo apt-get install libpcre3 libpcre3-dev # support pcre   jemalloc的安装也可以直接使用apt安装\n1  apt-get install -y libjemalloc-dev   如果在编译的过程中出现一些下面类似的错误，可以通过安装开发者工具包解决这个问题:\n1 2 3 4 5 6 7  checking for xsltproc... false checking for gcc... no checking for cc... no checking for cl.exe... no # ubuntu上安装开发者工具包 sudo apt install build-essential -y   编译安装 Nginx1.18.0 首先我们准备一个 nginx 的运行用户www\n1 2 3  groupadd www \u0026amp;\u0026amp; useradd www -g www # 开始编译nginx ./configure --prefix=/webserver/nginx18 --user=www --group=www --with-pcre --with-zlib=/root/workspace/packages/nginx/zlib-1.2.8 --with-openssl=/root/workspace/packages/nginx/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/root/workspace/packages/nginx/ngx_devel_kit-0.3.0 --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/root/workspace/packages/nginx/nginx_upstream_check_module-0.3.0 \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install   编译完成后，在启动 nginx\n1 2 3 4 5 6 7 8 9 10  /webserver/nginx18/sbin/nginx # 然后我们将nginx的work进程设置为2个，修改用户为www root@master:/webserver/nginx18/sbin# ps -fe |grep nginx root 50131 1 0 08:43 ? 00:00:00 nginx: master process ./nginx www 58005 50131 0 08:54 ? 00:00:00 nginx: worker process www 58006 50131 0 08:54 ? 00:00:00 nginx: worker process root@master:/webserver/nginx18/sbin# cp nginx /usr/local/bin/nginx root@master:/webserver/nginx18/sbin# nginx -t root@master:/webserver/nginx18/sbin# nginx -s reload   然后我们配置一个web.devopsman.cn的域名到192.168.99.128上，看一下服务运行和 DNS 解析后的效果:\n1 2 3 4 5 6 7 8 9 10  root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# curl -I web.devopsman.cn HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Sat, 11 Jul 2020 13:03:29 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Sat, 11 Jul 2020 12:38:29 GMT Connection: keep-alive ETag: \u0026#34;5f09b2c5-264\u0026#34; Accept-Ranges: bytes   高亮 Nginx 的配置 nginx 的源码包里面有配置 vim 编辑的 nginx 配置文件时内容格式高亮显示的配置文件，直接将其拷贝到.vim目录下即可\n1 2 3 4 5 6 7 8 9 10  root@master:~# cd /root/workspace/packages/nginx/nginx-1.18.0/contrib/ root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# ls -al total 24 drwxr-xr-x 4 www www 4096 Jul 11 08:02 . drwxr-xr-x 9 www www 4096 Jul 11 08:21 .. -rw-r--r-- 1 www www 1272 Apr 21 10:09 geo2nginx.pl -rw-r--r-- 1 www www 543 Apr 21 10:09 README drwxr-xr-x 2 www www 4096 Jul 11 08:02 unicode2nginx drwxr-xr-x 6 www www 4096 Apr 21 10:09 vim root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# cp -r vim/* ~/.vim/   拷贝进去之后，我们发现编辑 nginx 的配置文件变得高亮了\n","description":"Nginx专辑01 Nginx1.18源码自定义安装","id":3,"section":"posts","tags":["Nginx","编译","web server"],"title":"Nginx专辑- 01 Nginx1.18编译安装","uri":"https://linuxermaster.github.io/en/posts/20200711-nginx%E4%B8%93%E8%BE%91-01-nginx1.18%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"},{"content":"Replication Adapters harbor 提供了 harbor 与非 harbor 服务之间的镜像复制功能，通过复制镜像功能可以将 DockerHub 上的私有镜像给批量的同步到本地，也可以通过在多 Harbor 实例之间进行同步，达到镜像多副本，数据高可用的作用。如下图所示，Harbor 支持与 harbor、query.io、Docker-hub 等知名的镜像仓库之间同步，还支持 helm-hub 等 helm 图表之间的同步：\n下面我们通过 harbor 与 harbor 实例之间同步，看一下 Harbor 的镜像复制功能\nHarbor To Harbor 基于上节Harbor 开源镜像仓库企业级实践，安装了域名为harbor.devopsman.cn的 harbor 实例，根据同样的方式安装一个新的实例，并且配置它的域名为harbor.kubemaster.top，我们把上一节中推送到 harbor.devopsman.cn 的镜像 devopsman/kube-proxy:v1.18.0 镜像同步到 harbor.kubemaster.top，在准备好这些环境之后，我们开始配置镜像之间的同步。\n首先在 harbor.kubemaster.top 中新增一个仓库目标，如下图所示，配置好目前镜像仓库的地址、用户名和密码之后，点击测试保存\n然后 ，在系统管理下面的复制管理处，新增加一个复制策略，如下图:\n在源镜像仓库处选择我们之前在仓库管理处配置好的 harbor 实例，然后配置源资源过滤信息，其中的匹配信息如下，其中(Y)表示合法，(N)表示不合法\n   Pattern String(Match or not)     library/* library/hello-world(Y) library/my/hello-world(N)   library/** library/hello-world(Y) library/my/hello-world(Y)   {library,goharbor}/** library/hello-world(Y) goharbor/harbor-core(Y) google/hello-world(N)   1.? 1.0(Y) 1.01(N)    然后填写自定义的过滤器规则之后，保存。然后在复制管理功能处，找到并选择刚才新建的复制规则，点击复制：\n这样就手动的触发了复制，其状态为InProgress，我们点击对应的任务 ID，就能看到更多的复制信息，比如成功状态信息，同步的日志信息等\n这样目标私有镜像仓库中的镜像就被同步了过来，同时 harbor 也支持定时任务同步，其根据 cron 的语法规则实现私有镜像仓库之间的镜像的自动同步。\nHarbor 高可用 如果想要保证 harbor 实例数据的高可用，可以通过复制管理的功能在多个 harbor 之间互相配置同步策略，可以基于push mode实现任意一个实例上面推送了镜像，都会被自动的同步到其他的实例上去，结合 cron 的定时任务基本上就自实现了 harbor 的数据高可用，避免因单实例造成数据无法挽回。\n精彩文章回顾  Harbor 开源镜像仓库企业级实现 阿里云出品·Kubernetes 深入浅出实践 v1.0 微软出品·Kubernetes 最新学习指南 v3.0 火焰图：全局视野的 Linux 性能剖析 1k+在读 最流行的五款 Kubernetes 交互式可视化工具 900+在读 轻松爬取拉勾网岗位招聘信息 600+在读 Yearning - 最 Popular 的 MYSQL 审计平台 700+在读 Prometheus 监控系列-部署篇 500+在读  ","description":"Harbor的多实例间数据同步高可用实践","id":4,"section":"posts","tags":["Harbor","镜像仓库","replication","高可用"],"title":"Harbor2.0 Replication Manager","uri":"https://linuxermaster.github.io/en/posts/20200708-harbor2.0-%E5%A4%9A%E5%AE%9E%E4%BE%8B%E4%B9%8B%E9%97%B4%E9%95%9C%E5%83%8F%E5%A4%8D%E5%88%B6/"},{"content":"Harbor是一个开源的管理 Docker 镜像以及 helm 图表的项目，该项目包括了权限管理、LDAP 认证集成、日志事件审计、管理界面 portal、多 Harbor 实例之间数据同步的公布功能，同时今年 Harbor 也从 CNCF 中毕业了，提供了更合规、性能更好、操作体验更佳的 2.0 版本，帮助您在跨 kubernetes 和 Docker 等云原生计算平台持续高效的管理制品。\nHarbor 环境需求    Software Version Description     Docker engine 17.06.0-ce 及以上 安装参考: Docker Docs   Docker Compose 1.18.0 版本及以上即可 安装参考: Docker-Compose Docs   Openssl 为避免安全漏洞，建议升级到最新 可以选择自签证书: Config HTTPS也可以申请个人免费版证书    Harbor 离线安装 考虑到网络和测试的需求，我们通过迅雷在 GITHUB 上下载离线安装的 harbor 安装包，其中包含了 Harbor 所需要的基础镜像，离线安装时通过离线安装包内的脚本将导出的镜像文件通过docker load -i导入到服务器内，然后通过编排软件docker-compose运行，相当方便。使用到的基础镜像如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  goharbor/chartmuseum-photon v2.0.0 4db8d6aa63e9 8 weeks ago 127MB goharbor/redis-photon v2.0.0 c89ea2e53cc0 8 weeks ago 72.2MB goharbor/trivy-adapter-photon v2.0.0 6122c52b7e48 8 weeks ago 103MB goharbor/clair-adapter-photon v2.0.0 dd2210cb7f53 8 weeks ago 62MB goharbor/clair-photon 2 v2.0.0 f7c7fcc52278 8 weeks ago 171MB goharbor/notary-server-photon v2.0.0 983ac10ed8be 8 weeks ago 143MB goharbor/notary-signer-photon v2.0.0 bee1b6d75e0d 8 weeks ago 140MB goharbor/harbor-registryctl v2.0.0 c53c32d58d04 8 weeks ago 102MB goharbor/registry-photon v2.0.0 afdc1b7ada36 8 weeks ago 84.5MB goharbor/nginx-photon v2.0.0 17892f03e56c 8 weeks ago 43.6MB goharbor/harbor-log v2.0.0 5f8ff08e795c 8 weeks ago 82MB goharbor/harbor-jobservice v2.0.0 c68a2495bf55 8 weeks ago 116MB goharbor/harbor-core v2.0.0 3aa3af64baf8 8 weeks ago 138MB goharbor/harbor-portal v2.0.0 e0b1d3c894c4 8 weeks ago 52.4MB goharbor/harbor-db v2.0.0 5c76f0296cec 8 weeks ago 154MB goharbor/prepare v2.0.0 7266d49995ed 8 weeks ago 158MB   在安装之前，我们需要修改一下 Harbor 的配置文件，比如修改 harbor 使用的域名、HTTPS 证书的位置等,下面我们简单的说明:\n1 2 3 4  ☸️ k8sdev🔥 default ~/Downloads  🐳 👉 scp harbor-offline-installer-v2.0.0.tgz root@192.168.99.128:/root/ # 在192.168.99.128上进行Harbor的安装配置 root@master:~# tar xf harbor-offline-installer-v2.0.0.tgz root@master:~/harbor# mv harbor.yml.tmpl harbor.yml   我们重新命名配置文件 harbor.yml 后，编辑该文件\n1  hostname: harbor.devopsman.cn # 配置自己的harbor访问域名   如果你需要配置 HTTPS 来访问 harbor,那么需要申请证书或者自己生成的自签证书，这里我们直接在 DNSPOD 上申请一个免费的一年可用的证书\n实际上 https 的证书是配置给 harbor 的反向代理 nginx 的，我们一般访问 harbor 其实是访问 nginx 然后将请求转发给 harbor，在我们安装完成 harbor 之后，可以运行以下命令查看到 Nginx 相关的反向代理的配置:\n1  root@master:~/harbor# docker exec -it nginx cat /etc/nginx/nginx.conf   接下来，我们需要配置一下 HTTPS 可信证书，首先在 DNSPOD 上下载申请审批成功后的证书\n然后解压，将解压后的 Nginx 目录下的两个文件重命名放在/data/cert/目录下\n1 2 3 4 5 6 7 8 9 10  root@master:~/harbor/Nginx# ls -alh total 32K drwxr-xr-x 2 root root 4.0K Jul 7 2020 . drwxr-xr-x 100 root root 20K Jul 7 01:37 .. -rw-r--r-- 1 root root 3.7K Jul 7 2020 1_harbor.devopsman.cn_bundle.crt -rw-r--r-- 1 root root 1.7K Jul 7 2020 2_harbor.devopsman.cn.key # 重命名之后放在/data/cert目录下，如果没有这个目录，需要提前创建 mkdir -pv /data/cert root@master:~/harbor/Nginx# cp 1_harbor.devopsman.cn_bundle.crt /data/cert/harbor.devopsman.cn.crt root@master:~/harbor/Nginx# cp 2_harbor.devopsman.cn.key /data/cert/harbor.devopsman.cn.key   然后在 harbor.yml 下配置证书的位置即可:\n1 2 3 4 5 6 7  # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /data/cert/harbor.devopsman.cn.crt private_key: /data/cert/harbor.devopsman.cn.key   配置完证书，使用 prepare 进行配置 nginx 的 https 证书\n1 2 3 4 5 6  cd /root/harbor # harbor离线版解压后的文件目录 root@master:~/harbor# ./prepare prepare base dir is set to /root/harbor ... Generated configuration file: /compose_location/docker-compose.yml Clean up the input dir   无果没有报错，然后我们就可以直接使用 docker-compose 运行 harbor 的容器了\n1 2 3 4 5 6 7 8 9 10 11  root@master:~/harbor# docker-compose up -d Creating network \u0026#34;harbor_harbor\u0026#34; with the default driver Creating harbor-log ... done Creating harbor-portal ... done Creating registry ... done Creating registryctl ... done Creating harbor-db ... done Creating redis ... done Creating harbor-core ... done Creating nginx ... done Creating harbor-jobservice ... done   安装检测 然后我们可以直接通过 docker login 测试一下\n1 2 3 4 5 6 7 8  root@master:~/harbor# docker login harbor.devopsman.cn Username: admin Password: # 默认的密码在harbor.yml文件中，可以自行修改 WARNING! Your password will be stored unencrypted in /root/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   通过浏览器查看是否能登录以及证书的有效期\n此时我们可以通过推送一个测试的镜像，检查是够正常，我们先在浏览器中访问 harbor.devopsman.cn，然后找到推送命令，查看推送的格式，下面我们测试一下看看结果：\n我们选择一个 k8s 集群服务基础组件kube-proxy的镜像，重新打个镜像的 tag，然后推送到我们的devopsman仓库内:\n1 2 3 4 5 6 7 8 9 10 11  root@master:~/harbor# docker tag k8s.gcr.io/kube-proxy:v1.18.0 harbor.devopsman.cn/devopsman/kube-proxy:v1.18.0 root@master:~/harbor# docker push harbor.devopsman.cn/devopsman/kube-proxy:v1.18.0 The push refers to repository [harbor.devopsman.cn/devopsman/kube-proxy] 46b37415a80a: Pushed 0d8d54147a3a: Pushed 597151d24476: Pushed ad9fb2411669: Pushed 2dc2f2423ad1: Pushed 682fbb19de80: Pushed fc4976bd934b: Pushed v1.18.0: digest: sha256:b832454a96a848ad5c51ad8a499ef2173b627ded2c225e3a6be5aad9446cb211 size: 1786   通过查看，确实成功的完成了镜像的上传:\n这样就完成了 harbor 环境的基础搭建，接下来就好好的体验 Harbor 带来的新功能吧，通过图上看到 harbor2.0 目前支持Dark主题，helm 推送等，开始动手吧，体验 Harbor 的新功能和特性，本章节就完成了 harbor 的基础探索。\n","description":"Harbor是一个开源的管理 Docker 镜像以及 Helm 图表的项目","id":5,"section":"posts","tags":["Harbor","镜像仓库","HTTPS","高可用"],"title":"Harbor2.0 安装部署实践(HTTPS)","uri":"https://linuxermaster.github.io/en/posts/20200707-harbor2.0%E5%B8%A6https%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5/"},{"content":"Lazydocker的使用背景 平时的工作管理中会使用到各种各样的命令行工具，有些人是比较厌烦的去学习各种命令以及选项，比如Docker的各种命令和选项其实都让人很头大，于是就有人做出来一款名为Lazydocker的专为懒人设计的Docker和docker-compose终端管理工具，该工具使用Go语言开发基于gocui实现。\n如果你发现自己的项目出了问题，或者是服务down掉了，那么Lazydocker就可以立刻给你提供帮助。Lazydocker可以帮助我们DEBUG自己的项目或者服务，并且在出现问题时立刻重启所有组件，然后给我们提供详细的日志流。其中，日志流还会进行细项分类，并允许我们了解特定服务中发生的所有事情。是github上一个比较🔥的开源工具。\n除此之外，想要记住所有的Docker命令其实是很麻烦的，而且跨多个终端窗口跟踪容器也几乎是无法做到的。但是在Lazydocker的帮助下，我们就可以在一个终端窗口中查看到所有你所需要的信息，而且常用的命令仅需按下一个键即可实现。毫无疑问，Lazydocker绝对是懒人们的福音!我们先来看一下效果图：\nLazydocker的功能 现在让我们先了解一下Lazydocker的功能\n  清晰的查看所有的Docker和Docker-compose容器环境的状态\n  实时查看容器/服务日志；\n  查看容器指标的ascii图，这样您不仅可以感觉到而且看起来像开发人员\n  自定义这些图形以测量您想要的几乎任何指标\n  进入容器/服务；\n  重启/移除/重建容器或服务；\n  查看给定镜像的历史层信息\n  修改占用磁盘空间的容器、镜像或卷；\n  Lazydocker安装配置 你可以直接在Github上下载二进制文件，也可以通过容器运行该命令，此处我直接使用二进制命令\n1 2 3  wget https://github.com/jesseduffield/lazydocker/releases/download/v0.9/lazydocker_0.9_Darwin_x86_64.tar.gz tar xf lazydocker_0.9_Darwin_x86_64.tar.gz cp lazydocker /usr/local/bin/ \u0026amp;\u0026amp; chmod +x /usr/local/bin/lazydocker   因为该命令太长了，所以建议配置一个命令别名，方便我们使用:\n1 2  echo \u0026#34;alias lzd=\u0026#39;lazydocker\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc   此时，直接执行lzd即可在终端上显示容器的各种指标状态图\nLazydocker的配置 Lazydocker支持用户自定义配置，对于不同的操作系统其配置文件存在不同的位置上\n OSX: ~/Library/Application Support/jesseduffield/lazydocker/config.yml Linux: ~/.config/jesseduffield/lazydocker/config.yml Windows: C:\\\\Users\\\\\u0026lt;User\u0026gt;\\\\AppData\\\\Roaming\\\\jesseduffield\\\\lazydocker\\\\config.yml  不过你可以在打开Lazydocker之后，鼠标移到左上方，使用快捷键o即可打开配置文件进入编辑状态，此时直接编辑即可，想要知道每个配置文件选项的含义，可以参考开发配置参数查看，下面是一个配置的案例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  gui: scrollHeight: 2 theme: activeBorderColor: - green - bold inactiveBorderColor: - white optionsTextColor: - blue returnImmediately: false wrapMainPanel: false reporting: undetermined commandTemplates: dockerCompose: docker-compose restartService: \u0026#39;{{ .DockerCompose }} restart {{ .Service.Name }}\u0026#39; stopService: \u0026#39;{{ .DockerCompose }} stop {{ .Service.Name }}\u0026#39; serviceLogs: \u0026#39;{{ .DockerCompose }} logs --since=60m --follow {{ .Service.Name }}\u0026#39; viewServiceLogs: \u0026#39;{{ .DockerCompose }} logs --follow {{ .Service.Name }}\u0026#39; rebuildService: \u0026#39;{{ .DockerCompose }} up -d --build {{ .Service.Name }}\u0026#39; recreateService: \u0026#39;{{ .DockerCompose }} up -d --force-recreate {{ .Service.Name }}\u0026#39; viewContainerLogs: docker logs --timestamps --follow --since=60m {{ .Container.ID }} containerLogs: docker logs --timestamps --follow --since=60m {{ .Container.ID }} allLogs: \u0026#39;{{ .DockerCompose }} logs --tail=300 --follow\u0026#39; viewAlLogs: \u0026#39;{{ .DockerCompose }} logs\u0026#39; dockerComposeConfig: \u0026#39;{{ .DockerCompose }} config\u0026#39; checkDockerComposeConfig: \u0026#39;{{ .DockerCompose }} config --quiet\u0026#39; serviceTop: \u0026#39;{{ .DockerCompose }} top {{ .Service.Name }}\u0026#39; customCommands: containers: - name: bash attach: true command: docker exec -it {{ .Container.ID }} /bin/sh serviceNames: [] oS: openCommand: open {{filename}} openLinkCommand: open {{link}} update: dockerRefreshInterval: 100ms stats: graphs: - caption: CPU (%) statPath: DerivedStats.CPUPercentage color: blue - caption: Memory (%) statPath: DerivedStats.MemoryPercentage color: green   Lazydocker的快捷键 在lazydocker的交互式界面中，还提供了多种快捷键，大家可以通过快捷键快速的在多种功能之间切换。\nProject  e: edit lazydocker config o: open lazydocker config [: previous tab ]: next tab m: view logs enter: focus main panel  Containers  [: previous tab ]: next tab d: remove e: Hide/Show stopped containers s: stop r: restart a: attach m: view logs c: run predefined custom command b: view bulk commands enter: focus main panel  Services  d: remove containers s: stop r: restart a: attach m: view logs [: previous tab ]: next tab R: view restart options c: run predefined custom command b: view bulk commands enter: focus main panel  Images  [: previous tab ]: next tab c: run predefined custom command d: remove image b: view bulk commands enter: focus main panel  Volumes  [: previous tab ]: next tab c: run predefined custom command d: remove volume b: view bulk commands enter: focus main panel  Main  esc: return  更多精彩专辑  运维架构专辑: 包含一些运维技术的使用经验分享、技术架构案例、学习交流等 kubernetes实践案例:都是kubernetes相关的实践案例，毫无套路，大家都说好! DevOps实践案例: 专辑内包括Jenkins和Gitlab各自持续集成、持续发布的实践案例，以及各种模板库的最佳实践。  ","description":"Lazydocker-专为懒人设计的容器命令行可视化交互式工具","id":6,"section":"posts","tags":["docker","工具","可视化"],"title":"Docker容器命令行可视化工具-Lazydocker","uri":"https://linuxermaster.github.io/en/posts/20200615-lazydocker-%E4%B8%93%E4%B8%BA%E6%87%92%E4%BA%BA%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%AE%B9%E5%99%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E4%BA%A4%E4%BA%92%E5%BC%8F%E5%B7%A5%E5%85%B7/"},{"content":"安装 Apache HTTP 服务 Apache是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上，由于其跨平台和安全性被广泛使用，是最流行的Web服务器端软件之一。\n 执行如下命令，安装Apache服务及其扩展包。  yum -y install httpd httpd-manual mod_ssl mod_perl mod_auth_mysql 返回类似如下图结果则表示安装成功。\n执行如下命令，启动Apache服务。  systemctl start httpd.service 测试Apache服务是否安装并启动成功。  Apache默认监听80端口，所以只需在浏览器访问ECS分配的IP地址http://\u0026lt;ECS公网地址\u0026gt;，如下图：\n安装 MySQL 数据库 由于使用wordpress搭建云上博客，需要使用MySQL数据库存储数据，所以这一步我们安装一下MySQL。\n 执行如下命令，下载并安装MySQL官方的Yum Repository。  wget http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm yum -y install mysql57-community-release-el7-10.noarch.rpm yum -y install mysql-community-server 执行如下命令，启动 MySQL 数据库。  systemctl start mysqld.service 执行如下命令，查看MySQL运行状态。  systemctl status mysqld.service 执行如下命令，查看MySQL初始密码。  grep \u0026quot;password\u0026quot; /var/log/mysqld.log 执行如下命令，登录数据库。  mysql -uroot -p 执行如下命令，修改MySQL默认密码。   说明 新密码设置的时候如果设置的过于简单会报错，必须同时包含大小写英文字母、数字和特殊符号中的三类字符。\n ALTER USER 'root'@'localhost' IDENTIFIED BY 'NewPassWord1.'; 执行如下命令，创建wordpress库。  create database wordpress; 执行如下命令，创建wordpress库。 执行如下命令，查看是否创建成功。  show databases; 输入exit退出数据库。  安装 PHP 语言环境 WordPress是使用PHP语言开发的博客平台，用户可以在支持PHP和MySQL数据库的服务器上架设属于自己的网站。也可以把WordPress当作一个内容管理系统（CMS）来使用。\n 执行如下命令，安装PHP环境。  1  yum -y install php php-mysql gd php-gd gd-devel php-xml php-common php-mbstring php-ldap php-pear php-xmlrpc php-imap   执行如下命令创建PHP测试页面。  1  echo \u0026#34;\u0026lt;?php phpinfo(); ?\u0026gt;\u0026#34; \u0026gt; /var/www/html/phpinfo.php   执行如下命令，重启Apache服务。  1  systemctl restart httpd   打开浏览器，访问http://\u0026lt;ECS公网地址\u0026gt;/phpinfo.php，显示如下页面表示PHP语言环境安装成功。  Wordpress安装和配置 本小节将在已搭建好的LAMP 环境中，安装部署 WordPress\n 执行如下命令，安装wordpress。  1  yum -y install wordpress   显示如下信息表示安装成功。\n修改WordPress配置文件。  1）执行如下命令，修改wp-config.php指向路径为绝对路径。\n1 2 3 4 5 6  # 进入/usr/share/wordpress目录。 cd /usr/share/wordpress # 修改路径。 ln -snf /etc/wordpress/wp-config.php wp-config.php # 查看修改后的目录结构。 ll   2）执行如下命令，移动wordpress到Apache根目录。\n1 2 3  # 在Apache的根目录/var/www/html下，创建一个wp-blog文件夹。 mkdir /var/www/html/wp-blog mv * /var/www/html/wp-blog/   3）执行以下命令修改wp-config.php配置文件。\n在执行命令前，请先替换以下三个参数值。\n database_name_here为之前步骤中创建的数据库名称，本示例为wordpress。 username_here为数据库的用户名，本示例为root。 password_here为数据库的登录密码，本示例为NewPassWord1.。  1 2 3  sed -i \u0026#39;s/database_name_here/wordpress/\u0026#39; /var/www/html/wp-blog/wp-config.php sed -i \u0026#39;s/username_here/root/\u0026#39; /var/www/html/wp-blog/wp-config.php sed -i \u0026#39;s/password_here/NewPassWord1./\u0026#39; /var/www/html/wp-blog/wp-config.php   4）执行以下命令，查看配置文件信息是否修改成功。\n1  cat -n /var/www/html/wp-blog/wp-config.php   执行如下命令，重启Apache服务。  1  systemctl restart httpd   ","description":"wordpress博客网站搭建指南","id":7,"section":"posts","tags":["wordpress","博客"],"title":"ECS上手把手搭建个人博客系统wordpress","uri":"https://linuxermaster.github.io/en/posts/20200610-%E6%9E%B6%E6%9E%84%E5%AE%9E%E9%AA%8C%E5%AE%A4-%E4%BA%91%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"},{"content":"这次终于把自己给\u0026quot;坑\u0026quot;坏了，因为一次不可控制的\u0026quot;停电\u0026quot;让公司的部分老年机饱受了一次\u0026quot;灾难性\u0026quot;的洗礼，幸运的是部分幸存下来的主机中包括我们至关重要的服务，而就是那些看似无状态的服务深深的打了自己一巴掌。首当其冲的就是那个万恶的Openvpn服务。使用Openvpn主要是为了解决一下场景:\n我们的部分非业务办公服务主要服务于内部的办公人员，由于疫情的背景下，我们需要满足远程办公的条件以及非本地的同事也是需要访问这些资源的，包括一些暴露在公网限制访问的服务等，所以通过点对点登录访问的Openvpn就是我们一直以来的选择。\n灾难后的恢复永远是让人发狂，而这边大家又在不断的\u0026quot;吹更\u0026rdquo;，实在难以淡定下来，不懂网络的我也没法直接在那些老古董身上动刀子，在一段焦虑和无奈之后还是冷静下仔细思考如何快速的恢复\u0026quot;战场\u0026rdquo;，因为一台物理机老前辈因为掉电让磁盘歇工了，短时间内想要修复物理机恐怕已经不可能了，所以我开始这样做：\n第一步 网上冲浪，查看防火墙端口绑定规则 在网上疯狂的搜索着关于CISCO ASA5515-X的网络配置，查看防火墙中关于公网IP:x.x.x.x与主机之间端口射映关系，因为不断的变更交接人，记录的网络配置信息和文档早已随着时间飞逝了，这时候不免想来句你妹的\u0026hellip;\n1 2 3 4  ciscoasa# show nat ... ciscoasa# show run object ...   通过查看一波nat、object和service之后，发现了防火墙上配置的Public IP与内部服务OpenVPN主机之间端口的映射关系，于是赶紧找几个还有气的主机进行业务恢复\n第二步 照着葫芦画个瓢 这是一台Centos7主机\n1 2  [root@vpn ~]# cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core)   首先就是将其IP地址修改为防火墙内规则制定的主机IP\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  [root@vpn ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens160 TYPE=\u0026#34;Ethernet\u0026#34; PROXY_METHOD=\u0026#34;none\u0026#34; BROWSER_ONLY=\u0026#34;no\u0026#34; BOOTPROTO=\u0026#34;none\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34; NAME=\u0026#34;ens160\u0026#34; UUID=\u0026#34;aa08d0dd-5ba5-412c-84ab-716b885c4d89\u0026#34; DEVICE=\u0026#34;ens160\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=\u0026#34;172.16.99.129\u0026#34; PREFIX=\u0026#34;24\u0026#34; GATEWAY=\u0026#34;192.168.99.254\u0026#34; # 修改成需要的IP地址 IPV6_PRIVACY=\u0026#34;no\u0026#34; PEERDNS=\u0026#34;yes\u0026#34; DNS1=\u0026#34;114.114.114.114\u0026#34;   第三步 配置OpenVPN环境 在找对IP访问的映射关系之后,就是抓紧恢复服务，于是有了下面的安装配置段:\n 使用easy-rsa制作OpenVPN所需的证书以及客户端证书  1 2 3 4 5 6 7 8 9 10 11  yum install openvpn mkdir /data/tools -p wget -P /data/tools https://github.com/OpenVPN/easy-rsa/releases/download/3.0.1/EasyRSA-3.0.1.tgz tar zxf EasyRSA-3.0.1.tgz cp -rf EasyRSA-3.0.1 /etc/openvpn/easy-rsa cd /etc/openvpn/easy-rsa ./easyrsa init-pki # 初始化证书目录pki ./easyrsa build-ca nopass # 创建根证书，提示输入Common Name，名称随意，但是不能和服务端证书或客户端证书名称相同 ./easyrsa gen-dh # 生成Diffle Human参数，它能保证密钥在网络中安全传输   制作CA证书  1  ./easyrsa init-pki # 初始化证书目录pki   制作服务端OpenVPN Server证书  1  ./easyrsa build-server-full server nopass # server是服务端证书名称，可以用其它名称   制作客户端证书  1  ./easyrsa build-client-full barry nopass # barry是客户端证书名称，可以用其它名称   配置LDAP认证  1 2 3  yum install openvpn-auth-ldap -y [root@vpn openvpn]# ls -al /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so -rwxr-xr-x 1 root root 133320 Sep 6 2019 /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so   准备LDAP认证配置文件  \u0026lt;LDAP\u0026gt; # LDAP server URL URL\tldap://192.168.99.130 # Bind DN (If your LDAP server doesn't support anonymous binds) BindDN\tcn=openvpn,dc=openldap,dc=kubemaster,dc=top Password\topenvpn_Passsword # Network timeout (in seconds) Timeout\t15 # Enable Start TLS #TLSEnable\tno # Follow LDAP Referrals (anonymously) #FollowReferrals no # TLS CA Certificate File #TLSCACertFile\t/usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory #TLSCACertDir\t/etc/ssl/certs # Client Certificate and key # If TLS client authentication is required #TLSCertFile\t/usr/local/etc/ssl/client-cert.pem #TLSKeyFile\t/usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite\tALL:!ADH:@STRENGTH \u0026lt;/LDAP\u0026gt; \u0026lt;Authorization\u0026gt; # Base DN BaseDN\t\u0026quot;ou=People,dc=openldap,dc=kubemaster,dc=top\u0026quot; # User Search Filter SearchFilter\t\u0026quot;(\u0026amp;(uid=%u))\u0026quot; # Require Group Membership RequireGroup\tfalse # Add non-group members to a PF table (disabled) #PFTable\tips_vpn_users \u0026lt;Group\u0026gt; BaseDN\t\u0026quot;ou=Groups,dc=example,dc=com\u0026quot; SearchFilter\t\u0026quot;(|(cn=developers)(cn=artists))\u0026quot; MemberAttribute\tuniqueMember # Add group members to a PF table (disabled) #PFTable\tips_vpn_eng \u0026lt;/Group\u0026gt; \u0026lt;/Authorization\u0026gt; 配置服务端配置文件  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  [root@vpn openvpn]# cat server.conf |egrep -v \u0026#39;^$|^#|^\\;\u0026#39; port 11194 proto tcp dev tun ca /etc/openvpn/easy-rsa/pki/ca.crt cert /etc/openvpn/easy-rsa/pki/issued/server.crt key /etc/openvpn/easy-rsa/pki/private/server.key # This file should be kept secret dh /etc/openvpn/easy-rsa/pki/dh.pem server 10.8.0.0 255.255.255.0 # 这里是openvpn server的IP地址池 ifconfig-pool-persist ipp.txt push \u0026#34;dhcp-option DNS 114.114.114.114\u0026#34; # 下发给客户端的DNS push \u0026#34;dhcp-option DNS 8.8.8.8\u0026#34; client-to-client duplicate-cn keepalive 10 120 comp-lzo max-clients 50 user root group root persist-key persist-tun status openvpn-status.log log openvpn.log log-append openvpn.log verb 3 mute 10 client-cert-not-required plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026#34;/etc/openvpn/auth/ldap.conf\u0026#34; username-as-common-name push \u0026#34;route 192.168.0.0 255.255.0.0\u0026#34; push \u0026#34;route 192.168.99.0 255.255.255.0\u0026#34; # 下发给客户端的需要走VPN的网络流量，其它网段不走VPN，可正常上网。   开启路由转发  1 2  echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p   OpenVPN防火墙的配置，这里是最重要的一环，注意不要把你的网络设备名称写错了。  1  iptables -t nat -A POSTROUTING -s 10.8.0.0/16 -o ens160 -j MASQUERADE # 网络设备为ens160   配置OpenVPN的启停脚本  1 2 3 4 5  #!/bin/bash echo \u0026#34;OpenVPN ..........[STOP]\u0026#34; ps -ef |grep openvpn | grep -v grep | awk \u0026#39;{print $2}\u0026#39; | xargs kill echo \u0026#34;OpenVPN ..........[START]\u0026#34; /usr/local/openvpn/sbin/openvpn --config /etc/openvpn/server.conf \u0026amp;   开机服务自启,将下面内容写到/etc/rc.local  1 2  /usr/local/openvpn/sbin/openvpn --daemon --config /etc/openvpn/server.conf \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; exit 0   客户端的配置  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # 把服务器上这三个文件拷贝下来和客户端的配置文件放在一起 /etc/openvpn/easy-rsa/pki/private/barry.key /etc/openvpn/easy-rsa/pki/issued/barry.crt /etc/openvpn/easy-rsa/pki/ca.crt # 客户端配置文件内容 client dev tun proto tcp resolv-retry infinite nobind remote PUBLIC_ADDRESS 11194 # 就是与192.168.99.129上的11194绑定的那个公网IP地址 persist-key persist-tun ca ca.crt ns-cert-type server cert barry.crt key barry.key verb 3 # 日志等级 comp-lzo auth-user-pass   这样基本上就完成了OpenVPN的搭建部署，也可能是我最后一次整这玩意儿。下面是配置OpenVPN时候遇到的问题\n 明明在openvpn服务端配置了服务端同一网段的下发路由，但就是ping不通服务端同一网段的其他主机，那你需要认真检查下面的配置，包括网卡设备名  1 2  iptables -t nat -A POSTROUTING -s 10.8.0.0/16 -o ens160 -j MASQUERADE # 网络设备为ens160 sudo iptables -nL -t nat # 查看   为什么连上了OpenVPN我所有的流量都经过OpenVPN了,那是你启用了下面的配置，这个配置你也可以用于科学***/上网  1  push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34;   给客户端指定固定的VPN地址  1 2  client-config-dir ccd # 表示指定固定IP地址的客户端配置文件存储在openvpn服务端配置文件统计目录下的ccd目录里面 route 192.168.40.128 255.255.255.248 # 指定客户端IP的地址   为什么在linux/MacOS上配置都没有问题，而在window上有问题，此时你需要注意的是，在window上安装和启动OpenVPN都是需要管理员权限的，因为它会涉及到一些添加路由的操作，这些需要管理员权限。  ","description":"Centos7上安装部署openvpn并通过openldap认证","id":8,"section":"posts","tags":["openvpn","openldap","网络"],"title":"Openvpn-最难忘的一次Oenvpn『灾后重建』","uri":"https://linuxermaster.github.io/en/posts/20200619-%E6%9C%80%E9%9A%BE%E5%BF%98%E7%9A%84%E4%B8%80%E6%AC%A1openvpn%E7%81%BE%E5%90%8E%E9%87%8D%E5%BB%BA/"},{"content":"OpenVPN高级进阶 OpenVPN服务是一个比较老的服务了，但是作为一个核心的基础服务我觉得还是有必要认真仔细的掌握和理解，并且能根据其自身的功能优化和配置出我们需要的场景，只要能解决实际的问题场景，对我们来说就是有价值的，下面通过几个场景聊一下OpeVPN的高阶配置选项:\n背景：\n 公司内部网段是192.168.99.0/24； 所有人允许访问反向代理主机为192.168.99.130，但不能访问其他服务器； 特定的用户允许访问数据库服务器为192.168.99.131，不能访问其他服务器； 管理员能访问所有公司内网服务器。  场景1 限制主机访问 我们不需要VPN客户端访问VPN服务端所在集群中的所有其他主机,允许某些特定的VPN客户端访问指定的内网主机资源的时候，我们需要在客户端无感知的情况下对VPN服务端做一些设置满足以上场景:\n更新服务端的配置，将VPN地址池的网段划分为管理员网段、客户组网段、普通网络:\n1 2 3 4 5 6 7 8 9  # 在Openvpn服务端配置文件server.conf增加： #10.8.0.0是给所有VPN客户端的IP段； server 10.8.0.0 255.255.255.0 #10.8.1.0是给管理员分配的IP段； server 10.8.1.0 255.255.255.0 #10.8.2.0就是给特定用户组分配的IP段； server 10.8.2.0 255.255.255.0 #下面是定义服务器读取特殊客户端配置文件的目录为ccd,ccd是与Openvpn服务端配置文件同级目录中的ccd目录 client-config-dir ccd   然后给管理员配置访问网络\n1 2  cat ccd/sysadmin1 ifconfig-push 10.8.1.1 10.8.1.2   客户组网络:\n1 2  cat ccd/kehugroup ifconfig-push 10.8.2.1 10.8.2.2   这里需要注意的是, ccd目录下的文件名就是用户的Common Name，OpenVPN是根据该名称来获得指定客户端的，客户端的IP地址不是任意指定的，由于Windows的TAP驱动必须采用/30网段的IP，为兼容该协议，应从特定的IP地址中选择，而且是成组出现的。\n最后在完成网络的划分之后，在OpenVPN端进行Iptables限制:\n1 2 3  iptables -A FORWARD -i tun0 -s 10.8.0.0/24 -d 192.168.99.130 -j ACCEPT iptables -A FORWARD -i tun0 -s 10.8.1.0/24 -d 192.168.99.0/24 -j ACCEPT iptables -A FORWARD -i tun0 -s 10.8.2.0/24 -d 192.168.99.131 -j ACCEPT   场景2 打通OpenVPN客户端与服务端的内网 1 2 3 4 5 6  #让所有客户端都增加到内网192.168.99.0/24的路由 push \u0026#34;route 192.168.99.0 255.255.255.0\u0026#34; # 让所有的客户端都能访问仅允许服务端访问的网站(约束白名单) push \u0026#34;route 39.156.69.79 255.255.255.255\u0026#34; # 向客户端推送增加访问服务端子网的192.168.10.0/24的路由，注意服务端的IP是否是子网的网关，否则需要在子网网关处添加到达192.168.99.0的路由(客户端也是如此) push \u0026#34;route 192.168.10.0 255.255.255.0\u0026#34;   在子网网关处添加路由\n1  route add -net 192.168.99.0 mask 255.255.255.0 gw 192.168.10.254 dev ens160   场景3 OpenVPN提供DHCP与DNS OpenVPN内部提供了DHCP的服务，而不需要依赖外部的DHCP服务器。同样，也提供了DHCP服务的一些配置参数\n1 2 3 4 5 6 7 8  # openvpn服务端的配置 #定义客户端的DNS服务器地址 push \u0026#34;dhcp-option DNS 114.114.114.114\u0026#34; # 这是首选DNS push \u0026#34;dhcp-option DNS 8.8.8.8\u0026#34; # 这是备选DNS #定义客户端的WINS服务器地址 push \u0026#34;dhcp-options WINS 192.168.228.1\u0026#34; # 这是设置IP和主机名之间的映射与IP和域名之间的映射不同,较少使用 #让客户端发起的所有IP请求都通过OpenVPN服务器,可用于全局代理使用，启用后会出现浏览器内打不开网站等情况 push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34;   场景4 添加LDAP认证 通常情况下，OpenVPN客户端都需要通过SSL连接的，因此客户端必须要有ca证书，服务端可以通过设置client-cert-not-required让客户端不配置证书。但是一般通过OpenLDAP认证是最方便的事情:\n 安装LDAP`模块的配置  1 2 3  yum install openvpn-auth-ldap -y [root@vpn ~]# ls -al /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so -rwxr-xr-x 1 root root 133320 Sep 6 2020 /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so   准备ldap认证的配置文件/etc/openvpn/auth/ldap.conf  1 2  mkdir -p /etc/openvpn/auth cd $_ \u0026amp;\u0026amp; touch ldap.conf   下面是ldap.conf的配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  [root@vpn ~]# cat /etc/openvpn/auth/ldap.conf \u0026lt;LDAP\u0026gt; # LDAP server URL URL\tldap://192.168.99.130 # Bind DN (If your LDAP server doesn\u0026#39;t support anonymous binds) BindDN\tcn=openvpn,dc=openldap,dc=kubemaster,dc=top Password\topenvpnuserpasswd # Network timeout (in seconds) Timeout\t15 # Enable Start TLS #TLSEnable\tno # Follow LDAP Referrals (anonymously) #FollowReferrals no # TLS CA Certificate File #TLSCACertFile\t/usr/local/etc/ssl/ca.pem # TLS CA Certificate Directory #TLSCACertDir\t/etc/ssl/certs # Client Certificate and key # If TLS client authentication is required #TLSCertFile\t/usr/local/etc/ssl/client-cert.pem #TLSKeyFile\t/usr/local/etc/ssl/client-key.pem # Cipher Suite # The defaults are usually fine here # TLSCipherSuite\tALL:!ADH:@STRENGTH \u0026lt;/LDAP\u0026gt; \u0026lt;Authorization\u0026gt; # Base DN BaseDN\t\u0026#34;ou=People,dc=openldap,dc=kubemaster,dc=top\u0026#34; # User Search Filter SearchFilter\t\u0026#34;(\u0026amp;(uid=%u))\u0026#34; # uid或者cn一般都可以，根据自己的条件 # Require Group Membership RequireGroup\tfalse # 是否启用组成员关系 # Add non-group members to a PF table (disabled) #PFTable\tips_vpn_users \u0026lt;Group\u0026gt; BaseDN\t\u0026#34;ou=Groups,dc=example,dc=com\u0026#34; SearchFilter\t\u0026#34;(|(cn=developers)(cn=artists))\u0026#34; MemberAttribute\tuniqueMember # Add group members to a PF table (disabled) #PFTable\tips_vpn_eng \u0026lt;/Group\u0026gt; \u0026lt;/Authorization\u0026gt;   在完成ldap的配置之后，只需要在服务端增加以下配置即可:\n1 2 3  client-cert-not-required plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026#34;/etc/openvpn/auth/ldap.conf\u0026#34; username-as-common-name   这样就完成了基于openldap认证的OpenVPN客户端的用户鉴权。\n疑难杂症  VPN每隔10s左右会重新连接  基于密码认证的VPN出现每隔10s重新连接，说明你的账号在别的设备上进行登录了。请检查是否存在改情况并建议及时的修改密码。\nMac上的Tunnelblick总是处于不断认证的状态  出现这种情况说明你的Tunnelblick的版本太低，需要你及时的更新该软件的版本，就可以解决，TunnelBlick下载地址\nWindow上使用OpenVPN客户端连接时出现SSL ERROR  出现这种问题说明你的Window上的OpenVPN客户端版本太低，需要重新下载客户端并且使用管理员身份安装和启动。\nWindow上使用OpenVPN客户端连接时出现windows route add command failed  出现这种问题，属于打开VPN客户端的时候没有使用管理员身份打开，没有添加路由的权限。\n使用OpenVPN连接出现身份验证失败  出现这种问题，这是属于你的用户名或者密码填写错误，认真检查账户信息或者找相关技术负责人重新修改密码。\n","description":"Openvpn使用过程中的疑难杂症总结","id":9,"section":"posts","tags":["openvpn","openldap","网络"],"title":"Openvpn-高级进阶","uri":"https://linuxermaster.github.io/en/posts/20200620-openvpn%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6/"},{"content":"今天给大家分享目前最popular的MySQL SQL审计平台Yearning，这个平台可以帮助开发者快速的完成SQL语句的语法的审核、检测、执行和回滚等操作。在早期，我们是先整理出来一套SQL使用规范，然后通过部门会议审核规范的内容，最后要求按照内容应用在实际的工作中，但是这些不免有些开发者依旧就不住或者不上心，不按套路出牌，造成数据库使用不统一。\nYearning自身包含了一套通常适用的审核规范，基本上能满足日常需要，同时规范了日常开发需求所涉及到的SQL变动，在Yearning平台的辅助下，日常的SQL变动也更加贴近SQL使用的规范化、标准化的要求，同时100%基于GO语言研发的Yearning也可以通过自定义二次开发(遵循AGPL协议)增加一些符合自己审核策略，但是它的审核引擎Juno不是开源的。\nYearning的前端是基于Vue.js构建的，而这块审核规则的代码也是全部在JS中传递和处理的，另外它还提供SQL语法高亮、自动补全和智能提示、可视化等。\n使用感受  有工单记录，让变更的SQL记录落库保存，易于审计。 开发者自己提交，监测最大程度减小以往通过微信传送等粘贴出错几率，给DBA.运维省心。 再一次证明落实规范性的东西需要有工具化、流程化，靠自觉万万是做不到的。 SQL审批要设置多成员多级审核，最后自己执行落库。  下面列出支持的主要功能:\nYearning Feature  SQL查询  查询工单 导出 自动补全，智能提示 查询语句审计   SQL审核  流程化工单 SQL语句检测与执行 SQL回滚 历史审核记录   推送  E-mail工单推送 钉钉webhook机器人工单推送   用户权限及管理  角色划分 基于用户的细粒度权限 注册   其他  todoList LDAP登录 动态审核规则配置   AutoTask自动执行  体验 Yearning的安装十分简单，它依赖一个mysql数据库用于存储工单的数据、回滚的SQL语句，所以需要先初始化数据库-m，然后在启动-s。对于回滚的语句不得不说一下，真的要是涉及到大变更的操作的时候，还是建议手动或者可靠地备份方式进行。官方也提供了安装手册，一般建议容器化部署，方便省事，安装包内也有Dockerfile，二次开发的同学也可以自己构建发布。\n在需要配置钉钉或者微信的时候，可以通过选项设置通知时显示Yearning的平台地址\n下面是配置钉钉和OpenLDAP登录的参考:\n无门槛领取DNSPOD/腾讯的域名注册5元券\n","description":"Yearning帮你减小日常变更数据库的风险，规范有序记录进行数据库变更操作","id":10,"section":"posts","tags":["yearning","mysql","Database"],"title":"Yearning目前最流行的开源数据库审核平台","uri":"https://linuxermaster.github.io/en/posts/20200625-sql%E5%AE%A1%E6%A0%B8%E5%BC%80%E6%BA%90%E7%A5%9E%E5%A5%87yearning/"},{"content":"最近有很多朋友看了我的文章之后，问我你终端是怎么设置的，为什么如此炫酷，这这这\u0026hellip;让我怎么说，难道我的文章不干吗？还是特干看不下去了？好吧，今天趁着周末给大家分享一下，如何设置一个你认为很高大上的终端，对于常用终端的发烧友来说无疑是一篇值得收藏的好文章，哈哈\n想要配置这么高大上的终端？你需要安装一个叫做Iterm的远程连接工具，官方说是macOS Terminal Replacement，对的，你的电脑必须是Macos系统才可以，接下来我们一步一步的教你如何配置一个令你心仪的iterm，首先我们需要掌握一下基础的东西，然后在自定义一些喜欢的东西。\n前期准备 安装Iterm2 在官网下载好iterm2的二进制文件，然后直接托放到Macos系统的Application的文件夹内，然后你就可以在启动台``launch里面找到Iterm了。安装完成之后，打开软件，以下的操作均在iterm2上操作`。\n配置oh-my-zsh 现在我们就需要配置一个神助工具oh-my-zsh来让你的Iterm2起飞，最开始的时候，你的iterm是这样的\n下面，我们修改默认的bash为zsh，这里要注意的是，后期需要做一些命令别名,环境变量的时候，就不再是以前的bashrc等了，应是~/.zshrc或者/etc/zshrc啦。\n1 2  brew install zsh # 安装zsh chsh -s /bin/zsh   如果你想要修改回来\n1  chsh -s /bin/bash   修改之后，我们通过wget或者curl的方式下载oh-my-zsh，以下安装方式选择任意一种即可:\n curl方式  1  $ sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34;    wget方式  1  $ sh -c \u0026#34;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34;   执行完之后，就如下所示:\n实际上，就是通过curl或者wget的方式将oh-my-zsh的代码仓库克隆到你的家目录下/Users/marionxue/.oh-my-zsh下\n提供字体支持 接下来我们需要安装一些字体，这些字体提供了某些oh-my-zsh主题的特殊字符的支持，在美化配置上是必不可少的。\n1 2 3 4 5 6 7 8 9  git clone https://github.com/powerline/fonts.git --depth=1 # clone git clone https://github.com/powerline/fonts.git --depth=1 # install cd fonts ./install.sh # clean-up a bit cd .. rm -rf fonts   安装好之后，选择一款Powerline字体了：iterm2 -\u0026gt; Preferences -\u0026gt; Profiles -\u0026gt; Text -\u0026gt; Font -\u0026gt; Change Font（我用的是nerd-fonts因为该字体支持非 ASCII 码字体，如下图所示）,当你的终端在配置主题之后，出现了乱码，那么你的字体一定是没有选对，这里需要使用powerline系列的字体，这是需要注意的地方。如果你需要安装nerd-fonts,直接使用brew即可\n1 2  brew tap caskroom/fonts brew cask install font-hack-nerd-font   美化iterm2 一些特别吸引眼球的美化设置都是通过在主题的基础上自定义修改实现的，默认的oh-my-zsh使用的主题是ZSH_THEME=\u0026quot;robbyrussell\u0026quot;，个人并不好看，下面我们自己选择一个合适的主题，我们可以在/Users/marionxue/.oh-my-zsh/themes下面查看默认提供的主题。而配置文件就是我们之前提及到的~/.zshrc文件，\n我们打开该文件找到ZSH_THEME=\u0026quot;robbyrussell\u0026quot;，然后修改robbyrussell为你喜欢的主题即可，我这里使用的是ZSH_THEME=\u0026quot;agnoster\u0026quot;不过我也推荐这个主题。记得每次修改~/.zshrc文件之后，需要source ~/.zshrc让配置文件生效，我们看一下效果图：\n但是看起来比较单调，不是那么的高大上啊，于是有些人在网上肯定发现过以下这种样式，看起来相对比较完美，研究一下下面这种图的做法：\n仔细观察，命令提示符左侧显示的是git的分支，后侧显示的执行结果状态、执行命令的数量以及时间，这是一个比较流行的第三方 PowerLevel9k 开源主题，我们将其下载到~/.oh-my-zsh/custom/themes下，详细的配置设置参考github上的文档:\n1 2  cd ~/.oh-my-zsh/custom/themes https://github.com/bhilburn/powerlevel9k.git   修改配置文件中的主题设置为:\n1  ZSH_THEME=\u0026#34;powerlevel9k/powerlevel9k\u0026#34;   然后配置git,history和time，我们在\n1 2  POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(context dir vcs) POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(status history time)   这样就完成了上图的配置。当然你可以设置一些带有icon的模式比如:\n1  POWERLEVEL9K_MODE=\u0026#39;nerdfont-complete\u0026#39;   效果如下:\n你也可以自已定义一些背景颜色，你可以通过执行一下命令获取配色\n1 2  spectrum_bls # 显示前配色 spectrum_ls # 显示后配色   美化进阶 这里就是美化终端的基本操作，下面我们看一下如何在原有的基础上配置一下自己喜欢的icon和插件：\n红色框 红色框是iterm的资源状态栏，你可以在iterm2中使用快捷键command+,或者菜单栏打开首选项，进行设置，后面就不在赘述如何带开了，\n点击configure status bar将需要的托放在下面既可\n青色的🐳 青色的🐳是一个emoji.可以利用touchbar填充上去，可以写文字\n黄色的目录 这里是安装的一些高效易用的插件\n git-open: 一个快捷工具，当你cd到一个版本化的代码仓库目录下，执行git-open会自动打开浏览器并跳转到该代码仓库地址 zsh-autosuggestions:这是一个自动提示之前执行过的命令历史 zsh-syntax-highlighting:这是一个zsh语法高亮的插件  这些插件安装配置简单，只需要clone对应的代码仓库到/Users/marionxue/.oh-my-zsh/custom/plugins下，然后在.zshrc配置文件下的plugins处配置上即可，注意我这里使用的是agnoster主题。\n1 2 3 4 5 6 7 8 9 10 11 12  cd ~/.oh-my-zsh/custom/plugins git clone https://github.com/paulirish/git-open git clone https://github.com/zsh-users/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting vim ~/.zshrc plugins=( git zsh-syntax-highlighting zsh-autosuggestions git-open )   最后是配置生效\n1  source ~/.zshrc   蓝色框内的配置 这里是通过agnoster主题加上一些自己修改的主题实现的。然后结合~/.zshrc配置命令行提示符的显示，你可以在emoji网站1，emoji网站2上找到对应的Bytes\n其中的devcluster和kube-ops是我的k8s集群的集群名以及对应的命名空间，这里是利用kubectx、kube-ps1实现的，具体的你可以参考，下面是我的部分配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  ZSH_THEME=\u0026#34;agnoster\u0026#34; DISABLE_AUTO_UPDATE=\u0026#34;true\u0026#34; DISABLE_UPDATE_PROMPT=\u0026#34;true\u0026#34; plugins=( git zsh-syntax-highlighting zsh-autosuggestions kubectl kube-ps1 git-open ) KUBE_PS1_SYMBOL_ENABLE=false KUBE_PS1_PREFIX=\u0026#34;\\u2638\\uFE0F \u0026#34; KUBE_PS1_SUFFIX=\u0026#34; \u0026#34; KUBE_PS1_DIVIDER=\u0026#34;\\xf0\\x9f\\x94\\xa5 \u0026#34; PS1=\u0026#39;$(kube_ps1)\u0026#39;$PS1   好了，到这里就结束了，希望你能有所收获~\n","description":"为什么你的终端这么美丽","id":11,"section":"posts","tags":["iterm"],"title":"手把手带你打造骚气的命令行终端","uri":"https://linuxermaster.github.io/en/posts/20200613-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%A0%E7%9A%84%E7%BB%88%E7%AB%AF%E8%BF%99%E4%B9%88%E7%BE%8E%E4%B8%BD/"},{"content":"​ 最近发现很多的微信文章上出现了一种logo设计，跟P站的logo设计风格一样让人印象深刻，黑底白字，配上一小撮橙色，给人极强的冲击力。后来查了一下在Github上发现有一个有意思的程序员弄的一个在线创意Logo生成器，利用Logoly Pro可以生成类似YouTobe、P站等Logo风格的标志。分享给大家，于是就有了今天这篇文章。\nlogoly的由来 这个风格的制作是由tencent的一位大牛开发制作的，叫做白宦成,这里是他对于这个网页版工具的自述：\n我自己平时经常要做一些 Side Project ，在做 Side Project 的时候，就涉及到了要做 Logo ，但是作为一个没有设计感的程序员，在做 Logo 时总是会做出一些很丑的 Logo ，于是痛定思痛，想想有没有什么有用的工具可以帮助我生成好看的 Logo。对于我来说，也不需要太过复杂，能够满足我自己的要求就行。\nP站Logo风格图片素材制作的官方网站：https://logoly.pro/ 代码仓库里面是这样描述的，”一个简单的在线徽标生成器，适合想要轻松设计徽标的人们\u0026rdquo;。\nlogoly的功能和特色  生成P站风格的logo 支持导出png格式的图片 支持修改前缀/后缀/背景的颜色 支持设置字体的大小 支持自定义字体TODO，可能是为了防止侵权，避免使用一些未开源的字体  logoly的使用方法  打开logoly网站: https://logoly.pro/s 在文本框内输入需要的文本 修改你需要的颜色和字体的大小 点击导出即可下载png格式的图片 还是不太懂得，可以看一下哔哩哔哩B站上别人发的文章，或者就放弃使用吧！  ","description":"一个快速生成P站风格的在线工具","id":12,"section":"posts","tags":["logo","logoly","工具"],"title":"P站风格Logo生成器","uri":"https://linuxermaster.github.io/en/posts/20200607-p%E7%AB%99%E9%A3%8E%E6%A0%BClogo%E7%94%9F%E6%88%90%E5%99%A8/"},{"content":"分享一个使用Go编写的极简单的Demo案例，为什么要分享呢？涉及到几个小知识点\n Go mod的使用 GO中如何导入包 通过Go简单的演示分层构建 演示如何编写一个自动化构建的脚本式Jenkinfile.  1. Go mod如何使用 我们准备一个apis模块，在该模块中实现一个打印字符串的函数Says(str string):所以\n1 2 3 4 5 6 7 8 9  mkdir apis cat \u0026lt;\u0026lt; EOF \u0026gt; ./apis.go package apis import \u0026#34;fmt\u0026#34; func Says(str string) { fmt.Printf(\u0026#34;hello,%s\u0026#34;, name) }   然后我们初始化一下go mod\n1 2 3 4 5 6 7  go env -w GO111MODULE=\u0026#34;auto\u0026#34; go mod init code.kubemaster.top/DevOpsTeam/apis # 此时查看一下mod文件的内容为: cat go.mod module code.kubemaster.top/DevOpsTeam/apis go 1.14   然后工作区内生成一个go.mod文件\n然后我们初始化提交到代码仓库中即可,代码仓库的地址为https://code.kubemaster.top/DevOpsTeam/apis.git，\n2. 如何导入包 接下来应该编写Demo了，Demo依赖apis模块实现功能，Demo的文件名为main.go:\n1 2 3 4 5 6 7  package main import \u0026#34;code.kubemaster.top/DevOpsTeam/apis\u0026#34; func main() { apis.Says(\u0026#34;云原生·生态圈\\n\u0026#34;) }   这里有个注意的地方:\n当程序调用的模块与模块的文件名不一致的时候，需要通过别名引入，否则直接import即可：\n1 2  # 实际上模块名为apis,但是此处文件名是api,所以需要通过别名解决 import m_api \u0026#34;code.kubemaster.top/DevOpsTeam/api\u0026#34;   当需要运行和构建main.go的时候:\n1 2  go get -insecure code.kubemaster.top/DevOpsTeam/apis go run main.go   到这里基本上就明确go mod的基础使用了,然后我们将其提交代码到代码仓库：https://code.kubemaster.top/DevOpsTeam/goci.git。\n3. 应该怎样构建Go程序 使用Docker构建镜像，首先要准备一个Dockerfile,仔细思考一下，Go编写的程序会直接编译成指定编译架构的二进制文件，所以我们可以通过分层构建的方式首先在Go的环境中进行构建，再把构建后的二进制文件拷贝到微小镜像内，以便减小镜像的体积,下面写了一个Dockerfile的案例:\n1 2 3 4 5 6 7 8 9 10 11 12  FROMgolang:1.14asbuilderWORKDIR/go/src/code.kubemaster.top/DevOpsTeam/demos/ARGARCH=\u0026#34;amd64\u0026#34;ARGOS=\u0026#34;linux\u0026#34;COPYmain.go.RUNgoget-insecurecode.kubemaster.top/DevOpsTeam/apis\u0026amp;\u0026amp;\\CGO_ENABLED=0GOOS=linuxgobuild-a-installsuffixcgo-omain.FROMalpine:latestWORKDIR/root/COPY--from=builder/go/src/code.kubemaster.top/DevOpsTeam/demos/.CMD[\u0026#34;./main\u0026#34;]  在镜像编译后，镜像的体积为7.69M,在Dockerfile准备好之后，我们把Dockerfile提交到goci的代码仓库内。下面我们就可以配置一下Jenkinsfile，以便于我们持续构建了\n4. 通过Jenkinsfile实现持续构建 这里简单的通过脚本式pipeline实现服务的持续构建，很简单，但也是一个完整的基础框架:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  pipeline{agentanyenvironment{registry=\u0026#34;registry-vpc.cn-beijing.aliyuncs.com/kubemaster/gocodecitestdemo\u0026#34;registryCredential=\u0026#39;854bfe2f-7923-48a5-9156-7be54cc38a88\u0026#39;}stages{stage(\u0026#39;Cloning Git\u0026#39;){steps{git\u0026#39;https://code.kubemaster.top/DevOpsTeam/goci.git\u0026#39;}}stage(\u0026#39;Building image\u0026#39;){steps{script{dockerImage=docker.buildregistry+\u0026#34;:$BUILD_NUMBER\u0026#34;}}}stage(\u0026#39;Testing Image\u0026#39;){steps{sh\u0026#34;docker run --rm $registry:$BUILD_NUMBER\u0026#34;}}stage(\u0026#39;Deploy Image\u0026#39;){steps{script{docker.withRegistry(\u0026#39;https://registry-vpc.cn-beijing.aliyuncs.com\u0026#39;,registryCredential){dockerImage.push()}}}}stage(\u0026#39;Remove Unused docker image\u0026#39;){steps{sh\u0026#34;docker rmi $registry:$BUILD_NUMBER\u0026#34;}}}}  在完成Jenkinsfile的准备工作后，依旧提交到goci代码仓库内，然后我们在jenkins上配置即可:\n首先准备Jenkins job配置的xml配置文件goci.xml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  \u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;flow-definition plugin=\u0026#34;workflow-job@2.36\u0026#34;\u0026gt; \u0026lt;actions\u0026gt; \u0026lt;org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobAction plugin=\u0026#34;pipeline-model-definition@1.5.0\u0026#34;/\u0026gt; \u0026lt;org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction plugin=\u0026#34;pipeline-model-definition@1.5.0\u0026#34;\u0026gt; \u0026lt;jobProperties/\u0026gt; \u0026lt;triggers/\u0026gt; \u0026lt;parameters/\u0026gt; \u0026lt;options/\u0026gt; \u0026lt;/org.jenkinsci.plugins.pipeline.modeldefinition.actions.DeclarativeJobPropertyTrackerAction\u0026gt; \u0026lt;/actions\u0026gt; \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt; \u0026lt;keepDependencies\u0026gt;false\u0026lt;/keepDependencies\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty plugin=\u0026#34;gitlab-plugin@1.5.13\u0026#34;\u0026gt; \u0026lt;gitLabConnection\u0026gt;\u0026lt;/gitLabConnection\u0026gt; \u0026lt;/com.dabsquared.gitlabjenkins.connection.GitLabConnectionProperty\u0026gt; \u0026lt;org.jenkinsci.plugins.gitlablogo.GitlabLogoProperty plugin=\u0026#34;gitlab-logo@1.0.5\u0026#34;\u0026gt; \u0026lt;repositoryName\u0026gt;\u0026lt;/repositoryName\u0026gt; \u0026lt;/org.jenkinsci.plugins.gitlablogo.GitlabLogoProperty\u0026gt; \u0026lt;com.synopsys.arc.jenkinsci.plugins.jobrestrictions.jobs.JobRestrictionProperty plugin=\u0026#34;job-restrictions@0.8\u0026#34;/\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;definition class=\u0026#34;org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition\u0026#34; plugin=\u0026#34;workflow-cps@2.78\u0026#34;\u0026gt; \u0026lt;scm class=\u0026#34;hudson.plugins.git.GitSCM\u0026#34; plugin=\u0026#34;git@4.1.1\u0026#34;\u0026gt; \u0026lt;configVersion\u0026gt;2\u0026lt;/configVersion\u0026gt; \u0026lt;userRemoteConfigs\u0026gt; \u0026lt;hudson.plugins.git.UserRemoteConfig\u0026gt; \u0026lt;url\u0026gt;https://code.kubemaster.top/DevOpsTeam/goci.git\u0026lt;/url\u0026gt; \u0026lt;credentialsId\u0026gt;73a21ee2-2cdb-4658-8f99-309a3b77f2d4\u0026lt;/credentialsId\u0026gt; \u0026lt;/hudson.plugins.git.UserRemoteConfig\u0026gt; \u0026lt;/userRemoteConfigs\u0026gt; \u0026lt;branches\u0026gt; \u0026lt;hudson.plugins.git.BranchSpec\u0026gt; \u0026lt;name\u0026gt;*/master\u0026lt;/name\u0026gt; \u0026lt;/hudson.plugins.git.BranchSpec\u0026gt; \u0026lt;/branches\u0026gt; \u0026lt;doGenerateSubmoduleConfigurations\u0026gt;false\u0026lt;/doGenerateSubmoduleConfigurations\u0026gt; \u0026lt;submoduleCfg class=\u0026#34;list\u0026#34;/\u0026gt; \u0026lt;extensions/\u0026gt; \u0026lt;/scm\u0026gt; \u0026lt;scriptPath\u0026gt;Jenkinsfile\u0026lt;/scriptPath\u0026gt; \u0026lt;lightweight\u0026gt;true\u0026lt;/lightweight\u0026gt; \u0026lt;/definition\u0026gt; \u0026lt;triggers/\u0026gt; \u0026lt;disabled\u0026gt;false\u0026lt;/disabled\u0026gt;   然后，我们就可以创建构建job和执行触发构建了:\n1 2 3 4 5 6  # 获取jenkins-crumb crumb=$(curl -u \u0026#34;admin:admin\u0026#34; -s \u0026#39;http://jenkins.kubemaster.top/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,\u0026#34;:\u0026#34;,//crumb)\u0026#39;) # 创建job curl -X POST \u0026#34;http://admin:admin@jenkins.kubemaster.top/createItem?name=goci\u0026#34; --data-binary \u0026#34;@goci.xml\u0026#34; -H \u0026#34;Content-Type: text/xml\u0026#34; -H ${jenkins_crumb} # 执行job curl -X POST -u admin:admin -H ${jenkins_crumb} http://jenkins.kubemaster.top/job/goci/build   看到这里，基本上就结束了，基本上在实践中完成了Demo的编写，容器化构建，持续构建相关环节，下面我们看一下构建的结果：\n构建的视图:\n完成的构建的日志:\n","description":"分享一个使用`Go`编写的极简单的Demo案例","id":13,"section":"posts","tags":["go","jenkins","持续集成"],"title":"Go语言通过Jenkins实现CICD","uri":"https://linuxermaster.github.io/en/posts/20200606-go%E8%AF%AD%E8%A8%80%E9%80%9A%E8%BF%87jenkins%E5%AE%9E%E7%8E%B0cicd/"},{"content":"在微服务体系的建设中，单体服务的拆分使微服务变得越来越多，服务之间的依赖关系也越来越复杂，需要解决配置集中管理、配置变更后的自动刷新或者重载甚至需要实现动态配置。动态配置是什么样的，我想接触过Traefik和Gitlab Runner的同学应该都会有感触，虽然实现的原理有些不一样，但是都达到一种无须认为过多干预即实现服务的重载生效。\n我们本文会通过consul、consul-template、registrator和Docker等模拟一个使用consul作为服务发现，动态渲染nginx的配置文件实现upstream服务健康状态检查后自动的注册与下线。\n首选介绍一下这个工具的作用：\nconsul：是一款服务配置和发现工具，分布式的、高可用的以及极具扩展性。\nconsul-template:是一个从consul中获取k/v信息，然后将值v填充到基于go template语法的文件中。\nregistrator：是一个与docker.sock通信，将running着的Docker container自动的注册到consul的一个服务注册工具\n下面通过一个实际的例子来探索consul与consul-template结合解决那些微服务模式下动态发现、自动刷新的问题。\n首先通过docker运行一个consul节点，用于记录registrator发现的容器信息:\n1 2 3 4 5 6 7 8 9 10 11 12 13  docker run -d --name=consul --restart=always \\  -e \u0026#39;CONSUL_LOCAL_CONFIG={\u0026#34;skip_leave_on_interrupt\u0026#34;: true, \u0026#34;ui\u0026#34;: true, \u0026#34;dns_config\u0026#34;: { \u0026#34;allow_stale\u0026#34;: false }}\u0026#39; \\  -p 8300:8300 \\  -p 8301:8301 \\  -p 8301:8301/udp \\  -p 8302:8302/udp \\  -p 8302:8302 \\  -p 8400:8400 \\  -p 8500:8500 \\  -p 8600:8600 \\  -h node1 \\  consul agent -server -bind=192.168.99.129 -bootstrap-expect=1 -node=node1 \\  -client 0.0.0.0 -ui   ","description":"学习Consul的基础知识","id":14,"section":"posts","tags":["服务发现","分布式","consul"],"title":"Consul在微服务中的角色","uri":"https://linuxermaster.github.io/en/posts/20200605-consul%E5%9C%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E8%A7%92%E8%89%B2/"},{"content":"了解过Traefik,nginx-ingress的同学都知道他们的官方文档都是基于mkdocs和material主题制作而成，你觉得这种文档库怎么样？有没有心动把自己的文档也整成那样的？，下面我们直接动手干起来吧。\n配置pip国内的下载源\n1 2 3  ☸️ devcluster🔥 kube-ops ~  🐳 👉 cat ./.pip/pip.conf [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple   安装mkdocs\n1 2 3 4 5 6  ☸️ devcluster🔥 kube-ops ~  🐳 👉 pip3 install mkdocs Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Requirement already satisfied: mkdocs in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.1.2) ... WARNING: You are using pip version 19.2.3, however version 20.1.1 is available. You should consider upgrading via the \u0026#39;pip install --upgrade pip\u0026#39; command.   创建并运行一个新的文档库\n1 2 3 4  ☸️ devcluster🔥 kube-ops ~  🐳 👉 mkdocs new cloudnativecosystem ☸️ devcluster🔥 kube-ops ~  🐳 👉 cd cloudnativecosystem ☸️ devcluster🔥 kube-ops ~/cloudnativecosystem  🐳 👉 ls docs mkdocs.yml   创建一个新的GIT仓库https://github.com/linuxermaster/cloudnativecosystem_mkdocs\n配置通过SSH无密访问代码仓库\n1  ssh-keygen -t rsa -C \u0026#34;email@qq.com\u0026#34;   然后将公钥拷贝到GITHUB的SSH-KEY中\n测试本地是否可以远程连接\n1 2  # ssh -T git@github.com Hi linuxermaster! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access.   然后需要在cloudnativecosystem目录内初始化.git仓库，然后添加远程仓库地址\n1 2 3 4 5  cd cloudnativecosystem git init git remote add origin https://github.com/linuxermaster/cloudnativecosystem_mkdocs.git mkdocs build --clean # 创建编译后的静态页面以及样式文件 mkdocs gh-deploy --clean # push到gh-deploy分支下   此时，即可通过https://linuxermaster.github.io/cloudnativecosystem_mkdocs/访问\n1  httpstat https://linuxermaster.github.io/cloudnativecosystem_mkdocs/   现在开始优化mkdocs以及配置主题样式了，这部分工作都在mkdocs.yml中完成\n1 2 3 4 5 6 7 8 9 10 11  site_name:云原生生态圈nav:- 主页:\u0026#34;index.md\u0026#34;- 关于:\u0026#34;about.md\u0026#34;- 自动化:\u0026#34;devops.md\u0026#34;theme:materialrepo_url:https://github.com/linuxermaster/cloudnativecosystem_mkdocs.gitrepo_name:\u0026#34;Opening on Github\u0026#34;site_description:\u0026#34;这是一个mkdocs的demo测试知识库网站\u0026#34;site_author:\u0026#34;Marionxue\u0026#34;copyright:\u0026#34;1994 - 2020\u0026#34;  其中theme指定的是你的主题，这个主题就是我们常见到的traefik,nginx-ingress等在使用的官方文档的主题，如果使用它，你可能还需要额外的安装一下\n1  pip3 install mkdocs-material   安装之后，执行mkdocs gh-deploy --clean即可访问网站的主题:\n每次手动部署都是比较麻烦的，于是我们利用永久免费的travis来帮助解决这个问题:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  language:python# Set the build language to Pythonpython:3.6# Set the version of Python to usebranches:master# Set the branch to build frominstall:- pipinstallmkdocsmkdocs-materialpymdown-extensionspygments# Install the required dependenciesscript:true# Skip script (Don\u0026#39;t use this if one already exists)before_deploy:- mkdocsbuild--verbose--clean--strict# Build a local version of the docsdeploy:# Deploy documentation to Github in the gh_pages branchprovider:pagesskip_cleanup:truegithub_token:$GITHUB_API_KEYlocal_dir:siteon:branch:master  然后我们使用GITHUB的账号登录travis, 地址是:https://travis-ci.org，在登录进去之后，我们选择合适的启用CICD\n然后点击setting进入仓库的配置界面，设置以下三个环境变量\n其中,GITHUB_API_KEY是从gitub上获取的Access Token,剩下的两个是用户名和密码。完成之后，我们就可以手动的触发以下构建:\n紧接着，我们为了方便我们知道构建是否完成，我们在README.md文件上添加了一个构建的状态展示:\n复制RESULT信息到readme.md中，即可显示每次构建的状态信息:\n查看一下我们的构建历史\n","description":"使用mkdocs构建文档库并通过github和travis实现自动构建和发布","id":15,"section":"posts","tags":["mkdocs","github","travis"],"title":"mkdocs自动化部署知识库","uri":"https://linuxermaster.github.io/en/posts/20200604-mkdocs%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"content":"Horizontal Pod Autoscaler 可以根据 CPU 利用率自动伸缩 replication controller、deployment 和 replica set 中的Pod数量（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets\nPod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。\n通过实际的 Demo 开体验一下HPA基于资源实现的水平自动伸缩,为了演示 Horizontal Pod Autoscaler，我们将使用tomcat镜像作为测试对象，以下为配置 tomcat 的deployment的配置清单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  apiVersion:apps/v1kind:Deploymentmetadata:name:dp-tomcatnamespace:learningspec:revisionHistoryLimit:15replicas:1selector:matchLabels:app:dp-tomcatmatchExpressions:- key:appoperator:Invalues:[dp-tomcat]template:metadata:labels:app:dp-tomcatspec:nodeSelector:kubernetes.io/hostname:dev-k8s-05.xsl.linkcontainers:- name:dp-tomcatimage:tomcat:9.0resources:limits:memory:\u0026#34;1024Mi\u0026#34;cpu:\u0026#34;500m\u0026#34;ports:- containerPort:8080name:web  上面的 yaml 说明一下:\n 运行了一个副本数为 3 的 deployment 暴露出一个名为 nginxsvc 的 service 指定了revisionHistoryLimit表示保留历史版本的个数 因为要通过资源负载模拟 HPA 功能的场景，必须依赖 template 中对 pod 做资源限制 集群环境需要安装 metrics-server,方便观察集群资源负载  1 2 3 4 5 6 7 8  ➜ 00-template.resources.yaml git:(master) ✗ (☸ kubernetes-admin@kubernetes:default) k get pods -n learning NAME READY STATUS RESTARTS AGE abcontainer-58c67cfb87-g4j8q 1/1 Running 0 25d dp-tomcat-5b4465b6bf-82llc 1/1 Running 0 125m dp-tomcat-5b4465b6bf-zsttg 1/1 Running 0 126m nginx-deployment-8558b4659-6dkmr 1/1 Running 0 17s nginx-deployment-8558b4659-v8fzf 1/1 Running 0 17s nginx-deployment-8558b4659-w5tlq 1/1 Running 0 8m56s   现在，dp-tomcat服务已经运行，我们将通过 定义一个 HPA 资源对象来创建 Horizontal Pod Autoscaler。 以下配置清单将创建一个Horizontal Pod Autoscaler用于控制我们上一步骤中创建的 deployment，使 Pod 的副本数量在维持在 1 到 10 之间。 大致来说，HPA 将通过增加或者减少 Pod 副本的数量（通过 Deployment ）以保持所有 Pod 的平均 CPU 利用率在 50%以内\n定义一个 HPA 资源对象\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: dp-tomcat namespace: learning spec: minReplicas: 1 maxReplicas: 10 scaleTargetRef: kind: Deployment name: dp-tomcat apiVersion: apps/v1 targetCPUUtilizationPercentage: 10   接下来，我们通过部署一个含有ab命令的httpd镜像来模拟负载\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  apiVersion: apps/v1 kind: Deployment metadata: name: abcontainer namespace: abcontainer spec: selector: matchLabels: app: abcontainer template: metadata: labels: app: abcontainer spec: containers: - name: abcontainer image: httpd ports: - containerPort: 80   在部署完成 httpd 之后，进入到 abcontainer 通过 ab 增加负载\n1  k exec abcontainer-58c67cfb87-g4j8q -n learning -it -- bash   使用ab命令进行压力测试\n1  ab -c 5000 -n 2000000 http://tomcat-svc:8080/   查看 HPA 资源变化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE dp-tomcat Deployment/dp-tomcat 0%/10% 1 10 1 16m NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE dp-tomcat Deployment/dp-tomcat 98%/10% 1 10 1 18m # 中断ab测试 NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE dp-tomcat Deployment/dp-tomcat 36%/10% 1 10 10 19m # pods的变化 NAME READY STATUS RESTARTS AGE abcontainer-58c67cfb87-g4j8q 1/1 Running 0 25d dp-tomcat-96b7b9f6f-54v84 0/1 Pending 0 13s dp-tomcat-96b7b9f6f-55hsf 0/1 ContainerCreating 0 13s dp-tomcat-96b7b9f6f-btbzr 0/1 ContainerCreating 0 13s dp-tomcat-96b7b9f6f-dpppj 0/1 ContainerCreating 0 13s dp-tomcat-96b7b9f6f-k4jqq 1/1 Running 0 12m dp-tomcat-96b7b9f6f-mxd4f 1/1 Running 0 28s dp-tomcat-96b7b9f6f-tjpj6 1/1 Running 0 28s dp-tomcat-96b7b9f6f-zztqm 1/1 Running 0 28s   默认情况下,当ab模拟完成之后，当流量开始下降，5 分钟后 pod 的数量会慢慢恢复到 replicas 的值。\n更多精彩文章  微软·kubernetes 学习指南 v3.0 Jenkins 在 kubernetes 上的落地实践 OpenTracing 和 Jaeger 在 kubernetes 的应用 kubernetes 核心监控框架 Metrics-server  ","description":"Horizontal Pod Autoscaler通过Pod资源使用情况自动扩展","id":16,"section":"posts","tags":["Kubernetes","HPA","ab","压测测试"],"title":"Ab压力测试模拟实现Kubernetes Pod水平自动伸缩","uri":"https://linuxermaster.github.io/en/posts/20200530-ab%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E6%A8%A1%E6%8B%9F%E5%AE%9E%E7%8E%B0kubernetes-pod%E6%B0%B4%E5%B9%B3%E8%87%AA%E5%8A%A8%E4%BC%B8%E7%BC%A9/"},{"content":"autojump autojump 是一款非常方便的命令行下的目录跳转工具，它能帮你快速从目录访问的历史记录中统计出各个目录的访问频次和权重，这样，就能方便的让你在各个目录中迅速跳转了。只要你记得之前某个访问过的目录的大概名字，配合 autojump，就能快速的跳转过去，再也不用打一长串的 cd 命令了。使用方式如下：\n安装 1 2 3 4 5  brew install autojump # 大多数mac系统都会安装zsh,因此当安装配置好autojump之后，需要将以下代码写入到~/.zshrc [[ -s `brew --prefix`/etc/autojump.sh ]] \u0026amp;\u0026amp; . `brew --prefix`/etc/autojump.sh # 最后在source一遍 source ~/.zshrc   tig Git 已经成为我们平时经常用到的版本控制管理工具。通常，我们用 git log 命令来查看 git 提交的历史记录。如果你已经厌倦了 git log 那种千篇一律的界面，那么 tig 绝对是一个不可错过的命令行下查看 git 历史提交记录的工具\ntig 的界面看起来比起 git log 要酷炫不少，而且使用起来也挺方便。此外，tig 的默认按键绑定还跟 Vim 比较类似，真是 Vimer 的福音。\n1  brew install tig   git summary 平时我们在多人合作开发一个项目的时候，想要大致了解一下每个人对这个项目提交的 commit 数量和大致的贡献度，那么 git summary 这个命令绝对能满足你的要求\ngit summary 通过对项目中每一个 commit 的统计，能大致计算出每个 contributor 的提交次数和贡献百分比。让你对这个项目的贡献度能一目了然。\n默认 git 是不会带有这个命令的，需要在 Mac OS 下额外安装一个扩展包：\n1  brew install git-extras   the_silver_searcher UNIX/Linux 系统有不少好用的工具，用于文本搜索的 grep 或许是其中最常用的工具之一。尽管平时称心如意，在面对数百万行的代码库时，grep 的用户体验实在堪忧。还好，我们有 the silver searcher （即 ag）这样迅捷的替代品\n  在文本中搜索指定的字符串,显示包含字符串的行\n1  ag \u0026#34;password\u0026#34; ./tagret.file     显示含有指定字符串的文件名\n1  ag -l \u0026#39;password\u0026#39; .     tmux 当我们在远程执行命令的时候，经常性的会因为网络异常打嗝造成回话断开，命令执行失败，此时你就需要一个 Tmux，它是一个终端复用器（terminal multiplexer)。这样即使你远程连接服务器的笔记本断网了，也不会终端你在服务器上执行的命令；\n 它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用 它可以让新窗口\u0026quot;接入\u0026quot;已经存在的会话 它允许每个会话有多个连接窗口，因此可以多人实时共享会话。 它还支持窗口任意的垂直和水平拆分。  jq json 文件处理以及格式化显示，支持高亮，可以替换 python -m json.tool\nhttpstat HTTP 响应的可视化命令行工具,请求含有http/https前缀的 url,支持所有curl支持的除了-w,-D,-o,-S,-s之外的所有选项。\nthefuck 命令行打错了以后，打一个fuck就会自动纠正。\nshellcheck ShellCheck，用于 Shell 脚本的静态分析工具，在网页上检查你的脚本：https://www.shellcheck.net/ ,shellcheck 具体会检查一些什么问题呢，以下给出一个不完整的问题检查列表\n 引号问题 条件判断 ShellCheck 可以识别大多数不正确的条件判断语句 常见的对命令的错误使用 ShellCheck 识别很多初学者的语法错误 ShellCheck 可以提出一些风格改进建议 ShellCheck 可以识别一些数据和拼写错误 ShellCheck 可以做出一些增强脚本鲁棒性的建议 ShellCheck 警告你使用了 shebang 不支持的特性. ShellCheck 可以识别到一些其他问题  glances top/htop`的替代方案，官网地址:https://nicolargo.github.io/glances/\n","description":"十款 常用终端的技术人员必不可少的便捷工具","id":17,"section":"posts","tags":["Termianl","工具"],"title":"技术人员最常用的命令行工具","uri":"https://linuxermaster.github.io/en/posts/20200531-%E6%8A%80%E6%9C%AF%E4%BA%BA%E5%91%98%E6%9C%80%E5%B8%B8%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7/"},{"content":"Sample images from Pixabay\n","description":"cartoon gallery","id":18,"section":"gallery","tags":null,"title":"Cartoon","uri":"https://linuxermaster.github.io/en/gallery/cartoon/"},{"content":"Sample images from Pixabay\n","description":"photo gallery","id":19,"section":"gallery","tags":null,"title":"Photo","uri":"https://linuxermaster.github.io/en/gallery/photo/"},{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","description":"Hugo, the world’s fastest framework for building websites","id":24,"section":"","tags":null,"title":"About","uri":"https://linuxermaster.github.io/en/about/"},{"content":"使用场景 Jenkins Shared Libraries是一种在Jenkinsfile语法的基础上扩展Jenkins Pipeline的技术。通过编写Jenkins Shared Libraries可以实现将自定义的Steps当成模板，对流水线逻辑中重复或可以通用的部分进行抽象和封装，在使用的时候将其import进去即可。这样可以大大的减少了我们项目jenkinsfile的复杂度。在实际的工作使用中很常用，看看Jenkins共享库能帮我们解决的问题：\n 服务团队自动化的逐渐推进，使用pipeline的job越来越多，大多情况下每个项目都需要编写从别人那里拷贝一份完整的Jenkinsfile，很麻烦，如果你的公司正在转变到微服务,那么你肯定会感受到这种不变(这是微服务的锅) 不熟悉jenkins pipeline语法的人，随意更改就会造成各种错误，给运维造成负担。 每个项目下有一个Jenkinsfile就会造成修改Jenkinsfile也会触发一次不必要的服务构建。 修改一些全局工具的时候需要告知使用到该工具的所有人去修改jenkinsfile脚本。 \u0026hellip;  在实践中每个服务团队/Devops团队都应该通过维护一个或多个Shared Libraries项目再结合第三方的Jenkins插件定制团队自己的Jenkins流水线。只要认真维护share libraries这一个远程仓库就能解决以上的所有问题。在使用过程中，我们将share library存在一个git版本控制仓库里面，在需要的时候，将它引入到当前模快即可。\n在实际使用的时候，我们可以通过Github查看是否有别人写好的一些共享库，适合的我们可以直接给那过来，修改成适合自己的模板库。Jenkins的灵活取决于使用groovy语法，而Jenkins的模板库实际上就是通过groovy语言编写的一些基础的类文件，存在固定格式的目录结构中。\n1 2 3 4 5 6 7 8 9 10 11 12  (root) +- src # Groovy源文件 | +- org | +- foo | +- Bar.groovy # org.foo.bar的类文件 +- vars | +- foo.groovy # foo是一个全局变量文件，是可以从Pipeline中全局访问的 | +- foo.txt # 全局变量foo的说明帮助文件 +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar   在共享库的目录结构中需要注意的是src和vars目录，其中src里面存放的是可以在pipeline中调用的groovy类方法，在执行的时候默认会将其添加到java的class_Path环境变量指定的路径下，vars主要是一些脚本文件，在pipeline运行的时候被引用。想要灵活的使用Jenkins share library，可能你需要掌握groovy的基础语法。\n1    ","description":"","id":28,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200611-jenkins%E5%85%B1%E4%BA%AB%E5%BA%93jenkins-share-library%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"content":"配置MYSQL数据库 运行MySQL容器 1 2 3 4  docker container run -d --restart=always -v /data/mysql/my.cnf:/etc/mysql/conf.d/my.cnf \\ -v /data/mysql/mysql:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=PASSWORD \\ -p 33306:3306 mysql:5.7   SonarQube配置信息 1 2 3 4 5 6 7 8  mkdir -p /data/sonarqube docker run -d --name sonarqube -p 9000:9000 -p 9092:9092 sonarqube:7.7-community docker cp sonarqube:/opt/sonarqube/conf /data/sonarqube/ docker cp sonarqube:/opt/sonarqube/extensions /data/sonarqube/ wget -P /data/sonar/extensions/plugins https://github.com/SonarQubeCommunity/sonar-l10n-zh/releases/download/sonar-l10n-zh-plugin-1.27/sonar-l10n-zh-plugin-1.27.jar docker stop sonarqube docker rm sonarqube chown -R suoper.suoper /data/sonarqube   创建SonarQube数据库信息 1 2 3  CREATE DATABASE sonar CHARACTER SET utf8 COLLATE utf8_general_ci; GRANT ALL ON sonar.* TO \u0026#39;sonar\u0026#39;@\u0026#39;192.168.128.%\u0026#39; IDENTIFIED BY \u0026#39;sonar\u0026#39;; FLUSH PRIVILEGES;   配置SonarQube的Ldap 1 2 3 4 5 6 7  root@jenkins-master-128-93:/data/sonarqube/conf# cat sonar.properties | egrep -v \u0026#39;^$|^#\u0026#39; sonar.security.realm=LDAP sonar.authenticator.downcase=true ldap.url=ldap://192.168.128.92:389 ldap.bindDn=cn=gitlab,ou=People,dc=openldap,dc=xingshulin,dc=com ldap.bindPassword=ioPg20o5! ldap.user.baseDn=ou=People,dc=openldap,dc=xingshulin,dc=com   启动SonarQube服务 1  docker run -d --restart=always --name sonarqube -p 9000:9000 -e sonar.jdbc.username=root -e sonar.jdbc.password=\u0026#39;123456\u0026#39; -e sonar.jdbc.url=\u0026#34;jdbc:mysql://192.168.128.93:33306/sonar?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;rewriteBatchedStatements=true\u0026amp;useConfigs=maxPerformance\u0026amp;useSSL=false\u0026#34; -v /data/sonarqube/extensions:/opt/sonarqube/extensions -v /data/sonarqube/conf:/opt/sonarqube/conf sonarqube:7.7-community   配置域名解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  server { listen 80; server_name quality.xsl.link; access_log logs/access.log json; location / { proxy_pass http://192.168.128.93:9000; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; # allow 119.57.91.170; # allow 47.94.96.138; # allow 123.56.30.91; # allow 192.168.128.0/24; # deny all;  } }   ","description":"","id":29,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200617-sonarqube7.7%E7%A4%BE%E5%8C%BA%E7%89%88%E5%AE%89%E8%A3%85/"},{"content":"场景 最近发现一些朋友想要跳槽，正值疫情，也不知道现在市场的如何，同时目前的IT行业更是越来越难,技术革新越来越快，对新的岗位的需求也是不断的变化，因此就会想知道现在的应聘岗位对面试者的要求有哪些，各地的某个岗位薪资范围大概是多少等信息时候，我们就需要到某个招聘网站上不断的刷页面，看数据，但是简单的想一下，可以通过Python脚本来批量的分析招聘网站上各个岗位在不同城市的需求，高效的快捷的方便我们掌握大致的方向。\n实现 如何获取数据，需要掌握基本的Python爬虫知识，[requests](https://requests.readthedocs.io/en/master/ Requests)模块就可以搞定了，在爬取数据之后，将其存在Excel中，因此需要[xlwt](https://xlwt.readthedocs.io/en/latest/ xlwt)模块处理，当然在诸多的Python模块中，你可以选择你喜欢的，毕竟能抓老鼠的猫都是好猫。\n1 2  xlwt 1.3.0 requests 2.18.4   下面我们就拿拉钩网站为例，思考和获取部分的数据作为个人简单的分析参考，脚本中没有涉及到隐私数据信息，大可放心，同时也是为了找工作的小伙伴们提供一下参考的方向：\n注意：\n 脚本中获取的是通过指定的页的数量获取全国各城市的岗位信息，你可以修改FetchData方法中的referer和请求地址中城市的值，以便获取你需要的目标城市的岗位信息 如果获取的比较频繁的话，可能会出现以下情况，这里你可以通过设置代理的方式解决，免费的代理IP网站上有很多，你可以参考这篇获取代理的文章。  在交互式输入需要获取的页数之后，爬取的数据将会存储在当前执行位置下的data.xls。  下面就简单的提供一下写好的Python脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79  #!/usr/bin/env python3.4 # encoding: utf-8 \u0026#34;\u0026#34;\u0026#34; Created on 2020-06-26 @title: \u0026#39;爬去网站的招聘信息\u0026#39; @author: marionxue \u0026#34;\u0026#34;\u0026#34; import requests import xlwt # 获取存储职位信息的json对象，遍历获得公司名、福利待遇、工作地点、学历要求、工作类型、发布时间、职位名称、薪资、工作年限 def FetchData(url, datas): my_headers = { \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\u0026#34;, \u0026#34;Referer\u0026#34;: \u0026#34;https://www.lagou.com/jobs/list_Python?city=%E5%85%A8%E5%9B%BD\u0026amp;cl=false\u0026amp;fromSearch=true\u0026amp;labelWords=\u0026amp;suginput=\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/x-www-form-urlencoded;charset = UTF-8\u0026#34; } ses = requests.session() # 获取session ses.headers.update(my_headers) # 更新头部信息 ses.get(\u0026#34;https://www.lagou.com/jobs/list_%E9%83%91%E5%B7%9Ejava?city=%E5%85%A8%E5%9B%BD\u0026amp;cl=false\u0026amp;fromSearch=true\u0026amp;labelWords=\u0026amp;suginput=\u0026#34;) content = ses.post(url=url, data=datas) result = content.json() info = result[\u0026#39;content\u0026#39;][\u0026#39;positionResult\u0026#39;][\u0026#39;result\u0026#39;] info_list = [] for job in info: information = [] information.append(job[\u0026#39;positionId\u0026#39;]) # 岗位对应ID information.append(job[\u0026#39;city\u0026#39;]) # 岗位对应城市 information.append(job[\u0026#39;companyFullName\u0026#39;]) # 公司全名 information.append(job[\u0026#39;companyLabelList\u0026#39;]) # 福利待遇 information.append(job[\u0026#39;district\u0026#39;]) # 工作地点 information.append(job[\u0026#39;education\u0026#39;]) # 学历要求 information.append(job[\u0026#39;firstType\u0026#39;]) # 工作类型 information.append(job[\u0026#39;formatCreateTime\u0026#39;]) # 发布时间 information.append(job[\u0026#39;positionName\u0026#39;]) # 职位名称 information.append(job[\u0026#39;salary\u0026#39;]) # 薪资 information.append(job[\u0026#39;workYear\u0026#39;]) # 工作年限 info_list.append(information) return info_list def main(): page = int(input(\u0026#39;请输入你要抓取的页码总数：\u0026#39;)) info_result = [] title = [\u0026#39;岗位id\u0026#39;, \u0026#39;城市\u0026#39;, \u0026#39;公司全名\u0026#39;, \u0026#39;福利待遇\u0026#39;, \u0026#39;工作地点\u0026#39;, \u0026#39;学历要求\u0026#39;, \u0026#39;工作类型\u0026#39;, \u0026#39;发布时间\u0026#39;, \u0026#39;职位名称\u0026#39;, \u0026#39;薪资\u0026#39;, \u0026#39;工作年限\u0026#39;] info_result.append(title) for x in range(1, page + 1): url = \u0026#39;https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\u0026#39; datas = { \u0026#39;first\u0026#39;: \u0026#39;false\u0026#39;, \u0026#39;pn\u0026#39;: x, \u0026#39;kd\u0026#39;: \u0026#39;devops工程师\u0026#39;, } try: info = FetchData(url, datas) info_result = info_result + info print(\u0026#34;第%s页数据已采集\u0026#34; % x) except Exception as msg: print(\u0026#34;第%s页数据采集出现问题\u0026#34; % x) # 创建workbook,即excel workbook = xlwt.Workbook(encoding=\u0026#39;utf-8\u0026#39;) # 创建表,第二参数用于确认同一个cell单元是否可以重设值 worksheet = workbook.add_sheet(datas[\u0026#34;kd\u0026#34;], cell_overwrite_ok=True) for i, row in enumerate(info_result): # print(row) for j, col in enumerate(row): worksheet.write(i, j, col) workbook.save(\u0026#39;data.xls\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main()   当我们需要查看某个岗位的时候，我们只需要在58行处修改岗位的名称即可，然后输入你要采集多少页的数据即可，这样很快就会将数据采集并且存储在Excel表中\n数据显示 数据基本上完成采集，当然对于自己有需要的话，还可以继续完善啊，😆\n","description":"","id":30,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200626-python%E5%88%86%E6%9E%90%E5%BA%94%E8%81%98%E5%B2%97%E4%BD%8D%E6%9D%A1%E4%BB%B6%E4%BF%A1%E6%81%AF/"},{"content":"杏树林信息技术（北京）有限公司成立于2011年，是国内知名的互联网医疗企业。杏树林的初心是让医生行医更轻松，让医疗更高效，将“协和三宝”（图书馆、病案室、老教授）变成中国医生的三宝。旗下APP产品“病历夹”和“医口袋”，为医生提供专业内容和临床工具，用户覆盖37%的中国医生群体。公司被国际知名创业媒体Fast Company评为2015年全球最具创新力医疗企业，并先后获得蓝驰、真格、开物、双湖、健康元等机构的4轮投资。2016年起，杏树林率先打造了以病历为核心的一站式全场景医药营销云平台，致力于推动医药企业合规专业化营销转型，为医药产品提供基于病历的全场景营销服务，解决了医药企业合规、精准、有效学术营销的诉求。杏树林还成立了国内第一个“病历营销研究院”，总结病历营销的方法论和最佳实践，帮助医药企业落地基于病历营销的专业化转型。同时，通过杏树林互联网医院，已经形成了“患者招募-线上诊疗-送药上门-慢病管理”的闭环服务。截至当前，杏树林已经与中国市场上数十家领先医药工业、商业企业以及医疗器械企业达成长期业务合作，服务了一百多个医药产品。外资药企十强与国内工业十强超过半数是杏树林的客户。\n初级前端开发工程师(8k-15k) 【岗位职责】  新业务功能开发和维护; 负责定位并解决现有模块存在的问题; 持续优化页面，保证网站的高性能和可维护性;  【任职要求】 能力要求:\n 本科及以上学历，1 年以上互联网工作经验，可接受能力优秀的应届毕业生、在校实习 生; 有代码洁癖，对新技术有热情，关注技术发展方向; 极强的责任心，追求完美的习惯，刨根问底的精神; 有团队精神，能够积极主动推动团队更好的发展; 乐于技术的分享。  技术要求\n 熟练掌握 HTML5/CSS3/JavaScript 相关技能; 熟悉基本的计算机网络概念，熟悉 HTTP 协议，了解 TCP/IP 的基本工作原理，熟悉各 种 Web 标准规范; 使用过至少一种 JS 框架，React 或 Vue 等; 熟悉 hybird 开发，能解决跨浏览器和不同分辨率移动设备兼容性; 熟悉前端自动化和工程化，使用过 Webpack 或 Gulp 等常见构建工具。 熟悉 Node 开发，熟练使用 Git; 有小程序开发经验者优先、有 github 开源项目者优先;  中级前端开发工程师(12k-25k): 【岗位要求】  新业务功能开发和维护; 负责 Node 框架的性能优化和维护; 3、负责定位并解决现有模块存在的问题; 4、持续优化页面，保证网站的高性能和可维护性; 5、规范和文档的编写、维护。  【任职要求】 能力要求:\n 本科以上，2 年以上互联网工作经验; 能够发现团队里的问题，提出方案，并落地; 有代码洁癖，技术全面，对新技术有热情，关注技术发展方向; 极强的责任心，追求完美的习惯，刨根问底的精神; 有团队精神，能够积极主动推动团队更好的发展; 乐于技术的分享。  技术要求:\n  2 年以上 Web 前端工作经验，熟练掌握 HTML5/CSS3/JavaScript 相关技能;\n  熟悉基本的计算机网络概念，熟悉 HTTP 协议，了解 TCP/IP 的基本工作原理，熟悉各 种 Web 标准规范;\n  熟练使用至少一种 JS 框架，React 或 Vue 等，掌握其原理，能独立开发常用组件;\n  熟悉 hybird 开发，能解决跨浏览器和不同分辨率移动设备兼容性;\n  熟悉前端自动化和工程化，对 Webpack、Gulp 等常见构建工具有自己的认知与理解。\n  熟悉 Node 开发，熟练使用 Git;\n  有小程序开发经验者优先、有 github 开源项目者优先;\n  高级前端开发工程师(20k-35k) 【岗位职责】\n 新业务功能开发和维护; 负责框架的性能优化和维护; 负责定位并解决现有模块存在的问题; 持续优化页面，保证功能的高性能和可维护性; 规范和文档的编写、维护; 参与团队的技术沉淀、与团队一起成长。  【任职要求】 技术要求:\n 精通 CSS 、JavaScript 等前端技术; 熟悉基本的计算机网络概念，熟悉 HTTP 协议，了解 TCP/IP 的基本工作原理，熟悉各种 Web 标准规范; 使用过 React 或 Vue 等前端框架，掌握其原理，能独立开发常用组件; 对前端开发生态中的知识有基本了解(自动化构建、组件化、性能、安全等); 对中后台业务组件化开发有充分理解和实践; 对 web 开发场景常用的服务端、运维领域的相关技术有一定了解; 7、有服务端开发经验、其他语言经验优先。  能力要求:\n 本科及以上学历，5 年及以上 Web 前端工作经验; 能够发现团队里的问题，提出方案，并落地; 有代码洁癖，技术全面，对新技术有热情，关注技术发展方向; 极强的责任心，追求完美的习惯，刨根问底的精神; 有团队精神，能够积极主动推动团队更好的发展; 良好的沟通协调能力，在团队中能够扮演一定的技术管理能力。  员工福利  健康：提供全科医生问诊服务及定制化年度体检，全方位解决员工及家属的各级医疗需求，为员工及家人健康保驾护航。 激励：有吸引力的业绩奖励，每年2次调薪机会。 保险：六险一金（养老保险、医疗保险、工伤保险、失业保险、生育保险、住房公积金及补充医疗保险） 活动：团建经费、各种俱乐部：羽毛球、足球、篮球、游泳、滑雪、骑行等。 关怀：从心到身的全面关怀，读书分享、内部培训、餐补、下午茶饮料、水果零食、按摩椅等 其他：技术分享、免费的下午茶饮料等\u0026hellip;  加入我们 如果您对以上的岗位感兴趣，您可以扫描以下的二维码加我，直接将您的建立送达HR处。\n","description":"","id":31,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200628-%E6%9D%8F%E6%A0%91%E6%9E%97%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E6%8B%9B%E8%81%98/"},{"content":"kuboard Kuboard 是一款免费的 Kubernetes 管理工具，提供了丰富的功能，结合已有或新建的代码仓库、镜像仓库、CI/CD工具等，可以便捷的搭建一个生产可用的 Kubernetes 容器云平台，轻松管理和运行云原生应用。您也可以直接将 Kuboard 安装到现有的 Kubernetes 集群，通过 Kuboard 提供的 Kubernetes RBAC 管理界面，将 Kubernetes 提供的能力开放给您的开发团队。\nKuboard 提供的功能有：\n Kubernetes 基本管理功能  节点管理 名称空间管理 存储类/存储卷管理 控制器（Deployment/StatefulSet/DaemonSet/CronJob/Job/ReplicaSet）管理 Service/Ingress 管理 ConfigMap/Secret 管理 CustomerResourceDefinition 管理   Kubernetes 问题诊断  Top Nodes / Top Pods 事件列表及通知 容器日志及终端 KuboardProxy (kubectl proxy 的在线版本) PortForward (kubectl port-forward 的快捷版本) 复制文件 （kubectl cp 的在线版本）   认证与授权  Github/GitLab 单点登录 KeyCloak 认证 LDAP 认证 完整的 RBAC 权限管理   Kuboard 特色功能  Kuboard 官方套件  Grafana+Prometheus 资源监控 Grafana+Loki+Promtail 日志聚合   Kuboard 自定义名称空间布局 Kuboard 中英文语言包    Lens lens是一个免费、开源的可以控制多kubernetes集群的IDE工具，如下图，左侧图标显示的是不同的Kubernetes集群，在此IDE上，可以可视化交互式的快速查看集群状态、每个资源对象的运行状态等。IDE支持Mac、windows和Linux操作系统，可以直接在Github上下载。下面看一下lens这款IDE支持的详细的功能：\n kubernetes IDE  对于需要每天处理Kubernetes集群的人来说，Lens是功能最强大的IDE。它是适用于MacOS，Windows和Linux操作系统的独立应用程序。确保正确设置和配置了群集。享受增强的可见性，实时统计信息，日志流和动手故障排除功能。借助Lens，您可以更轻松，更快速地使用集群，从而从根本上提高生产力和业务速度。\n 多集群管理  可以从一个统一的IDE放心地访问和使用群集。Lens可与任意数量的Kubernetes集群一起使用。使用内置的kubectl来执行Kubernetes RBAC，从而可以访问集群。群集可以是本地群集（例如，迷你库），也可以是外部群集（例如，EKS，AKS，GKE，Pharos，UCP，Rancher或OpenShift）。只需导入带有集群详细信息的kubeconfig即可添加集群。添加后，可以轻松地在集群之间进行切换。使用Lens，您将为所有群集获得一个统一的IDE！\n 多工作区  将集群组织成逻辑组。工作区用于将多个群集组织成逻辑组。对于需要处理多个（甚至数百个）群集的DevOps和SRE，它们非常有用。单个工作空间包含集群及其完整配置的列表。创建和在工作空间之间切换很容易。\n 内置普罗米修斯统计  查看作为仪表板一部分集成的所有相关图形和资源利用率图表。Lens带有内置的多租户Prometheus设置，该设置将尊重每个用户的RBAC。用户将看到他们有权访问的所有名称空间和资源的可视化。所有图形和资源利用率图表的设计均易于访问，并且在适当的上下文中均可使用，无论您操作的是仪表板的哪一部分。\n 上下文终端感应  内置终端随附了kubectl，该API始终在正确的上下文中与您的集群兼容。Lens内置终端将确保Kubernetes集群API的版本与kubectl的版本兼容。它会即时下载并分配正确的版本，因此您不必这样做。通过切换kubectl上下文以匹配您当前使用的集群，它也将使您高枕无忧。\nKubeSphere KubeSphere 是在 Kubernetes 之上构建的以应用为中心的企业级分布式容器平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大减轻开发、测试、运维的日常工作的复杂度，旨在解决 Kubernetes 本身存在的存储、网络、安全和易用性等痛点。除此之外，平台已经整合并优化了多个适用于容器场景的功能模块，以完整的解决方案帮助企业轻松应对敏捷开发与自动化运维、DevOps、微服务治理、灰度发布、多租户管理、工作负载和集群管理、监控告警、日志查询与收集、服务与网络、应用商店、镜像构建与镜像仓库管理和存储管理等多种业务场景。后续版本还将提供和支持多集群管理、大数据、人工智能等更为复杂的业务场景。\n因为功能齐全，显得用起来相对很笨重，kubesphere支持在线和离线安装，如果想要体验，可以先最小化安装一下试试，kubesphere还有devops，logging，service mesh，告警通知等可以按需尝试体验。\nWayne 是360开源的一个通用的、基于 Web 的 Kubernetes 多集群管理平台。通过可视化 Kubernetes 对象模板编辑的方式，降低业务接入成本， 拥有完整的权限管理系统，适应多租户场景，是一款适合企业级集群使用的发布平台。Wayne已大规模服务于360搜索，承载了内部绝大部分业务，稳定管理了近千个业务，上万个容器，运行了两年多时间，经受住了生产的考验。\nwayne的功能特性\n 视化操作：提供直观、简便的方式操作 Kubernetes 集群，减小学习成本，快速上线业务。 多样的编辑模式：支持图形化编辑，也支持 Json、Yaml 两种高级定制化编辑模式。 微内核架构：采用可扩展的插件化方式开发，定制化选择特性功能，更方便的集成符合企业需求的新功能。 多集群管理：可以同时管理多个 Kubernetes 集群，更方便的管理多个集群。 丰富的权限管理：将资源抽象化为部门、项目级别，角色的权限可以更细化的控制，适用于多部门、多项目的统一集中管理。 多种登录模式：支持企业级 LDAP 登录、支持 OAuth2 登录，支持数据库登录多种模式。 完备的审计：所有操作都会有完整的审计功能，方便追踪操作历史。 开放平台：支持 APIKey 开放平台，用户可自主申请相关 APIKey 并管理自己的项目。 多层次监控：提供多级别的监控统计信息，实时关注集群的运行状态。  Kubernetes Dashbaord Kubernetes仪表板是Kubernetes集群的基于Web的通用UI。它允许用户管理群集中运行的应用程序并对其进行故障排除，以及管理群集本身。如果想要正常的显示资源的使用情况，需要kubernetes集群中有metrics-server，并且原生态的Kubernetes Dashboard不支持其他第三方认证\n","description":"","id":32,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200629-kubernetes%E5%9B%BE%E5%BD%A2%E5%8C%96%E7%95%8C%E9%9D%A2/"},{"content":"Yaml基础语法与技巧  字符支持：YAML支持Unicode字符集，可以使用UTF-8、UTF-16、UTF-32字符集  1 2 3 4 5 6  date:2020-06-28info:- name:Marionxuetags:- num:1- descript:\u0026#34;writing somthing here\u0026#34;   缩进：YAML中缩进是非常重要的规范，不支持Tab，支持空格，没有严格要求空格个数，但是需要确保同一层次的左侧对齐 单行注释：单行注释使用#进行标记，可以在单行的任何位置开始注释的内容 多行注释：不提供特殊的多行注释，使用多行行首的单行注释#实现多行注释的需求  1 2 3  info:# 下面是两个空格，然后一个短横线- name:Marionxue   基本数据类型：支持整型、浮点型、时间戳类型、Null等基本数据类型 组合数据类型：支持键/值方式和列表类型，并可进行嵌套组合 键/值方式：使用冒号:进行分隔，也可使用{}结合逗号进行表达 列表类型：使用横线-进行分隔，也可使用[]结合逗号进行表达 开始符号: ---用于表示开始的符号，在一个文件中包含多个YAML设定的时候使用非常常见。 结束符号：…用于表示yaml文件结束  1 2 3 4 5 6 7 8 9 10  ---# start- [blue,red,green]#list- [Age,Bag]- site:{osc:www.oschina.net, baidu:www.baidu.com}# key/value list- describle:| Hi,all:mynameisxxx.- code:\u0026gt; fmt.Println(\u0026#34;姓名: %s\u0026#34;,name)    单引号与双引号：字符串类型可以不使用单引号和双引号，使用单引号和双引号与不使用的时候在特殊字符及其转义的时候有些细微的区别，可用倒斜线**（\\）**进行特殊字符转义\n  区块的字串用缩排和修饰词（非必要）来和其他资料分隔，有新行保留（使用符号|）或新行折叠（使用符号\u0026gt;）两种方式\n |表示保留区块中的回车换行 \u0026gt;表示将区块中的回车换行替换为空行，最终连成一行     强制类型转换：可以使用!!用于强制类型转换\n重复性内容：可以使用锚点标记\u0026amp;和应用标记*结合使用可以处理重复性的内容\n保留字符：@和`为当前YAML规格的保留字符\n较长的描绘性说明：使用|与\u0026gt;以及\u0026gt;-来处理常见的对于较长的描绘性说明的要求\n空白字符限制：在使用逗号及冒号时，后须接一个空白字符   YAML使用可打印的Unicode字符，可使用UTF-8或UTF-16 使用空白字符**（不能使用Tab）**分层，同层元素左侧对齐 单行注解由井字号**（ # ）**开始，可以出现在行中任何位置 每个清单成员以单行表示，并用短杠+空白**（- ）**起始 每个杂凑表的成员用冒号+空白**（: ）**分开键和值 杂凑表的键值可以用问号 **(?)**起始，表示多个词汇组成的键值 字串一般不使用引号，但必要的时候可以用引号框住 使用双引号表示字串时，可用倒斜线**（\\）**进行特殊字符转义 区块的字串用缩排和修饰词（非必要）来和其他资料分隔，有新行保留（使用符号|）或新行折叠（使用符号\u0026gt;）两种方式 在单一档案中，可用连续三个连字号（\u0026mdash;）区分多个档案 可选择性的连续三个点号（\u0026hellip;）用来表示档案结尾(在流式传输时非常有用，不需要关闭流即可知道到达结尾处) 重复的内容可使从参考标记星号 (*)复制到锚点标记（\u0026amp;） 指定格式可以使用两个惊叹号 ( !! )，后面接上名称  ","description":"","id":33,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200629-yaml%E8%AF%AD%E6%B3%95%E6%A0%BC%E5%BC%8F%E5%AD%A6%E4%B9%A0/"},{"content":"日志类查看 1  dmesg -T   ","description":"","id":34,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200630-linux%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E9%94%A6%E5%9B%8A/"},{"content":"2020年6月24日，BFE开源项目被CNCF （Cloud Native Computing Foundation，云原生计算基金会）正式接纳为Sandbox Project。这是百度第一个被CNCF接纳的开源项目，也是在网络方向上中国第一个被CNCF接纳的开源项目。\nBFE原名为Baidu Front End（百度统一前端），是百度的统一七层流量转发平台。BFE平台目前已接入百度大部分流量，每日转发请求接近1万亿，峰值QPS超过1000万。在2019年百度春晚红包活动中，BFE平台在超大用户压力、数次流量波峰下平稳运行，保证了春晚红包活动的顺利进行。\n作为综合的流量转发平台，BFE平台集成了以下4大功能：\n 流量接入和转发：支持HTTP、HTTPS、HTTP/2、QUIC等多种协议，并支持强大的应用层路由能力 流量全局调度：支持由外网流量调度和内网流量调度共同构成的全局流量调度系统 安全和防攻击：支持黑名单封禁、精细限流和应用层防火墙（WAF）等多种防攻击能力 实时数据分析：支持分钟级的超高维度时序报表  作为BFE平台的核心组件，BFE转发引擎从2012年开始研发，并于2014年使用Go语言完成重构。由于基于Go语言，和业界普遍使用的Nginx开源软件相比，BFE具有以下优势：\n 研发效率高：Go语言的开发效率远高于C语言（及Lua），在代码的可维护性方面也有巨大优势。 系统的安全和稳定性高：Go语言没有C语言固有的缓冲区溢出隐患，规避了大量的稳定性和安全风险；另外对于异常可以捕捉，保证程序在快速迭代上线的情况下也不崩溃。  有理由相信，从长期趋势看，基于更高级编程语言的软件系统会逐步取得竞争的优势。\nCPU等硬件资源的价格仍会快速下降，而开发人力成本、项目研发风险、系统稳定性/安全性方面会成为更重要的决策考虑。从这方面出发，主要基于C语言的Nginx会逐步衰落，而类似BFE这样的基于更高级编程语言的软件会逐步成为主流。\n另外，BFE在设计中，还特别增加了企业级应用场景的考虑：\n 转发场景的直接支持：和Nginx这样从Web Server转型为Proxy的进化路径不同，BFE直接为转发场景设计，从转发模型和转发配置方面更满足转发场景的需求 多租户的支持：在云计算的场景下，多租户复用是普遍的需求。在BFE的设计中，内置提供了多租户的支持。 结构化的配置：BFE的配置设计，大量使用JSON这样的结构化方式，便于和相关配置管理系统对接 丰富的监控探针：作为一个工业级软件，在BFE的设计中充分考虑了线上监控的需求，BFE程序通过HTTP方式向外暴露数千个内部状态变量  为了促进负载均衡技术的交流和发展，BFE的转发引擎于2019年7月正式开源，并获得了广泛的关注。2019年11月19日，BFE开源项目登上GitHub Trending Top 3。2019年12月，BFE开源项目的Github stars超过3000。\nBFE开源支持以下重要能力：\n1、主流网络协议接入\n 支持HTTP/HTTPS/SPDY/HTTP2/WebSocket等 支持TLS/HTTP/ WebSocket反向代理模式  2、可扩展插件框架\n 通过可扩展插件框架，快速定制开发扩展模块，满足业务定制化需求 内置重写、重定向、流量修改、封禁等丰富插件  3、基于请求内容的分流\n 基于领域专有语言的分流规则，满足复杂业务场景定制化流量转发 支持完备的分流条件原语集，包括基于请求内容（URI/Header/Cookie等）以及请求上下文（IP、协议、标签、时间等）的条件原语。  4、灵活的负载均衡策略\n 支持集群级别负载均衡及实例级别负载均衡，实现多可用区容灾及过载保护 内置加权轮询、加权最小连接数策略，基于IP或请求内容识别用户实现会话保持  CNCF是云计算领域全球顶级的开源社区。BFE开源项目在2020年启动了加入CNCF的申请工作。经过一系列的准备工作，于2020年6月18日通过CNCF SIG-NETWORK的答辩，并在不到一周内收到了被CNCF TOC接受的通知。在加入CNCF后，BFE将改名为Beyond Front End。\nBFE开源技术已在百度内被HTTPDNS、云加速、BML等产品使用，并将和百度的云原生产品进一步深入结合。BFE商用产品已经被度小满、央视网等客户选用，并已经在多个客户进行了测试验证。BFE将进一步扩大开源范围，加强开源生态的建设，并基于开源建立百度负载均衡的商业生态。\n","description":"","id":35,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200701-%E7%99%BE%E5%BA%A6%E5%BC%80%E6%BA%90bfe%E8%A2%ABcncf%E6%8E%A5%E7%BA%B3%E4%B8%BAsandbox-project/"},{"content":"Etcd集群的部署实践 我们准备三台机器，分别在这三台机器上安装部署etcd\n1 2  wget https://github.com/etcd-io/etcd/releases/download/v3.4.9/etcd-v3.4.9-linux-amd64.tar.gz   在每个节点的机器上执行一下环境设定\n1 2 3 4 5 6 7 8 9  TOKEN=token-01 CLUSTER_STATE=new NAME_1=machine-1 NAME_2=machine-2 NAME_3=machine-3 HOST_1=10.240.0.17 HOST_2=10.240.0.18 HOST_3=10.240.0.19 CLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380   ","description":"","id":36,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200703-etcd%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%AE%9E%E8%B7%B5%E7%AF%87/"},{"content":"文章背景 日常的工作中，会收到一堆CPU使用率过高的告警邮件，遇到某台服务的CPU被占满了，这时候我们就要去查看是什么进程将服务器的CPU资源占用满了。通常我们会通过top或者htop来快速的查看占据CPU最高的那个进程，如下图：\n这里是通过一个普通的服务器做演示使用，如图所示当前服务器占用CPU最高的是一个叫做kube-apiserver命令运行的一个进程，该进程的PID为25633,当然你可能遇到一个服务器上运行有多个服务，想快速知道占用率最高的那几个进程的话，你可以使用以下命令:\n1 2  ps aux|head -1;ps -aux | sort -k3nr | head -n 10 //查看前10个最占用CPU的进程 ps aux|head -1;ps -aux | sort -k4nr | head -n 10 //查看前10个最占用内存的进程   但是通过以上的方法获取到服务器占用资源的进程之后，还是不知道CPU使用究竟耗时在哪里,不清楚瓶颈在哪里，此时就可以通过Linux系统的性能分析工具perf分析，分析其返回的正在消耗CPU的函数以及调用栈。然后可以通过解析perf采集的数据，渲染到火焰图🔥，就清楚的知道究竟占用系统CPU资源的罪魁祸首了。\n在制作火焰图之前，需要先来说说这个Linux性能分析工具perf,该工具是一个相对简单易上手的性能分析工具，是Performance单词的缩写，通过其perf的命令选项完成系统事件的采集到解析，我们来简单的认识一下：\nlinux上的性能分析工具Perf 安装perf 我目前的服务器发行版是Ubuntu 16.04.6 LTS因此需要先安装perf才能使用，该工具由linux-tools-common提供，但是它需要安装后面的依赖。\n1 2 3 4 5  #安装 root@master:~# apt install linux-tools-common linux-tools-4.4.0-142-generic linux-cloud-tools-4.4.0-142-generic -y root@master:~# perf -v #显示perf的版本 perf version 4.4.167   在安装完成时候，我们就可以对上图CPU使用率最高的进程ID为25633的进程进行采样分析。\n首选我们采集一下该进程的调用栈信息:\n1 2 3  root@master:~# sudo perf record -F 99 -p 25633 -g -- sleep 30 [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.039 MB perf.data (120 samples) ]   这个命令会产生一个大的数据文件，取决与你采集的进程与CPU的配置，如果一台服务器有16个 CPU，每秒抽样99次，持续30秒，就得到 47,520 个调用栈，长达几十万甚至上百万行。上面的命令中，perf record表示记录，-F 99表示每秒99次，-p 25633是进程号，即对哪个进程进行分析，-g表示记录调用栈，sleep 30则是持续30秒，参数信息可以视情况调整。生成的数据采集文件在当前目录下，名称为perf.data。\nperf record命令可以从高到低排列统计每个调用栈出现的百分比，显示结果如下图所示:\n1  root@master:~# sudo perf report -n --stdio   这样的效果对使用者来说还是不那么直观易读，这时候，火焰🔥图也就真正的派上用途了。\n制作火焰🔥图 火焰🔥图并非一定就是火焰系列的颜色主题，只是通过🔥色系更能表达出含义。火焰图常见的类型有 On-CPU, Off-CPU, 还有 Memory, Hot/Cold, [Differential](http://www.brendangregg.com/blog/2014-11-09/differential-flame-graphs.html \u0026quot;Differential\u0026quot;) 等等. on-CPU/off-cpu`的区别就是一个是用于CPU是性能瓶颈，一个是IO是性能瓶颈，当你不知道当前的服务器的性能瓶颈究竟是什么的时候，你可以使用这两种类型进行对比，通过两种火焰图的差别是比较大的，如果两张火焰图长得差不多, 那么通常认为CPU被其它进程抢占了.\n另外一种情况就是如果无法确定当前的系统瓶颈, 可以通过压测工具来确认 : 通过压测工具看看能否让CPU使用率趋于饱和, 如果能那么使用 On-CPU 火焰图, 如果不管怎么压, CPU 使用率始终上不来, 那么多半说明程序被 IO 或锁卡住了, 此时适合使用 Off-CPU 火焰图. 你可以通过压测工具进行测试，目前比较常用的就是ab和wrk，我建议尝试使用诸如 wrk 之类更现代的压测工具.\n 如果选择 ab 的话, 那么务必记得开启 -k 选项, 以避免耗尽系统的可用端口\n Github上有Brendan D. Gregg 的 Flame Graph 工程实现了一套生成火焰图的脚本.我们可以直接克隆下来直接用。\n1  cd \u0026amp;\u0026amp; git clone https://github.com/brendangregg/FlameGraph.git   生成火焰🔥图，我们一般都遵循以下流程\n 捕获堆栈: 使用perf捕捉进程运行堆栈信息 折叠堆栈: 对抓取的系统和程序运行每一时刻的堆栈信息进行分析组合, 将重复的堆栈累计在一起, 从而体现出负载和关键路径，通过stackcollapse脚本完成 生成火焰图：分析 stackcollapse 输出的堆栈信息渲染成火焰图  Flame Graph中提供了抓取不同信息的脚本，可以按需使用。下面我们需要对捕获到的进程堆栈信息perf.data进行折叠，生成折叠的堆栈信息:\n1  root@master:~# perf script -i /root/perf.data \u0026amp;\u0026gt; /root/perf.unfold   用 stackcollapse-perf.pl 将 perf 解析出的内容 perf.unfold 中的符号进行折叠\n1 2 3 4 5 6 7  root@master:~/FlameGraph# ls aix-perf.pl docs example-perf.svg pkgsplit-perf.pl stackcollapse-aix.pl stackcollapse-go.pl stackcollapse-ljp.awk stackcollapse-pmc.pl stackcollapse-vsprof.pl test.sh demos example-dtrace-stacks.txt files.pl range-perf.pl stackcollapse-bpftrace.pl stackcollapse-instruments.pl stackcollapse-perf.pl stackcollapse-recursive.pl stackcollapse-vtune.pl dev example-dtrace.svg flamegraph.pl README.md stackcollapse-elfutils.pl stackcollapse-java-exceptions.pl stackcollapse-perf-sched.awk stackcollapse-sample.awk stackcollapse-xdebug.php difffolded.pl example-perf-stacks.txt.gz jmaps record-test.sh stackcollapse-gdb.pl stackcollapse-jstack.pl stackcollapse.pl stackcollapse-stap.pl test root@master:~/FlameGraph# ./stackcollapse-perf.pl /root/perf.unfold \u0026amp;\u0026gt; /root/perf.folded root@master:~/FlameGraph#   最后就是生成火焰🔥图了\n1  root@master:~/FlameGraph# ./flamegraph.pl /root/perf.folded \u0026gt; /root/perf.svg   当然也可以通过管道符|将整个过程简化:\n1  cd \u0026amp;\u0026amp; perf script | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl \u0026gt; process.svg   最后在谷歌浏览器上打开该火焰图:\n火焰图是基于stack信息生成的SVG 图片, 用来展示 CPU 的调用栈。\n  y 轴表示调用栈, 每一层都是一个函数. 调用栈越深, 火焰就越高, 顶部就是正在执行的函数, 下方都是它的父函数.\n  x 轴表示抽样数, 如果一个函数在 x 轴占据的宽度越宽, 就表示它被抽到的次数多, 即执行的时间长. 注意, x 轴不代表时间, 而是所有的调用栈合并后, 按字母顺序排列的.\n  火焰图就是看顶层的哪个函数占据的宽度最大. 只要有\u0026quot;平顶\u0026quot;(plateaus), 就表示该函数可能存在性能问题。颜色没有特殊含义, 因为火焰图表示的是 CPU 的繁忙程度, 所以一般选择暖色调.\n当调用栈不完整调用栈过深时，某些系统只返回前面的一部分（比如前10层）;当函数名缺失，函数没有名字，编译器只用内存地址来表示（比如匿名函数），所以使用火焰图也是存在分析不到的地方。你也可以通过以下脚本进行采集分析火焰图:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  if [ $# -ne 1 ];then echo \u0026#34;Usage: $0seconds\u0026#34; exit 1 fi perf record -a -g -o perf.data \u0026amp; PID=`ps aux| grep \u0026#34;perf record\u0026#34;| grep -v grep| awk \u0026#39;{print $2}\u0026#39;` if [ -n \u0026#34;$PID\u0026#34; ]; then sleep $1 kill -s INT $PID fi # wait until perf exite sleep 1 perf script -i perf.data \u0026amp;\u0026gt; perf.unfold perl stackcollapse-perf.pl perf.unfold \u0026amp;\u0026gt; perf.folded perl flamegraph.pl perf.folded \u0026gt;perf.svg   ","description":"","id":37,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200704-%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9Acpu%E6%B6%88%E8%80%97%E5%9C%A8%E5%93%AA%E4%BA%9B%E5%87%BD%E6%95%B0%E4%B8%8A/"},{"content":"最近在阿里云开发者藏经阁中发现一本有趣的电子书，由声东编写的《深入浅出Kubernetes》，文末有下载方式：\n 一次搞懂6个核心原理吃透理论基础 一次学会6个典型问题的华丽操作  124页的阿里云《深入浅出Kubernetes》从理论到实践的干货大纲:\n理论篇  这么理解集群控制器，能行！ 集群网络详解 集群伸缩原理 认证与调度 集群服务的三个要点和一种实现 镜像拉去这件小事  实践篇  读懂这一篇，集群节点不下线 节点下线姊妹篇 我们为什么会删除不了集群的命名空间？ 阿里云ACK产品安全组配置管理 二分之一活的微服务 半夜两点Ca证书过期问题处理惨况总结   在微信公众号内回复\u0026quot;阿里云\u0026quot;即可下载《深入浅出Kubernetes》PDF版。\n 精彩文章回顾   火焰图：全局视野的Linux性能剖析 1k+在读\n  最流行的五款Kubernetes交互式可视化工具 900+在读\n  轻松爬取拉勾网岗位招聘信息 600+在读\n  Yearning - 最Popular的MYSQL审计平台 700+在读\n  Prometheus监控系列-部署篇 500+在读\n  ","description":"","id":38,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200705-%E9%98%BF%E9%87%8C%E4%BA%91kubernetes%E7%94%B5%E5%AD%90%E4%B9%A6%E5%88%86%E4%BA%AB/"},{"content":"前段时间，发现新运维社区的赵班长(赵舜东)更新了一版《运维知识体系 v3.1》其中新增了一些目前比较火的、成熟的运维解决方案，也包括容器编排，微服务框架lstio等。分层的归类总结各个层次常用的开源软件以及服务组合解决方案，其中包括客户端层、外部层、网络层、接入层、应用服务层、存储层、基础服务层、容器层、操作系统层、基础设施层等，以及技术在运维、运维自动化成长方向的发展以及迭代的趋势，这里贴出运维知识体系-v3.1以及Web缓存知识体系-V3.0供大家学习参考。\n运维知识体系-v3.1 Web缓存知识体系-V3.0  图片素材来源: https://www.unixhot.com/\n 精彩文章回顾  Jenkinsh专辑|解决Jenkins安装的疑难杂症 Harbor2.0开源镜像仓库企业级实现 Harbor2.0配置高可用的Harbor镜像仓库 阿里云出品·Kubernetes 深入浅出实践 v1.0 微软出品·Kubernetes 最新学习指南 v3.0 火焰图：全局视野的 Linux 性能剖析 1k+在读 最流行的五款 Kubernetes 交互式可视化工具 900+在读 轻松爬取拉勾网岗位招聘信息 600+在读 Yearning - 最 Popular 的 MYSQL 审计平台 700+在读  ","description":"","id":41,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/20200710-%E8%BF%90%E7%BB%B4%E7%9A%84%E7%9F%A5%E8%AF%86%E9%9D%A2%E7%A9%B6%E7%AB%9F%E6%9C%89%E5%A4%9A%E5%A4%A7/"},{"content":"请使用 Chrome 浏览器。\n请阅读下方文本熟悉工具使用方法，本文可直接拷贝到微信中预览。\n1 Markdown Nice 简介  支持自定义样式的 Markdown 编辑器 支持微信公众号、知乎和稀土掘金 欢迎扫码回复「排版」加入用户群  2 主题 https://preview.mdnice.com/themes/\n欢迎提交主题，提供更多文章示例~~\n3 通用语法 3.1 标题 在文字写书写不同数量的#可以完成不同的标题，如下：\n一级标题 二级标题 三级标题 3.2 无序列表 无序列表的使用，在符号-后加空格使用。如下：\n 无序列表 1 无序列表 2 无序列表 3  如果要控制列表的层级，则需要在符号-前使用空格。如下：\n 无序列表 1 无序列表 2  无序列表 2.1 无序列表 2.2    由于微信原因，最多支持到二级列表。\n3.3 有序列表 有序列表的使用，在数字及符号.后加空格后输入内容，如下：\n 有序列表 1 有序列表 2 有序列表 3  3.4 引用 引用的格式是在符号\u0026gt;后面书写文字。如下：\n 读一本好书，就是在和高尚的人谈话。 ——歌德\n  雇用制度对工人不利，但工人根本无力摆脱这个制度。 ——阮一峰\n 3.5 粗体和斜体 粗体的使用是在需要加粗的文字前后各加两个*。\n而斜体的使用则是在需要斜体的文字前后各加一个*。\n如果要使用粗体和斜体，那么就是在需要操作的文字前后加三个*。如下：\n这个是粗体\n这个是斜体\n这个是粗体加斜体\n注：由于 commonmark 标准，可能会导致加粗与想象不一致，如下\n**今天天气好晴朗，**处处好风光。\n这个是正常现象，请参考加粗 Issue。\n3.6 链接 微信公众号仅支持公众号文章链接，即域名为https://mp.weixin.qq.com/的合法链接。使用方法如下所示：\n对于该论述，欢迎读者查阅之前发过的文章，你是《未来世界的幸存者》么？\n3.7 分割线 可以在一行中用三个以上的减号来建立一个分隔线，同时需要在分隔线的上面空一行。如下：\n3.8 删除线 删除线的使用，在需要删除的文字前后各使用两个~，如下：\n这是要被删除的内容。\n3.9 表格 可以使用冒号来定义表格的对齐方式，如下：\n   姓名 年龄 工作     小可爱 18 吃可爱多   小小勇敢 20 爬棵勇敢树   小小小机智 22 看一本机智书    3.10 图片 插入图片，如果是行内图片则无图例，否则有图例，格式如下：\n可以通过在图片尾部添加宽度和高度控制图片大小，用法如下：\n![同时设置宽度和高度](https://imgkr.cn-bj.ufileos.com/4f78d8e8-77f6-4ea8-8a93-305087da06bd.png =150x150)\n![只设置宽度，推荐使用百分比](https://imgkr.cn-bj.ufileos.com/4f78d8e8-77f6-4ea8-8a93-305087da06bd.png =40%x)\n该语法比较特殊，其他 Markdown 编辑器不完全通用。\n支持 jpg、png、gif、svg 等图片格式，其中 svg 文件仅可在微信公众平台中使用，svg 文件示例如下：\n 支持图片拖拽和截图粘贴到编辑器中上传，上传时使用当前选择的图床。 可使用格式-\u0026gt;图片上传本地图片，网站仅支持「图壳」图床，失败率低可长久保存！  注：仅支持 https 的图片，图片粘贴到微信时会自动上传微信服务器，不必担心使用上述图床会导致微信内图片丢失。\n4. 特殊语法 4.1 脚注  支持平台：微信公众号、知乎。\n 脚注与链接的区别如下所示：\n1 2  链接：[文字](链接) 脚注：[文字](脚注解释 \u0026#34;脚注名字\u0026#34;)   有人认为在大前端时代的背景下，移动端开发（Android、IOS）将逐步退出历史舞台。\n全栈工程师在业务开发流程中起到了至关重要的作用。\n脚注内容请拉到最下面观看。\n4.2 代码块  支持平台：微信代码主题仅支持微信公众号！其他主题无限制。\n 如果在一个行内需要引用代码，只要用反引号引起来就好，如下：\nUse the printf() function.\n在需要高亮的代码块的前一行及后一行使用三个反引号，同时第一行反引号后面表示代码块所使用的语言，如下：\n1 2 3 4 5 6 7  // FileName: HelloWorld.java public class HelloWorld { // Java 入口程序，程序从此入口  public static void main(String[] args) { System.out.println(\u0026#34;Hello,World!\u0026#34;); // 向控制台打印一条语句  } }   支持以下语言种类：\nbash clojure，cpp，cs，css dart，dockerfile, diff erlang go，gradle，groovy haskell java，javascript，json，julia kotlin lisp，lua makefile，markdown，matlab objectivec perl，php，python r，ruby，rust scala，shell，sql，swift tex，typescript verilog，vhdl xml yaml 如果想要更换代码主题，可在上方挑选，不支持代码主题自定义。\n其中微信代码主题与微信官方一致，有以下注意事项：\n 带行号且不换行，代码大小与官方一致 需要在代码块处标志语言，否则无法高亮 粘贴到公众号后，用鼠标点代码块内外一次，完成高亮  diff 不能同时和其他语言的高亮同时显示，且需要调整代码主题为微信代码主题以外的代码主题才能看到 diff 效果，使用效果如下:\n1 2  + 新增项 - 删除项   其他主题不带行号，可自定义是否换行，代码大小与当前编辑器一致\n4.3 数学公式  支持平台：微信公众号、知乎。\n 行内公式使用方法，比如这个化学公式：$\\ce{Hg^2+ -\u0026gt;[I-] HgI2 -\u0026gt;[I-] [Hg^{II}I4]^2-}$\n块公式使用方法如下：\n$$H(D_2) = -\\left(\\frac{2}{4}\\log_2 \\frac{2}{4} + \\frac{2}{4}\\log_2 \\frac{2}{4}\\right) = 1$$\n矩阵：\n$$\n\\begin{pmatrix}\n1 \u0026amp; a_1 \u0026amp; a_1^2 \u0026amp; \\cdots \u0026amp; a_1^n \\\n1 \u0026amp; a_2 \u0026amp; a_2^2 \u0026amp; \\cdots \u0026amp; a_2^n \\\n\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\n1 \u0026amp; a_m \u0026amp; a_m^2 \u0026amp; \\cdots \u0026amp; a_m^n \\\n\\end{pmatrix}\n$$\n公式由于微信不支持，目前的解决方案是转成 svg 放到微信中，无需调整，矢量不失真。\n目前测试如果公式量过大，在 Chrome 下会存在粘贴后无响应，但是在 Firefox 中始终能够成功。\n4.4 TOC  支持平台：微信公众号、知乎。\n TOC 全称为 Table of Content，列出全部标题。由于示例标题过多，需要使用将下方代码段去除即可。\n[TOC] 由于微信只支持到二级列表，本工具仅支持二级标题和三级标题的显示。\n4.5 注音符号  支持平台：微信公众号。\n 支持注音符号，用法如下：\nMarkdown Nice 这么好用，简直是{喜大普奔|hē hē hē hē}呀！\n4.6 横屏滑动幻灯片  支持平台：微信公众号。\n 通过\u0026lt;![](url),![](url)\u0026gt;这种语法设置横屏滑动滑动片，具体用法如下：\n\u0026lt;,,\u0026gt;\n5 其他语法 5.1 HTML 支持原生 HTML 语法，请写内联样式，如下：\n橙色居右\n橙色居中\n5.2 UML 不支持，推荐使用开源工具https://draw.io/制作后再导入图片\n5.3 组件图床 组件目前共支持 3 种图床和 1 种自定义图床，主要特点如下：\n   图床 费用 有效期 失败率     SM.MS 免费 长期 高   阿里云 付费 自定义 低   七牛云 10G 免费 自定义 低   自定义 高昂 自定义 自定义    4 个图床的缺点：\n   图床 缺点     SM.MS 失败率高可用性很差   阿里云 配置繁琐，费用昂贵   七牛云 配置繁琐，需购买长期域名   自定义 搭建后台繁琐    5.4 更多文档 更多文档请参考 markdown-nice-docs\n","description":"","id":43,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/docs-template/"},{"content":"1 GitlabRunner 简介 1.1 Gitlab-runner GitLab Runner 是一个开源项目，用于运行您的作业并将结果发送回 GitLab。它与GitLab CI结合使用，GitLab CI是GitLab随附的用于协调作业的开源持续集成服务。\n1.2 要求  GitLab Runner 是用Go编写的，可以作为一个二进制文件运行，不需要特定于语言的要求。它旨在在 GNU / Linux，macOS 和 Windows 操作系统上运行。只要您可以在其他操作系统上编译 Go 二进制文件，其他操作系统就可能会运行。 如果要使用 Docker，请安装最新版本。GitLab Runner 需要最少的 Docker v1.13.0。 GitLab Runner 版本应与 GitLab 版本同步。尽管较旧的 Runner 仍可以使用较新的 GitLab 版本，反之亦然，但在某些情况下，如果版本存在差异，则功能可能不可用或无法正常工作。在次要版本更新之间可以保证向后兼容性，但是请注意，GitLab 的次要版本更新会引入新功能，这些新功能将要求 Runner 在同一次要版本上使用。  1.3 特点  运行模式：  同时执行多个作业。 对多个服务器（甚至每个项目）使用多个令牌。 限制每个令牌的并行作业数。   job 可以运行在：  在本地。 使用 Docker 容器。 使用 Docker 容器并通过 SSH 执行作业。 使用 Docker 容器在不同的云和虚拟化管理程序上自动缩放。 连接到远程 SSH 服务器。   用 Go 编写并以单个二进制文件的形式分发，而没有其他要求。 支持 Bash，Windows Batch 和 Windows PowerShell。 在 GNU / Linux，macOS 和 Windows（几乎可以在任何可以运行 Docker 的地方）上运行。 允许自定义 job 运行环境。 Gitlab runner 的配置文件热加载，无需重启。 易于使用的设置，并支持 Docker，Docker-SSH，Parallels 或 SSH 运行环境。 可以利用 Docker 容器的缓存。 易于安装，可作为 GNU / Linux，macOS 和 Windows 的服务。 嵌入式 Prometheus 指标 HTTP 服务器。 裁判工作者监视 Prometheus 度量标准和其他特定于工作的数据并将其传递给 GitLab。  2 Gitlab Runner 部署使用篇 2.1 安装篇  安装须知！！！\n   通过包管理器安装 gitlab runner 服务会自动的创建 gitlab runner 用户\n  如果需要使用 Docker 环境或者利用 Docker 缓存构建，需将gitlab-runner用户加入到 docker 用户组\n1  usermod -aG docker gitlab-runner     如果授权之后依旧显示无权限操作docker.sock，此时你需要修改 Gitlab runner 所运行在的主机上的 Docker daemon\n  2.2 注册篇 2.2.1 Gitlab Runner 注册  交互式注册  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  docker run --rm -t -i -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register Runtime platform arch=amd64 os=linux pid=6 revision=ac8e767a version=12.6.0 Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): https://code.xingshulin.com Please enter the gitlab-ci token for this runner: 4tutaeWWL3srNEcmHs1s Please enter the gitlab-ci description for this runner: [00e4f023b5ae]: devops-service-runner Please enter the gitlab-ci tags for this runner (comma separated): build Registering runner... succeeded runner=4tutaeWW Please enter the executor: parallels, virtualbox, docker-ssh+machine, kubernetes, docker+machine, custom, docker, docker-ssh, shell, ssh: shell Runner registered successfully. Feel free to start it, but if it\u0026#39;s running already the config should be automatically reloaded!    直接注册  1 2 3 4 5 6 7 8 9 10  docker run -itd --rm -v ~/data/gitlab-runner/config:/etc/gitlab-runner gitlab/gitlab-runner:v12.6.0 register \\  --non-interactive \\  --executor \u0026#34;shell\u0026#34; \\  --url \u0026#34;https://code.xingshulin.com\u0026#34; \\  --registration-token \u0026#34;JRzzw2j1Ji6aBjwvkxAv\u0026#34; \\  --description \u0026#34;devops-runner\u0026#34; \\  --tag-list \u0026#34;build,deploy\u0026#34; \\  --run-untagged=\u0026#34;true\u0026#34; \\  --locked=\u0026#34;false\u0026#34; \\  --access-level=\u0026#34;not_protected\u0026#34;   2.2.2 gitlab runner 选项 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  -c value, --config value 指定配置文件 --template-config value 指定模板配置文件 --tag-list value 指定runner的标签列表，逗号分隔 -n, --non-interactive 无交互进行runner注册 --leave-runner 如果注册失败，不用删除runner -r value, --registration-token value runner的注册token --run-untagged 注册运行未加标签的构建，默认当标签列表为空时值为true --locked 锁定runner 默认true --access-level value 设置访问等级 not_protected or ref_protected; 默认 not_protected --maximum-timeout value 为作业设置最大运行超时时间 默认零 单位秒 --paused 设置runner为 paused,默认 \u0026#39;false\u0026#39; --name value, --description value Runner 名称 --limit value 程序处理的最大构建数量default: \u0026#34;0\u0026#34; --output-limit value 最大的构建大小单位kb default: \u0026#34;0\u0026#34; --request-concurrency value 作业请求的最大并发数 default: \u0026#34;0\u0026#34; -u value, --url value GitlabCI服务器地址 -t value, --token value GitlabCI服务器token   2.2.3 gitlab runner 类型  stuck: 表示当前的 pipeline 没有适配到合适的 Tags 的 Runner\n  shared ： 运行整个平台项目的作业（gitlab） group： 运行特定 group 下的所有项目的作业（group） specific: 运行指定的项目作业（project） locked： 无法运行项目作业 paused： 不会运行作业  3. GitLab Runner Pipeline 3.1 job 在每个项目中，我们使用名为.gitlab-ci.yml的 YAML 文件配置 GitLab CI / CD 管道。\n这里在 pipeline 中定义了两个作业，每个作业运行不同的命令。命令可以是 shell 或脚本。\njob1: script: \u0026quot;execute-script-for-job1\u0026quot; job2: script: \u0026quot;execute-script-for-job2\u0026quot;  可以定义一个或多个作业(job)。 每个作业必须具有唯一的名称（不能使用关键字）。 每个作业是独立执行的。 每个作业至少要包含一个 script。  3.2 script job: script: - uname -a - bundle exec rspec **注意：**有时， script命令将需要用单引号或双引号引起来. 例如，包含冒号命令（ : ）需要加引号，以便被包裹的 YAML 解析器知道来解释整个事情作为一个字符串，而不是一个”键：值”对. 使用特殊字符时要小心： : ， { ， } ， [ ， ] ， , ， \u0026amp; ， * ， # ， ? ， | ， - ， \u0026lt; ， \u0026gt; ， = ! ， % ， @ .\n3.3 before_script 用于定义一个命令，该命令在每个作业之前运行。必须是一个数组。指定的script与主脚本中指定的任何脚本串联在一起，并在单个 shell 中一起执行。\n3.4 after_script 用于定义将在每个作业（包括失败的作业）之后运行的命令。这必须是一个数组。指定的脚本在新的 shell 中执行，与任何before_script或script脚本分开。\n可以在全局定义，也可以在 job 中定义。在 job 中定义会覆盖全局。\nbefore_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com stages: - build - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build script: - echo \u0026quot;mvn clean \u0026quot; - echo \u0026quot;mvn install\u0026quot; after_script: - echo \u0026quot;after script in job\u0026quot; deploy: stage: deploy script: - echo \u0026quot;hello deploy\u0026quot; after_script: - echo \u0026quot;after-script\u0026quot; after_script 失败不会影响作业失败。\n\nbefore_script 失败导致整个作业失败，其他作业将不再执行。作业失败不会影响 after_script 运行。\n\n3.5 stages 用于定义作业可以使用的阶段，并且是全局定义的。同一阶段的作业并行运行，不同阶段按顺序执行。\nstages： - build - test - deploy 这里定义了三个阶段，首先 build 阶段并行运行，然后 test 阶段并行运行，最后 deploy 阶段并行运行。deploy 阶段运行成功后将提交状态标记为 passed 状态。如果任何一个阶段运行失败，最后提交状态为 failed。\n 未定义 stages\n 全局定义的 stages 是来自于每个 job。如果 job 没有定义 stage 则默认是 test 阶段。如果全局未定义 stages,则按顺序运行 build,test,deploy。\n\n如果作业中定义了其他阶段，例如”codescan”则会出现错误。原因是因为除了 build test deploy 阶段外的其他阶段作为.pre 运行（也就是作为第一个阶段运行，需要将此作业的 stage 指定为.pre）。\n\ncodescan: stage: .pre script: - echo \u0026quot;codescan\u0026quot; 定义 stages 控制 stage 运行顺序\n一个标准的 yaml 文件中是需要定义 stages，可以帮助我们对每个 stage 进行排序。\nstages: - build - test - codescan - deploy \n3.6 .pre \u0026amp; .post .pre 始终是整个管道的第一个运行阶段，.post 始终是整个管道的最后一个运行阶段。 用户定义的阶段都在两者之间运行。.pre和.post的顺序无法更改。如果管道仅包含.pre或.post阶段的作业，则不会创建管道。\n\n3.7 stage 是按 JOB 定义的，并且依赖于全局定义的stages 。 它允许将作业分为不同的阶段，并且同一stage作业可以并行执行（取决于特定条件 ）。\nunittest: stage: test script: - echo \u0026quot;run test\u0026quot; interfacetest: stage: test script: - echo \u0026quot;run test\u0026quot; \n可能遇到的问题： 阶段并没有并行运行。\n在这里我把这两个阶段在同一个 runner 运行了，所以需要修改 runner 每次运行的作业数量。默认是 1，改为 10.\n\nvim /etc/gitlab-runner/config.toml 更改后自动加载无需重启。\nconcurrent = 10 3.8 variables 定义变量，pipeline 变量、job 变量、Runner 变量。job 变量优先级最大。\n3.9 综合实例 综合实例:\nbefore_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com stages: - build - test - codescan - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build script: - echo \u0026quot;mvn clean \u0026quot; - echo \u0026quot;mvn install\u0026quot; - echo \u0026quot;$DOMAIN\u0026quot; after_script: - echo \u0026quot;after script in buildjob\u0026quot; unittest: stage: test script: - echo \u0026quot;run test\u0026quot; deploy: stage: deploy script: - echo \u0026quot;hello deploy\u0026quot; - sleep 2; codescan: stage: codescan script: - echo \u0026quot;codescan\u0026quot; - sleep 5; after_script: - echo \u0026quot;after-script\u0026quot; - ech 实验效果\n\n可能遇到的问题： pipeline 卡主,为降低复杂性目前没有学习 tags，所以流水线是在共享的 runner 中运行的。需要设置共享的 runner 运行没有 tag 的作业。\n\n3.10 tags 用于从允许运行该项目的所有 Runner 列表中选择特定的 Runner,在 Runner 注册期间，您可以指定 Runner 的标签。\ntags可让您使用指定了标签的 runner 来运行作业,此 runner 具有 ruby 和 postgres 标签。\njob: tags: - ruby - postgres 给定带有osx标签的 OS X Runner 和带有windows标签的 Windows Runner，以下作业将在各自的平台上运行。\nwindows job: stage: - build tags: - windows script: - echo Hello, %USERNAME%! osx job: stage: - build tags: - osx script: - echo \u0026quot;Hello, $USER!\u0026quot; \n3.11 allow_failure allow_failure允许作业失败，默认值为false 。启用后，如果作业失败，该作业将在用户界面中显示橙色警告. 但是，管道的逻辑流程将认为作业成功/通过，并且不会被阻塞。 假设所有其他作业均成功，则该作业的阶段及其管道将显示相同的橙色警告。但是，关联的提交将被标记为”通过”，而不会发出警告。\njob1: stage: test script: - execute_script_that_will_fail allow_failure: true \n3.12 when on_success前面阶段中的所有作业都成功（或由于标记为allow_failure而被视为成功）时才执行作业。 这是默认值。\non_failure当前面阶段出现失败则执行。\nalways -执行作业，而不管先前阶段的作业状态如何，放到最后执行。总是执行。\n3.13 manual 手动 manual -手动执行作业,不会自动执行，需要由用户显式启动. 手动操作的示例用法是部署到生产环境. 可以从管道，作业，环境和部署视图开始手动操作。\n此时在 deploy 阶段添加 manual，则流水线运行到 deploy 阶段为锁定状态，需要手动点击按钮才能运行 deploy 阶段。\n\n3.14 delayed 延迟 delayed 延迟一定时间后执行作业（在 GitLab 11.14 中已添加）。\n有效值'5',10 seconds,30 minutes, 1 day, 1 week 。\n\n实验 demo\nbefore_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com stages: - build - test - codescan - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build script: - echo \u0026quot;mvn clean \u0026quot; - echo \u0026quot;mvn install\u0026quot; - echo \u0026quot;$DOMAIN\u0026quot; after_script: - echo \u0026quot;after script in buildjob\u0026quot; unittest: stage: test script: - ech \u0026quot;run test\u0026quot; when: delayed start_in: '30' allow_failure: true deploy: stage: deploy script: - echo \u0026quot;hello deploy\u0026quot; - sleep 2; when: manual codescan: stage: codescan script: - echo \u0026quot;codescan\u0026quot; - sleep 5; when: on_success after_script: - echo \u0026quot;after-script\u0026quot; - ech 3.15 retry 配置在失败的情况下重试作业的次数。\n当作业失败并配置了retry ，将再次处理该作业，直到达到retry关键字指定的次数。如果retry设置为 2，并且作业在第二次运行成功（第一次重试），则不会再次重试. retry值必须是一个正整数，等于或大于 0，但小于或等于 2（最多两次重试，总共运行 3 次）.\nunittest: stage: test retry: 2 script: - ech \u0026quot;run test\u0026quot; \n默认情况下，将在所有失败情况下重试作业。为了更好地控制retry哪些失败，可以是具有以下键的哈希值：\n max ：最大重试次数. when ：重试失败的案例.  根据错误原因设置重试的次数。\nalways ：在发生任何故障时重试（默认）. unknown_failure ：当失败原因未知时。 script_failure ：脚本失败时重试。 api_failure ：API失败重试。 stuck_or_timeout_failure ：作业卡住或超时时。 runner_system_failure ：运行系统发生故障。 missing_dependency_failure: 如果依赖丢失。 runner_unsupported ：Runner不受支持。 stale_schedule ：无法执行延迟的作业。 job_execution_timeout ：脚本超出了为作业设置的最大执行时间。 archived_failure ：作业已存档且无法运行。 unmet_prerequisites ：作业未能完成先决条件任务。 scheduler_failure ：调度程序未能将作业分配给运行scheduler_failure。 data_integrity_failure ：检测到结构完整性问题。 3.16 实验 定义当出现脚本错误重试两次，也就是会运行三次。\nunittest: stage: test tags: - build only: - master script: - ech \u0026quot;run test\u0026quot; retry: max: 2 when: - script_failure 效果\n\n3.17 timeout 超时  job 配置超时时间  特定作业配置超时，作业级别的超时可以超过项目级别的超时，但不能超过 Runner 特定的超时。\nbuild: script: build.sh timeout: 3 hours 30 minutes test: script: rspec timeout: 3h 30m  项目设置流水线超时时间  超时定义了作业可以运行的最长时间（以分钟为单位）。 这可以在项目的**“设置”\u0026gt;” CI / CD”\u0026gt;“常规管道”设置下进行配置** 。 默认值为 60 分钟。\n\n runner 超时时间  此类超时（如果小于项目定义的超时 ）将具有优先权。此功能可用于通过设置大超时（例如一个星期）来防止 Shared Runner 被项目占用。未配置时，Runner 将不会覆盖项目超时。\n\n此功能如何工作：\n示例 1-运行程序超时大于项目超时\nrunner 超时设置为 24 小时，项目的 CI / CD 超时设置为 2 小时。该工作将在 2 小时后超时。\n示例 2-未配置运行程序超时\nrunner 不设置超时时间，项目的 CI / CD 超时设置为 2 小时。该工作将在 2 小时后超时。\n示例 3-运行程序超时小于项目超时\nrunner 超时设置为 30 分钟，项目的 CI / CD 超时设置为 2 小时。工作在 30 分钟后将超时\n3.18 parallel 配置要并行运行的作业实例数,此值必须大于或等于 2 并且小于或等于 50。\n这将创建 N 个并行运行的同一作业实例. 它们从job_name 1/N到job_name N/N依次命名。\ncodescan: stage: codescan tags: - build only: - master script: - echo \u0026quot;codescan\u0026quot; - sleep 5; parallel: 5 \n3.19 综合实例 before_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com stages: - build - test - codescan - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build script: - echo \u0026quot;mvn clean \u0026quot; - echo \u0026quot;mvn install\u0026quot; - echo \u0026quot;$DOMAIN\u0026quot; after_script: - echo \u0026quot;after script in buildjob\u0026quot; unittest: stage: test script: - ech \u0026quot;run test\u0026quot; when: delayed start_in: '5' allow_failure: true retry: max: 1 when: - script_failure timeout: 1 hours 10 minutes deploy: stage: deploy script: - echo \u0026quot;hello deploy\u0026quot; - sleep 2; when: manual codescan: stage: codescan script: - echo \u0026quot;codescan\u0026quot; - sleep 5; when: on_success parallel: 5 after_script: - echo \u0026quot;after-script\u0026quot; - ech 3.20 only \u0026amp; except only 和 except 是两个参数用分支策略来限制 jobs 构建：\n  only定义哪些分支和标签的 git 项目将会被 job 执行。\n  except定义哪些分支和标签的 git 项目将不会被 job 执行。\njob: # use regexp only: - /^issue-.*$/ # use special keyword except: - branches   3.21 rules rules允许按顺序评估单个规则对象的列表，直到一个匹配并为作业动态提供属性. 请注意， rules不能与only/except组合使用。\n可用的规则条款包括：\n if （类似于only:variables ） changes （ only:changes相同） exists  3.21.1 rules:if 如果DOMAIN的值匹配，则需要手动运行。不匹配on_success。 条件判断从上到下，匹配即停止。多条件匹配可以使用\u0026amp;\u0026amp; ||\nvariables: DOMAIN: example.com codescan: stage: codescan tags: - build script: - echo \u0026quot;codescan\u0026quot; - sleep 5; #parallel: 5 rules: - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' when: manual - when: on_success 3.21.2 rules:changes 接受文件路径数组。 如果提交中Jenkinsfile文件发生的变化则为 true。\ncodescan: stage: codescan tags: - build script: - echo \u0026quot;codescan\u0026quot; - sleep 5; #parallel: 5 rules: - changes: - Jenkinsfile when: manual - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' when: manual - when: on_success 3.21.3 rules:exists 接受文件路径数组。当仓库中存在指定的文件时操作。\ncodescan: stage: codescan tags: - build script: - echo \u0026quot;codescan\u0026quot; - sleep 5; #parallel: 5 rules: - exists: - Jenkinsfile when: manual - changes: - Jenkinsfile when: on_success - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' when: on_success - when: on_success 3.21.4 rules:allow_failure 使用allow_failure: true rules:在不停止管道本身的情况下允许作业失败或手动作业等待操作.\njob: script: \u0026quot;echo Hello, Rules!\u0026quot; rules: - if: '$CI_MERGE_REQUEST_TARGET_BRANCH_NAME == \u0026quot;master\u0026quot;' when: manual allow_failure: true 在此示例中，如果第一个规则匹配，则作业将具有以下when: manual和allow_failure: true。\n3.22 workflow:rules 顶级workflow:关键字适用于整个管道，并将确定是否创建管道。when ：可以设置为always或never . 如果未提供，则默认值always。\nvariables: DOMAIN: example.com workflow: rules: - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' - when: always 3.23 综合实例 before_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com workflow: rules: - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' when: always - when: never stages: - build - test - codescan - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build script: - echo \u0026quot;mvn clean \u0026quot; - echo \u0026quot;mvn install\u0026quot; - ech \u0026quot;$DOMAIN\u0026quot; after_script: - echo \u0026quot;after script in buildjob\u0026quot; rules: - exists: - Dockerfile when: on_success allow_failure: true - changes: - Dockerfile when: manual - when: on_failure unittest: stage: test script: - ech \u0026quot;run test\u0026quot; when: delayed start_in: '5' allow_failure: true retry: max: 1 when: - script_failure timeout: 1 hours 10 minutes deploy: stage: deploy script: - echo \u0026quot;hello deploy\u0026quot; - sleep 2; rules: - if: '$DOMAIN == \u0026quot;example.com\u0026quot;' when: manual - if: '$DOMAIN == \u0026quot;aexample.com\u0026quot;' when: delayed start_in: '5' - when: on_failure codescan: stage: codescan script: - echo \u0026quot;codescan\u0026quot; - sleep 5; when: on_success parallel: 5 after_script: - echo \u0026quot;after-script\u0026quot; - ech 3.24 cache 缓存 用来指定需要在 job 之间缓存的文件或目录。只能使用该项目工作空间内的路径。不要使用缓存在阶段之间传递工件，因为缓存旨在存储编译项目所需的运行时依赖项。\n如果在 job 范围之外定义了cache ，则意味着它是全局设置，所有 job 都将使用该定义。如果未全局定义或未按 job 定义则禁用该功能。\n3.24.1 cache:paths 使用paths指令选择要缓存的文件或目录，路径是相对于项目目录，不能直接链接到项目目录之外。\n$CI_PROJECT_DIR 项目目录\n在 job build 中定义缓存，将会缓存 target 目录下的所有.jar 文件。\nbuild: script: test cache: paths: - target/*.jar 当在全局定义了 cache:paths 会被 job 中覆盖。以下实例将缓存 binaries 目录。\ncache: paths: - my/files build: script: echo \u0026quot;hello\u0026quot; cache: key: build paths: - target/ 由于缓存是在 job 之间共享的，如果不同的 job 使用不同的路径就出现了缓存覆盖的问题。如何让不同的 job 缓存不同的 cache 呢？设置不同的cache:key。\n3.24.2 cache:key 缓存标记 为缓存做个标记，可以配置 job、分支为 key 来实现分支、作业特定的缓存。为不同 job 定义了不同的 cache:key 时， 会为每个 job 分配一个独立的 cache。cache:key变量可以使用任何预定义变量，默认default ，从 GitLab 9.0 开始，默认情况下所有内容都在管道和作业之间共享。\n按照分支设置缓存\ncache: key: ${CI_COMMIT_REF_SLUG} files： 文件发生变化自动重新生成缓存(files 最多指定两个文件)，提交的时候检查指定的文件。\n根据指定的文件生成密钥计算 SHA 校验和，如果文件未改变值为 default。\ncache: key: files: - Gemfile.lock - package.json paths: - vendor/ruby - node_modules prefix: 允许给定 prefix 的值与指定文件生成的秘钥组合。\n在这里定义了全局的 cache，如果文件发生变化则值为 rspec-xxx111111111222222 ，未发生变化为 rspec-default。\ncache: key: files: - Gemfile.lock prefix: ${CI_JOB_NAME} paths: - vendor/ruby rspec: script: - bundle exec rspec 例如，添加$CI_JOB_NAME prefix将使密钥看起来像： rspec-feef9576d21ee9b6a32e30c5c79d0a0ceb68d1e5 ，并且作业缓存在不同分支之间共享，如果分支更改了Gemfile.lock ，则该分支将为cache🔑files具有新的 SHA 校验和. 将生成一个新的缓存密钥，并为该密钥创建一个新的缓存. 如果Gemfile.lock未发生变化 ，则将前缀添加default ，因此示例中的键为rspec-default 。\n3.24.3 cache:policy 策略 默认：在执行开始时下载文件，并在结束时重新上传文件。称为” pull-push缓存策略.\npolicy: pull 跳过下载步骤\npolicy: push 跳过上传步骤\nstages: - setup - test prepare: stage: setup cache: key: gems paths: - vendor/bundle script: - bundle install --deployment rspec: stage: test cache: key: gems paths: - vendor/bundle policy: pull script: - bundle exec rspec ... 3.25 综合实例(一) 全局缓存 before_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com cache: paths: - target/ stages: - build - test - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build tags: - build only: - master script: - ls - id - mvn clean package -DskipTests - ls target - echo \u0026quot;$DOMAIN\u0026quot; - false \u0026amp;\u0026amp; true ; exit_code=$? - if [ $exit_code -ne 0 ]; then echo \u0026quot;Previous command failed\u0026quot;; fi; - sleep 2; after_script: - echo \u0026quot;after script in job\u0026quot; unittest: stage: test tags: - build only: - master script: - echo \u0026quot;run test\u0026quot; - echo 'test' \u0026gt;\u0026gt; target/a.txt - ls target retry: max: 2 when: - script_failure deploy: stage: deploy tags: - build only: - master script: - echo \u0026quot;run deploy\u0026quot; - ls target retry: max: 2 when: - script_failure after_script: - echo \u0026quot;after-script\u0026quot; Pipeline 日志分析\nbuild 作业运行时会对项目代码打包，然后生成 target 目录。作业结束创建缓存。\n\n开始第二个作业 test，此时会把当前目录中的 target 目录删除掉（因为做了 git 对比）。\n\n获取到第一个作业生成的缓存 target 目录。\n\n开始第三个作业，同样先删除了 target 目录，然后获取了第二个作业的缓存。最后生成了当前的缓存。\n\nRunner 缓存\n在做本次实验的时候我现在本地 runner 清除了项目的工作目录和历史缓存。\n[root@zeyang-nuc-service ~]# cd /home/gitlab-runner/builds/1Cxihk7-/0/demo/demo-maven-service/ [root@zeyang-nuc-service demo-maven-service]# ls Jenkinsfile README.md aaaaa jenkins pom.xml src target [root@zeyang-nuc-service demo-maven-service]# cd .. [root@zeyang-nuc-service demo]# ls demo-maven-service demo-maven-service.tmp [root@zeyang-nuc-service demo]# rm -fr demo-maven-service [root@zeyang-nuc-service demo]# rm -fr demo-maven-service.tmp/ [root@zeyang-nuc-service demo]# cd [root@zeyang-nuc-service ~]# cd /home/gitlab-runner/cache/ [root@zeyang-nuc-service cache]# ls demo [root@zeyang-nuc-service cache]# rm -rf * 项目代码默认不会删除，可以发现是第二次作业的缓存。（因为上面的例子中第三次作业并没有修改缓存内容）\n[root@zeyang-nuc-service cache]# cd /home/gitlab-runner/builds/1Cxihk7-/0/demo/demo-maven-service/ [root@zeyang-nuc-service demo-maven-service]# ls Jenkinsfile README.md aaaaa jenkins pom.xml src target [root@zeyang-nuc-service demo-maven-service]# cd .. [root@zeyang-nuc-service demo]# ls demo-maven-service demo-maven-service.tmp [root@zeyang-nuc-service demo]# rm -fr * [root@zeyang-nuc-service demo]# ls [root@zeyang-nuc-service demo]# ls demo-maven-service demo-maven-service.tmp [root@zeyang-nuc-service demo]# cd demo-maven-service [root@zeyang-nuc-service demo-maven-service]# ls Jenkinsfile README.md aaaaa jenkins pom.xml src target [root@zeyang-nuc-service demo-maven-service]# cat target/a.txt test 进入 runner 缓存目录中查看缓存。\n[root@zeyang-nuc-service ~]# cd /home/gitlab-runner/cache/demo/demo-maven-service/default/ [root@zeyang-nuc-service default]# ls cache.zip [root@zeyang-nuc-service default]# unzip cache.zip Archive: cache.zip creating: target/ inflating: target/a.txt creating: target/classes/ creating: target/classes/com/ creating: target/classes/com/mycompany/ creating: target/classes/com/mycompany/app/ inflating: target/classes/com/mycompany/app/App.class creating: target/maven-archiver/ inflating: target/maven-archiver/pom.properties creating: target/maven-status/ creating: target/maven-status/maven-compiler-plugin/ creating: target/maven-status/maven-compiler-plugin/compile/ creating: target/maven-status/maven-compiler-plugin/compile/default-compile/ inflating: target/maven-status/maven-compiler-plugin/compile/default-compile/createdFiles.lst inflating: target/maven-status/maven-compiler-plugin/compile/default-compile/inputFiles.lst creating: target/maven-status/maven-compiler-plugin/testCompile/ creating: target/maven-status/maven-compiler-plugin/testCompile/default-testCompile/ inflating: target/maven-status/maven-compiler-plugin/testCompile/default-testCompile/createdFiles.lst inflating: target/maven-status/maven-compiler-plugin/testCompile/default-testCompile/inputFiles.lst inflating: target/my-app-1.1-SNAPSHOT.jar creating: target/test-classes/ creating: target/test-classes/com/ creating: target/test-classes/com/mycompany/ creating: target/test-classes/com/mycompany/app/ inflating: target/test-classes/com/mycompany/app/AppTest.class [root@zeyang-nuc-service default]# ls cache.zip target [root@zeyang-nuc-service default]# cd target/ [root@zeyang-nuc-service target]# ls a.txt classes maven-archiver maven-status my-app-1.1-SNAPSHOT.jar test-classes [root@zeyang-nuc-service target]# cat a.txt test 此时此刻再次运行流水线作业，第一个作业用的是上个作业最后生成的缓存。\n\n进入 runner 缓存目录查看,cache.zip 时间已经发生的变化。\n[root@zeyang-nuc-service default]# ll total 12 -rw------- 1 gitlab-runner gitlab-runner 9172 Apr 29 10:27 cache.zip drwxrwxr-x 6 root root 127 Apr 29 10:05 target 结论： 全局缓存生效于未在作业中定义缓存的所有作业，这种情况如果每个作业都对缓存目录做了更改，会出现缓存被覆盖的场景。\n3.25 综合实例（二） 控制缓存策略\n例如 build 阶段我们需要生成新的 target 目录内容，可以优化设置 job 运行时不下载缓存。\n\nbefore_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com cache: paths: - target/ stages: - build - test - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build tags: - build only: - master script: - ls - id - cat target/a.txt - mvn clean package -DskipTests - ls target - echo \u0026quot;$DOMAIN\u0026quot; - false \u0026amp;\u0026amp; true ; exit_code=$? - if [ $exit_code -ne 0 ]; then echo \u0026quot;Previous command failed\u0026quot;; fi; - sleep 2; after_script: - echo \u0026quot;after script in job\u0026quot; cache: policy: pull #不下载缓存 unittest: stage: test tags: - build only: - master script: - echo \u0026quot;run test\u0026quot; - echo 'test' \u0026gt;\u0026gt; target/a.txt - ls target - cat target/a.txt retry: max: 2 when: - script_failure deploy: stage: deploy tags: - build only: - master script: - cat target/a.txt - echo \u0026quot;run deploy\u0026quot; - ls target - echo \u0026quot;deploy\u0026quot; \u0026gt;\u0026gt; target/a.txt retry: max: 2 when: - script_failure after_script: - echo \u0026quot;after-script\u0026quot; 3.26 artifacts 用于指定在作业成功或者失败时应附加到作业的文件或目录的列表。作业完成后，工件将被发送到 GitLab，并可在 GitLab UI 中下载。\n3.26.1 artifacts:paths 路径是相对于项目目录的，不能直接链接到项目目录之外。\n将制品目录设置为 target 目录\nartifacts: paths: - target/ \n禁用工件传递\njob: stage: build script: make build dependencies: [] 您可能只想为标记的发行版创建构件，以避免用临时构建构件填充构建服务器存储。仅为标签创建工件（ default-job不会创建工件）：\ndefault-job: script: - mvn test -U except: - tags release-job: script: - mvn package -U artifacts: paths: - target/*.war only: - tags 3.26.2 artifacts:expose_as 关键字expose_as可用于在合并请求 UI 中公开 job 的制品文件。\n例如，要匹配单个文件：\n1 2 3 4 5 6 7  test:script:- echo1artifacts:expose_as:\u0026#34;artifact 1\u0026#34;paths:- path/to/file.txt  使用此配置，GitLab 将在指向的相关合并请求中添加链接file1.txt。\n\n查看制品文件\n\n请注意以下几点：\n 每个合并请求最多可以公开 10 个作业工件。 如果指定了目录，那么如果目录中有多个文件，则该链接将指向指向作业工件浏览器。 如果开启 GitlabPages 可以对.html .htm .txt .json .log 扩展名单个文件工件渲染工件。  3.26.3 artifacts:name 通过name指令定义所创建的工件存档的名称。可以为每个归档使用唯一的名称。 artifacts:name变量可以使用任何预定义变量。默认名称是artifacts，下载artifacts改为artifacts.zip。\n使用当前作业的名称创建档案\njob: artifacts: name: \u0026quot;$CI_JOB_NAME\u0026quot; paths: - binaries/ 使用内部分支或标记的名称（仅包括 binaries 目录）创建 artifacts，\njob: artifacts: name: \u0026quot;$CI_COMMIT_REF_NAME\u0026quot; paths: - binaries/ 使用当前作业的名称和当前分支或标记（仅包括二进制文件目录）创建 artifacts\njob: artifacts: name: \u0026quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME\u0026quot; paths: - binaries/ 要创建一个具有当前阶段名称和分支名称的 artifacts\njob: artifacts: name: \u0026quot;$CI_JOB_STAGE-$CI_COMMIT_REF_NAME\u0026quot; paths: - binaries/ 3.26.4 artifacts:when 用于在作业失败时或尽管失败而上传工件。on_success 仅在作业成功时上载工件。这是默认值。on_failure 仅在作业失败时上载工件。always 上载工件，无论作业状态如何。\n要仅在作业失败时上传工件：\njob: artifacts: when: on_failure 3.26.5 artifacts:expire_in 制品的有效期，从上传和存储到 GitLab 的时间开始算起。如果未定义过期时间，则默认为 30 天。\nexpire_in的值以秒为单位的经过时间，除非提供了单位。可解析值的示例：\n‘42’ ‘3 mins 4 sec’ ‘2 hrs 20 min’ ‘2h20min’ ‘6 mos 1 day’ ‘47 yrs 6 mos and 4d’ ‘3 weeks and 2 days’ 一周后过期\njob: artifacts: expire_in: 1 week 3.26.6 artifacts:reports 用于从作业中收集测试报告，代码质量报告和安全报告. 在 GitLab 的 UI 中显示这些报告。\n**注意：**无论作业结果（成功或失败），都将收集测试报告。\n artifacts:reports:junit  收集 junit 单元测试报告，收集的 JUnit 报告将作为工件上传到 GitLab，并将自动显示在合并请求中。\nbuild: stage: build tags: - build only: - master script: - mvn test - mvn cobertura:cobertura - ls target artifacts: name: \u0026quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME\u0026quot; when: on_success expose_as: 'artifact 1' paths: - target/*.jar reports: junit: target/surefire-reports/TEST-*.xml **注意：**如果您使用的 JUnit 工具导出到多个 XML 文件，则可以在一个作业中指定多个测试报告路径，它们将被自动串联到一个文件中. 使用文件名模式（ junit: rspec-*.xml ），文件名数组（ junit: [rspec-1.xml, rspec-2.xml, rspec-3.xml] ）或其组合（ junit: [rspec.xml, test-results/TEST-*.xml] ）。\n\n\n如果无法显示此页面，需要更改系统设置。此选项可能会加大资源占用，默认禁用了需要启用。\n登录gitlab su - git $ gitlab-rails console -------------------------------------------------------------------------------- GitLab: 12.9.0 (9a382ff2c82) FOSS GitLab Shell: 12.0.0 PostgreSQL: 10.12 -------------------------------------------------------------------------------- Feature.enable(:junit_pipeline_view)Loading production environment (Rails 6.0.2) irb(main):001:0\u0026gt; irb(main):002:0\u0026gt; irb(main):003:0\u0026gt; Feature.enable(:junit_pipeline_view) =\u0026gt; true irb(main):004:0\u0026gt; 参考链接：https://docs.gitlab.com/ee/ci/junit_test_reports.html\n artifacts:reports:cobertura  收集的 Cobertura 覆盖率报告将作为工件上传到 GitLab，并在合并请求中自动显示。\nbuild: stage: build tags: - build only: - master script: - mvn test - mvn cobertura:cobertura - ls target artifacts: name: \u0026quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME\u0026quot; when: on_success expose_as: 'artifact 1' paths: - target/*.jar reports: junit: target/surefire-reports/TEST-*.xml cobertura: target/site/cobertura/coverage.xml $ gitlab-rails console -------------------------------------------------------------------------------- GitLab: 12.9.0 (9a382ff2c82) FOSS GitLab Shell: 12.0.0 PostgreSQL: 10.12 -------------------------------------------------------------------------------- Loading production environment (Rails 6.0.2) irb(main):001:0\u0026gt; irb(main):002:0\u0026gt; irb(main):003:0\u0026gt; Feature.enable(:coverage_report_view) =\u0026gt; true  maven 集成 cobertura 插件  \u0026lt;plugins\u0026gt; \u0026lt;!-- cobertura plugin start --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.codehaus.mojo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cobertura-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;formats\u0026gt; \u0026lt;format\u0026gt;html\u0026lt;/format\u0026gt; \u0026lt;format\u0026gt;xml\u0026lt;/format\u0026gt; \u0026lt;/formats\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- cobertura plugin end --\u0026gt; \u0026lt;/plugins\u0026gt; 执行 mvn cobertura:cobertura 运行测试并产生 Cobertura 覆盖率报告。\n参考链接：https://docs.gitlab.com/12.9/ee/user/project/merge_requests/test_coverage_visualization.html\n备注: 实验未做出效果，具体问题待排查。\n\n3.27 dependencies 定义要获取工件的作业列表，只能从当前阶段之前执行的阶段定义作业。定义一个空数组将跳过下载该作业的任何工件不会考虑先前作业的状态，因此，如果它失败或是未运行的手动作业，则不会发生错误。\n\n如果设置为依赖项的作业的工件已过期或删除，那么依赖项作业将失败。\n3.28 综合实例 before_script: - echo \u0026quot;before-script!!\u0026quot; variables: DOMAIN: example.com cache: paths: - target/ stages: - build - test - deploy build: before_script: - echo \u0026quot;before-script in job\u0026quot; stage: build tags: - build only: - master script: - ls - id - mvn test - mvn cobertura:cobertura - ls target - echo \u0026quot;$DOMAIN\u0026quot; - false \u0026amp;\u0026amp; true ; exit_code=$? - if [ $exit_code -ne 0 ]; then echo \u0026quot;Previous command failed\u0026quot;; fi; - sleep 2; after_script: - echo \u0026quot;after script in job\u0026quot; artifacts: name: \u0026quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME\u0026quot; when: on_success #expose_as: 'artifact 1' paths: - target/*.jar #- target/surefire-reports/TEST*.xml reports: junit: target/surefire-reports/TEST-*.xml cobertura: target/site/cobertura/coverage.xml coverage: '/Code coverage: \\d+\\.\\d+/' unittest: dependencies: - build stage: test tags: - build only: - master script: - echo \u0026quot;run test\u0026quot; - echo 'test' \u0026gt;\u0026gt; target/a.txt - ls target retry: max: 2 when: - script_failure deploy: stage: deploy tags: - build only: - master script: - echo \u0026quot;run deploy\u0026quot; - ls target retry: max: 2 when: - script_failure after_script: - echo \u0026quot;after-script\u0026quot; 3.29 needs 并行阶段 可无序执行作业，无需按照 stages 顺序运行某些 jobs，可以让多个 stage 同时运行。\nstages: - build - test - deploy module-a-build: stage: build script: - echo \u0026quot;hello3a\u0026quot; - sleep 10 module-b-build: stage: build script: - echo \u0026quot;hello3b\u0026quot; - sleep 10 module-a-test: stage: test script: - echo \u0026quot;hello3a\u0026quot; - sleep 10 needs: [\u0026quot;module-a-build\u0026quot;] module-b-test: stage: test script: - echo \u0026quot;hello3b\u0026quot; - sleep 10 needs: [\u0026quot;module-b-build\u0026quot;] \n如果needs:设置为指向因only/except规则而未实例化的作业，或者不存在，则创建管道时会出现 YAML 错误。\n暂时限制了作业在needs:可能需要的最大作业数分配,ci_dag_limit_needs功能标志已启用（默认）分配 10 个，如果功能被禁用为 50。\nFeature::disable(:ci_dag_limit_needs) # 50 Feature::enable(:ci_dag_limit_needs) #10 3.30 制品下载 在使用needs，可通过artifacts: true或artifacts: false来控制工件下载。 默认不指定为 true。\nmodule-a-test: stage: test script: - echo \u0026quot;hello3a\u0026quot; - sleep 10 needs: - job: \u0026quot;module-a-build\u0026quot; artifacts: true 相同项目中的管道制品下载,通过将project关键字设置为当前项目的名称，并指定引用，可以使用needs从当前项目的不同管道中下载工件。在下面的示例中，build_job将使用other-refref 下载最新成功的build-1作业的工件：\nbuild_job: stage: build script: - ls -lhR needs: - project: group/same-project-name job: build-1 ref: other-ref artifacts: true 不支持从parallel:运行的作业中下载工件。\n3.31 include https://gitlab.com/gitlab-org/gitlab/-/tree/master/lib/gitlab/ci/templates\n可以允许引入外部 YAML 文件，文件具有扩展名.yml或.yaml 。使用合并功能可以自定义和覆盖包含本地定义的 CI / CD 配置。相同的 job 会合并，参数值以源文件为准。\n3.32 local 引入同一存储库中的文件，使用相对于根目录的完整路径进行引用，与配置文件在同一分支上使用。\nci/localci.yml: 定义一个作业用于发布。\nstages: - deploy deployjob: stage: deploy script: - echo 'deploy' .gitlab-ci.yml 引入本地的 CI 文件’ci/localci.yml’。\ninclude: local: 'ci/localci.yml' stages: - build - test - deploy buildjob: stage: build script: ls testjob: stage: test script: ls 效果\n\n3.33 file 包含来自另一个项目的文件\ninclude: - project: demo/demo-java-service ref: master file: '.gitlab-ci.yml' 3.34 template 只能使用官方提供的模板 https://gitlab.com/gitlab-org/gitlab/tree/master/lib/gitlab/ci/templates\ninclude: - template: Auto-DevOps.gitlab-ci.yml 3.35 remote 用于通过 HTTP / HTTPS 包含来自其他位置的文件，并使用完整 URL 进行引用. 远程文件必须可以通过简单的 GET 请求公开访问，因为不支持远程 URL 中的身份验证架构。\ninclude: - remote: 'https://gitlab.com/awesome-project/raw/master/.gitlab-ci-template.yml' 3.36 extends 继承模板 jobs\nstages: - test variables: RSPEC: 'test' .tests: script: echo \u0026quot;mvn test\u0026quot; stage: test only: refs: - branches testjob: extends: .tests script: echo \u0026quot;mvn clean test\u0026quot; only: variables: - $RSPEC 合并后\ntestjob: stage: test script: mvn clean test only: variables: - $RSPEC refs: - branches 3.37 extends \u0026amp; include aa.yml\n#stages: # - deploy deployjob: stage: deploy script: - echo 'deploy' only: - dev .template: stage: build script: - echo \u0026quot;build\u0026quot; only: - master include: local: 'ci/localci.yml' stages: - test - build - deploy variables: RSPEC: 'test' .tests: script: echo \u0026quot;mvn test\u0026quot; stage: test only: refs: - branches testjob: extends: .tests script: echo \u0026quot;mvn clean test\u0026quot; only: variables: - $RSPEC newbuildjob: script: - echo \u0026quot;123\u0026quot; extends: .template 这将运行名为useTemplate的作业，该作业运行echo Hello! 如.template作业中所定义，并使用本地作业中所定义的alpine Docker 映像.\n3.38 trigger 管道触发 当 GitLab 从trigger定义创建的作业启动时，将创建一个下游管道。允许创建多项目管道和子管道。将trigger与when:manual一起使用会导致错误。\n  多项目管道： 跨多个项目设置流水线，以便一个项目中的管道可以触发另一个项目中的管道。[微服务架构]\n  父子管道: 在同一项目中管道可以触发一组同时运行的子管道,子管道仍然按照阶段顺序执行其每个作业，但是可以自由地继续执行各个阶段，而不必等待父管道中无关的作业完成。\n  3.39 多项目管道 当前面阶段运行完成后，触发 demo/demo-java-service 项目 master 流水线。创建上游管道的用户需要具有对下游项目的访问权限。如果发现下游项目用户没有访问权限以在其中创建管道，则staging作业将被标记为失败。\nstaging: variables: ENVIRONMENT: staging stage: deploy trigger: project: demo/demo-java-service branch: master strategy: depend project关键字，用于指定下游项目的完整路径。该branch关键字指定由指定的项目分支的名称。使用variables关键字将变量传递到下游管道。 全局变量也会传递给下游项目。上游管道优先于下游管道。如果在上游和下游项目中定义了两个具有相同名称的变量，则在上游项目中定义的变量将优先。默认情况下，一旦创建下游管道，trigger作业就会以success状态完成。strategy: depend将自身状态从触发的管道合并到源作业。\n\n在下游项目中查看管道信息\n\n在此示例中，一旦创建了下游管道，该staging将被标记为成功。\n3.40 父子管道 创建子管道 ci/child01.yml\nstages: - build child-a-build: stage: build script: - echo \u0026quot;hello3a\u0026quot; - sleep 10 在父管道触发子管道\nstaging2: variables: ENVIRONMENT: staging stage: deploy trigger: include: ci/child01.yml strategy: depend \n准备工作注册 docker 类型的 runner\ngitlab-runner register \\ --non-interactive \\ --executor \u0026quot;docker\u0026quot; \\ --docker-image alpine:latest \\ --url \u0026quot;http://192.168.1.200:30088/\u0026quot; \\ --registration-token \u0026quot;JRzzw2j1Ji6aBjwvkxAv\u0026quot; \\ --description \u0026quot;docker-runner\u0026quot; \\ --tag-list \u0026quot;newdocker\u0026quot; \\ --run-untagged=\u0026quot;true\u0026quot; \\ --locked=\u0026quot;false\u0026quot; \\ --docker-privileged \\ --access-level=\u0026quot;not_protected\u0026quot; [[runners]] name = \u0026quot;docker-runner\u0026quot; url = \u0026quot;http://192.168.1.200:30088/\u0026quot; token = \u0026quot;xuaLZD7xUVviTsyeJAWh\u0026quot; executor = \u0026quot;docker\u0026quot; [runners.custom_build_dir] [runners.cache] [runners.cache.s3] [runners.cache.gcs] [runners.docker] pull_policy = \u0026quot;if-not-present\u0026quot; tls_verify = false image = \u0026quot;alpine:latest\u0026quot; privileged = true disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\u0026quot;/cache\u0026quot;] shm_size = 0 3.41 image 默认在注册 runner 的时候需要填写一个基础的镜像，请记住一点只要使用执行器为 docker 类型的 runner 所有的操作运行都会在容器中运行。 如果全局指定了 images 则所有作业使用此 image 创建容器并在其中运行。 全局未指定 image，再次查看 job 中是否有指定，如果有此 job 按照指定镜像创建容器并运行，没有则使用注册 runner 时指定的默认镜像。\n#image: maven:3.6.3-jdk-8 before_script: - ls build: image: maven:3.6.3-jdk-8 stage: build tags: - newdocker script: - ls - sleep 2 - echo \u0026quot;mvn clean \u0026quot; - sleep 10 deploy: stage: deploy tags: - newdocker script: - echo \u0026quot;deploy\u0026quot; 3.42 services 工作期间运行的另一个 Docker 映像，并 link 到image关键字定义的 Docker 映像。这样，您就可以在构建期间访问服务映像.\n服务映像可以运行任何应用程序，但是最常见的用例是运行数据库容器，例如mysql 。与每次安装项目时都安装mysql相比，使用现有映像并将其作为附加容器运行更容易，更快捷。\nservices: - name: mysql:latest alias: mysql-1 3.43 environment deploy to production: stage: deploy script: git push production HEAD:master environment: name: production url: https://prod.example.com 3.44 inherit 使用或禁用全局定义的环境变量（variables）或默认值(default)。\n使用 true、false 决定是否使用，默认为 true\ninherit: default: false variables: false 继承其中的一部分变量或默认值使用 list\ninherit: default: - parameter1 - parameter2 variables: - VARIABLE1 - VARIABLE2 4 模板库设计 为了实现模板复用，减少重复代码。本次课程开始我们将使用模板库来完成流水线。开始之前还是要把语法学好便于进一步实施。\n创建一个 git 仓库用于存放模板demo/demo-gitlabci-service ，然后创建一个 template 目录存放所有 pipeline 的模板，创建一个 jobs 目录存放 job 模板。\n这样我们可以将一些 maven、ant、gradle、npm 工具通过一个 job 模板和不同的构建命令实现。templates 的好处是我们在其中定义了模板流水线，这些流水线可以直接让项目使用。当遇到个性化项目的时候就可以在当前项目创建.gitlab-ci.yml 文件来引用模板文件，再进一步实现个性化需要。\n4.1 模板库信息 github : https://github.com/zeyangli/gitlabci-templates\n4.2 项目信息    视频中仓库地址 github 仓库地址 备注     demo/demo-maven-service zeyangli/gitlabci-demo-maven-service 测试 ci 语法项目   cidevops-java-service zeyangli/gitlabci-cidevops-java-service 构建工具集成项目   cidevops-npm-service zeyangli/gitlabci-cidevops-npm-service npm 集成项目   cidevops-interfacetest-service zeyangli/gitlabci-cidevops-interfacetest-service 自动化测试集成   cidevops-gitlabci-service zeyangli/gitlabci-templates 模板库项目    4.3 集成构建工具 构建工具是用来将代码编译打包成制品的工具。例如前端项目我们一般使用 npm 进行打包，后端 java 项目我们一般使用 maven、gradle 进行打包。构建工具很多很多，但是集成到 gitlab 中是一样的。所以这里简单介绍使用 gitlabCI 集成 npm/maven 完成前后端项目的构建。\n4.4 软件包下载  maven 软件包下载 gradle 软件包下载 ant 软件包下载 node 软件包下载  4.5 环境配置 首先，我们需要在 runner 机器中安装配置好 apache-maven。\n#解压 tar zxf apache-maven-xxxx.tar.gz -C /usr/local tar zxf gradle-xxxx.tar.gz -C /usr/local tar zxf node-xxxxx.tar.gz -C /usr/local tar zxf apache-ant-xxxx.tar.gz -C /usr/local #添加环境变量 vim /etc/profile export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export ANT_HOME=/usr/local/apache-ant-1.10.5 export GRADLE_HOME=/usr/local/gradle-5.3 export NODE_HOME=/usr/local/node-v10.15.3-linux-x64 export JAVA_HOME=/usr/local/jdk1.8.0_201 export PATH=$PATH:$MAVEN_HOME/bin:$ANT_HOME/bin:$GRADLE_HOME/bin:$NODE_HOME/bin export PATH=$PATH:$JAVA_HOME/bin # 生效全局环境变量 source /etc/profile 4.6 maven 集成模板库配置 我们首先在 jobs 目录中创建一个 build.yml，然后在里面编写 build 作业模板。\n.build: stage: build tags: - build script: - $BUILD_SHELL - ls 我们计划将测试相关的 job 都定义在 jobs/test.yml 中，我们开始创建并编写 test 作业。运行测试 shell 并收集单元测试报告。\n#单元测试 .test: stage: test tags: - build script: - $TEST_SHELL - ls artifacts: reports: junit: ${JUNIT_REPORT_PATH} 然后我们在 template 目录中创建 maven 流水线模板。 templates/java-pipeline.yml\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/test.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' ##构建命令 CACHE_DIR: 'target/' TEST_SHELL : 'mvn test' ##测试命令 JUNIT_REPORT_PATH: 'target/surefire-reports/TEST-*.xml' ##单元测试报告 cache: paths: - ${CACHE_DIR} stages: - build - test build: stage: build extends: .build test: stage: test extends: .test 最后我们在项目中添加.gitlab-ci.yml 来引用模板构建流水线。\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'templates/java-pipeline.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' TEST_SHELL: 'mvn test' CACHE_DIR: 'target/' \n好的，上面我们已经实现了构建，但是一般还回运行单元测试。接下来我们实现。\n4.7 npm 集成模板配置 template/web-pipeline.yml\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' variables: BUILD_SHELL: 'npm run build' ##构建命令 CACHE_DIR : \u0026quot;dist/\u0026quot; ##构建缓存 cache: paths: - ${CACHE_DIR} - node_modules/ stages: - install - build install: stage: install script: - 'npm install' build: stage: build extends: .build 4.8 集成代码质量 sonarqube 4.8.1 内容  SonarQube 基础简介 配置 GitLabCI 代码扫描 配置 GitLabCI 合并扫描  4.8.2 准备工作 参考链接：https://docs.sonarqube.org/latest/analysis/gitlab-cicd/\n扩展插件： https://github.com/mc1arke/sonarqube-community-branch-plugin/releases\n参考文章：参考文章链接\nGitlab 内置环境变量： http://192.168.1.200:30088/help/ci/variables/README#variables\n在 SonarQube 中创建项目组添加用户，为用户分配权限。使用用户 token 分析扫描项目。\n4.8.3 准备用户 \n4.8.4 创建群组 \n4.8.5 创建权限模板 \n4.8.6 分配权限 一般给这个组中的成员管理员权限\n\n4.8.7 为项目授权权限模板 \n4.8.8 添加 token \n4.8.9 扫描分析 jobs/codeanalysis.yml\n.codeanalysis-java: stage: code_analysis tags: - build script: - echo $CI_MERGE_REQUEST_IID $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME $CI_MERGE_REQUEST_TARGET_BRANCH_NAME - \u0026quot;$SCANNER_HOME/bin/sonar-scanner -Dsonar.projectKey=${CI_PROJECT_NAME} \\ -Dsonar.projectName=${CI_PROJECT_NAME} \\ -Dsonar.projectVersion=${CI_COMMIT_REF_NAME} \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription=${CI_PROJECT_TITLE} \\ -Dsonar.links.homepage=${CI_PROJECT_URL} \\ -Dsonar.sources=${SCAN_DIR} \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries=target/test-classes \\ -Dsonar.java.surefire.report=target/surefire-reports \\ -Dsonar.branch.name=${CI_COMMIT_REF_NAME}\u0026quot; artifacts: paths: - \u0026quot;$ARTIFACT_PATH\u0026quot; template/java-pipeline.yml\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/test.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/codeanalysis.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' ##构建命令 CACHE_DIR: 'target/' TEST_SHELL : 'mvn test' ##测试命令 JUNIT_REPORT_PATH: 'target/surefire-reports/TEST-*.xml' ##单元测试报告 # 代码扫描 SCANNER_HOME : \u0026quot;/usr/local/buildtools/sonar-scanner-3.2.0.1227-linux\u0026quot; SCAN_DIR : \u0026quot;src\u0026quot; ARTIFACT_PATH : 'target/*.jar' ##制品目录 cache: paths: - ${CACHE_DIR} stages: - build - test - code_analysis build: stage: build extends: .build rules: - when: on_success test: stage: test extends: .test rules: - when: on_success code_analysis: stage: code_analysis extends: .codeanalysis-java 实现效果\n\n4.8.10 Pull request 集成 配置 SonarQube，添加 gitlabtoken 和服务信息。系统设置 -\u0026gt; pull request。\n注意下面的配置在配置文件中定义不生效哦，可能是因为版本的问题导致的。暂且忽略。\ncom.github.mc1arke.sonarqube.plugin.branch.pullrequest.gitlab.token=b8Gs1quX5GSeHwyuMWyY com.github.mc1arke.sonarqube.plugin.branch.pullrequest.gitlab.url=http://192.168.1.200:30088 sonar.pullrequest.provider=GitlabServer 如果你想通过 API 操作可以参考：\ncurl -u \u0026quot;$SONAR_API_TOKEN\u0026quot; -X POST \u0026quot;http://sonarqube.example.com/api/settings/set?key=sonar.pullrequest.provider\u0026amp;value=GitlabServer\u0026quot; curl -u \u0026quot;$SONAR_API_TOKEN\u0026quot; -X POST \u0026quot;http://sonarqube.example.com/api/settings/set?key=com.github.mc1arke.sonarqube.plugin.branch.pullrequest.gitlab.url\u0026amp;value=http://gitlab.example.com\u0026quot; curl -u \u0026quot;$SONAR_API_TOKEN\u0026quot; -X POST \u0026quot;http://sonarqube.example.com/api/settings/set?key=com.github.mc1arke.sonarqube.plugin.branch.pullrequest.gitlab.token\u0026amp;value= $ GITLAB_TOKEN\u0026quot; \n添加扫描作业，主要是分析参数。\n.codeanalysis-mr: stage: code_analysis only: - merge_requests tags: - build script: - \u0026quot;$SCANNER_HOME/bin/sonar-scanner -Dsonar.projectKey=${CI_PROJECT_NAME} \\ -Dsonar.projectName=${CI_PROJECT_NAME} \\ -Dsonar.projectVersion=${CI_COMMIT_REF_NAME} \\ -Dsonar.ws.timeout=30 \\ -Dsonar.projectDescription=${CI_PROJECT_TITLE} \\ -Dsonar.links.homepage=${CI_PROJECT_URL} \\ -Dsonar.sources=${SCAN_DIR} \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries=target/test-classes \\ -Dsonar.java.surefire.report=target/surefire-reports \\ -Dsonar.pullrequest.key=${CI_MERGE_REQUEST_IID} \\ -Dsonar.pullrequest.branch=${CI_MERGE_REQUEST_SOURCE_BRANCH_NAME} \\ -Dsonar.pullrequest.base=${CI_MERGE_REQUEST_TARGET_BRANCH_NAME} \\ -Dsonar.gitlab.ref_name=${CI_COMMIT_REF_NAME} \\ -Dsonar.gitlab.commit_sha=${CI_COMMIT_SHA} \\ -Dsonar.gitlab.project_id=${CI_PROJECT_PATH} \\ -Dsonar.pullrequest.gitlab.repositorySlug=$CI_PROJECT_ID \u0026quot; #-Dsonar.branch.name=${CI_COMMIT_REF_NAME} -X \u0026quot; templates/java-pipeline.yml\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/test.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/codeanalysis.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' ##构建命令 CACHE_DIR: 'target/' TEST_SHELL : 'mvn test' ##测试命令 JUNIT_REPORT_PATH: 'target/surefire-reports/TEST-*.xml' ##单元测试报告 # 代码扫描 SCANNER_HOME : \u0026quot;/usr/local/buildtools/sonar-scanner-3.2.0.1227-linux\u0026quot; SCAN_DIR : \u0026quot;src\u0026quot; ARTIFACT_PATH : 'target/*.jar' ##制品目录 cache: paths: - ${CACHE_DIR} stages: - build - test - code_analysis build: stage: build extends: .build rules: - when: on_success test: stage: test extends: .test rules: - when: on_success code_analysis: stage: code_analysis extends: .codeanalysis-java codeanalysis_mr: stage: code_analysis extends: .codeanalysis-mr 创建合并请求运行流水线，最终效果。\n\n4.9 制品库集成 4.9.1 artifactory .build: stage: build tags: - build script: - $BUILD_SHELL - ls .deploy-artifact: stage: deploy-artifact tags: - build script: - curl -u${ARTIFACT_USER}:${ARTIFACT_PASSWD} -T ${ARTIFACT_PATH} \u0026quot;$ARTIFACTORY_URL/$ARTIFACTORY_NAME/$TARGET_FILE_PATH/$TARGET_ARTIFACT_NAME\u0026quot; 4.9.2 定义变量 \ntemplate\ninclude: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/test.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/codeanalysis.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' ##构建命令 CACHE_DIR: 'target/' TEST_SHELL : 'mvn test' ##测试命令 JUNIT_REPORT_PATH: 'target/surefire-reports/TEST-*.xml' ##单元测试报告 # 代码扫描 SCANNER_HOME : \u0026quot;/usr/local/buildtools/sonar-scanner-3.2.0.1227-linux\u0026quot; SCAN_DIR : \u0026quot;src\u0026quot; ARTIFACT_PATH : 'target/*.jar' ##制品目录 #上传制品库 ARTIFACTORY_URL: \u0026quot;http://192.168.1.200:30082/artifactory\u0026quot; ARTIFACTORY_NAME: \u0026quot;cidevops\u0026quot; TARGET_FILE_PATH: \u0026quot;$CI_PROJECT_NAMESPACE/$CI_PROJECT_NAME/$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID\u0026quot; TARGET_ARTIFACT_NAME: \u0026quot;$CI_PROJECT_NAME-$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID.jar\u0026quot; cache: paths: - ${CACHE_DIR} stages: - build - test - parallel01 build: stage: build extends: .build rules: - when: on_success test: stage: test extends: .test rules: - when: on_success code_analysis: stage: parallel01 extends: .codeanalysis-java codeanalysis_mr: stage: parallel01 extends: .codeanalysis-mr deploy_artifact: stage: parallel01 extends: .deploy-artifact \n\n4.9.3 下载制品 .down-artifact: stage: down-artifact tags: - build script: - curl -u${ARTIFACT_USER}:${ARTIFACT_PASSWD} -O \u0026quot;$ARTIFACTORY_URL/$ARTIFACTORY_NAME/$TARGET_FILE_PATH/$TARGET_ARTIFACT_NAME\u0026quot; - ls template\ndown_artifact: stage: down_artifact extends: .down-artifact \n\n4.9.4 镜像仓库 https://cr.console.aliyun.com/cn-beijing/instances/repositories\n\n.build-docker: stage: buildimage tags: - build script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWD $CI_REGISTRY - docker build -t ${IMAGE_NAME} -f ${DOCKER_FILE_PATH} . - docker push ${IMAGE_NAME} - docker rmi ${IMAGE_NAME} include: - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/build.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/test.yml' - project: 'cidevops/cidevops-gitlabci-service' ref: master file: 'jobs/codeanalysis.yml' variables: BUILD_SHELL: 'mvn clean package -DskipTests' ##构建命令 CACHE_DIR: 'target/' TEST_SHELL : 'mvn test' ##测试命令 JUNIT_REPORT_PATH: 'target/surefire-reports/TEST-*.xml' ##单元测试报告 # 代码扫描 SCANNER_HOME : \u0026quot;/usr/local/buildtools/sonar-scanner-3.2.0.1227-linux\u0026quot; SCAN_DIR : \u0026quot;src\u0026quot; ARTIFACT_PATH : 'target/*.jar' ##制品目录 #上传制品库 ARTIFACTORY_URL: \u0026quot;http://192.168.1.200:30082/artifactory\u0026quot; ARTIFACTORY_NAME: \u0026quot;cidevops\u0026quot; TARGET_FILE_PATH: \u0026quot;$CI_PROJECT_NAMESPACE/$CI_PROJECT_NAME/$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID\u0026quot; TARGET_ARTIFACT_NAME: \u0026quot;$CI_PROJECT_NAME-$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID.jar\u0026quot; #构建镜像 CI_REGISTRY: 'registry.cn-beijing.aliyuncs.com' CI_REGISTRY_USER: '610556220zy' #CI_REGISTRY_PASSWD: 'xxxxxxxx.' IMAGE_NAME: \u0026quot;$CI_REGISTRY/$CI_PROJECT_PATH:$CI_COMMIT_REF_NAME-$CI_COMMIT_SHORT_SHA-$CI_PIPELINE_ID\u0026quot; DOCKER_FILE_PATH: \u0026quot;./Dockerfile\u0026quot; cache: paths: - ${CACHE_DIR} stages: - build - test - parallel01 - down_artifact build: stage: build extends: .build rules: - when: on_success test: stage: test extends: .test rules: - when: on_success code_analysis: stage: parallel01 extends: .codeanalysis-java codeanalysis_mr: stage: parallel01 extends: .codeanalysis-mr deploy_artifact: stage: parallel01 extends: .deploy-artifact down_artifact: stage: down_artifact extends: .down-artifact build_image: stage: parallel01 extends: .build-docker \n\n4.10 自动化测试集成 4.10.1 开启 gitlab pages root# vim /etc/gitlab/gitlab.rb ##! Define to enable GitLab Pages pages_external_url \u0026quot;http://pages.gitlab.com/\u0026quot; gitlab_pages['enable'] = true gitlab_pages['inplace_chroot'] = true gitlab-ctl reconfigure 更新 gitlab.yml 文件\ncontainers: - name: gitlab image: gitlab/gitlab-ce:12.9.0-ce.0 imagePullPolicy: IfNotPresent ports: - containerPort: 30088 name: web protocol: TCP - containerPort: 22 name: agent protocol: TCP - containerPort: 80 name: page protocol: TCP 开放 80 端口\nkind: Service apiVersion: v1 metadata: labels: k8s-app: gitlab name: gitlab namespace: devops spec: type: NodePort ports: - name: web port: 30088 targetPort: 30088 nodePort: 30088 - name: slave port: 22 targetPort: 22 nodePort: 30022 - name: page port: 80 targetPort: 80 nodePort: 80 selector: k8s-app: gitlab \n完整的 yaml 参考 github 中。\nFAQ：未开启 chroot\nhttps://gitlab.com/gitlab-org/gitlab-pages/-/issues/129\n\u0026quot;Failed to bind mount /var/opt/gitlab/gitlab-rails/shared/pages on /tmp/gitlab-pages-1524473513642136363/pages. operation not permitted 4.10.2 运行自动化测试 在这里定义了两个 stage，interface_test 作业用于运行自动化测试，此时自动化测试已经配置好 ant+jmeter 集成所以直接运行 ant 命令即可。考虑到每个人安装的 jmeter 环境目录不一致所以可以通过-D 选项指定 jmeterhome。运行完成接口测试后，测试报告在项目当前目录的result/htmlfile中。在此将测试报告整理成制品存放。\npages 作业用于将测试报告中的 html 文件通过 pages 功能展示。首先获取 interface_test 作业的制品，然后将测试报告移动到 public 目录中。最后将 public 目录作为制品收集，有效期 30 天。\njmeter 安装好了，之后需要将/usr/local/apache-jmeter-5.1.1/extras 目录中的 ant-jmeter-1.1.1.jar 复制到 ant 安装目录下的 lib 中。\nstages: - tests - deploy interface_test: stage: tests tags: - build script: - ant -Djmeter.home=/usr/local/buildtools/apache-jmeter-5.2.1 artifacts: paths: - result/htmlfile/ pages: stage: deploy dependencies: - interface_test script: - mv result/htmlfile/ public/ artifacts: paths: - public expire_in: 30 days only: - master 效果：\n\n4.10.3 上下游项目触发自动化测试 jobs/test.yml\n.interfacetest: stage: interface_test trigger: project: cidevops/cidevops-interfacetest-service branch: master strategy: depend templates.yml\ninterfact_test: stage: interface_test extends: .interfacetest \n4.11 Kubernetes 集成 4.11.1 基于 kubernetes 部署 runner 4.11.1.1 安装 helm3 https://github.com/helm/helm/releases tar -zxvf helm-v3.0.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm 4.11.1.2 配置 chart 存储库 ## 添加chart存储库 helm repo add gitlab https://charts.gitlab.io ## 验证源 helm repo list ##查询可以安装的gitlab-runner chart helm search repo -l gitlab/gitlab-runner 4.11.1.3 更新配置信息 ## 获取相关版本的chart包 helm fetch gitlab/gitlab-runner --version=0.15.0 [root@zeyang-nuc-service ~]# ls Desktop es Documents gitlab-runner-0.15.0.tgz values.yml\n## GitLab Runner Image ## ## By default it's using gitlab/gitlab-runner:alpine-v{VERSION} ## where {VERSION} is taken from Chart.yaml from appVersion field ## ## ref: https://hub.docker.com/r/gitlab/gitlab-runner/tags/ ## image: gitlab/gitlab-runner:alpine-v12.9.0 ## 镜像下载策略 imagePullPolicy: IfNotPresent ## Gitlab服务器地址 gitlabUrl: http://192.168.1.200:30088/ ## runner注册token runnerRegistrationToken: \u0026quot;JRzzw2j1Ji6aBjwvkxAv\u0026quot; ## 终止之前注销所有跑步者 unregisterRunners: true ## 当停止管道时等待其作业终止时间 terminationGracePeriodSeconds: 3600 ## Set the certsSecretName in order to pass custom certficates for GitLab Runner to use ## Provide resource name for a Kubernetes Secret Object in the same namespace, ## this is used to populate the /home/gitlab-runner/.gitlab-runner/certs/ directory ## ref: https://docs.gitlab.com/runner/configuration/tls-self-signed.html#supported-options-for-self-signed-certificates ## # certsSecretName: ## 配置最大并发作业数 concurrent: 10 ## 新作业检查间隔 checkInterval: 30 ## GitlabRunner日志级别 debug, info, warn, error, fatal, panic logLevel: info ## Configure GitLab Runner's logging format. Available values are: runner, text, json ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-global-section ## # logFormat: ## For RBAC support: rbac: create: true ## Define specific rbac permissions. resources: [\u0026quot;pods\u0026quot;, \u0026quot;pods/exec\u0026quot;, \u0026quot;secrets\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;create\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;delete\u0026quot;] ## Run the gitlab-bastion container with the ability to deploy/manage containers of jobs ## cluster-wide or only within namespace clusterWideAccess: false ## Use the following Kubernetes Service Account name if RBAC is disabled in this Helm chart (see rbac.create) ## # serviceAccountName: default ## Specify annotations for Service Accounts, useful for annotations such as eks.amazonaws.com/role-arn ## ## ref: https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html ## # serviceAccountAnnotations: {} ## Configure integrated Prometheus metrics exporter ## ref: https://docs.gitlab.com/runner/monitoring/#configuration-of-the-metrics-http-server metrics: enabled: true ## Configuration for the Pods that that the runner launches for each new job ## runners: ## Default container image to use for builds when none is specified ## image: ubuntu:16.04 ## Specify one or more imagePullSecrets ## ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # imagePullSecrets: [] ## Specify the image pull policy: never, if-not-present, always. The cluster default will be used if not set. ## imagePullPolicy: \u0026quot;if-not-present\u0026quot; ## Defines number of concurrent requests for new job from GitLab ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section ## 限制来自GitLab的对新作业的并发请求数 requestConcurrency: 1 ## Specify whether the runner should be locked to a specific project: true, false. Defaults to true. ## locked: false ## Specify the tags associated with the runner. Comma-separated list of tags. ## ## ref: https://docs.gitlab.com/ce/ci/runners/#using-tags ## tags: \u0026quot;kubernetes-runner,k8s\u0026quot; ## Specify if jobs without tags should be run. ## If not specified, Runner will default to true if no tags were specified. In other case it will ## default to false. ## ## ref: https://docs.gitlab.com/ce/ci/runners/#allowing-runners-with-tags-to-pick-jobs-without-tags ## runUntagged: true ## Specify whether the runner should only run protected branches. ## Defaults to False. ## ## ref: https://docs.gitlab.com/ee/ci/runners/#protected-runners ## protected: false ## Run all containers with the privileged flag enabled ## This will allow the docker:dind image to run if you need to run Docker ## commands. Please read the docs before turning this on: ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html#using-docker-dind ## privileged: true ## The name of the secret containing runner-token and runner-registration-token # secret: gitlab-runner ## Namespace to run Kubernetes jobs in (defaults to the same namespace of this release) ## # namespace: ## The amount of time, in seconds, that needs to pass before the runner will ## timeout attempting to connect to the container it has just created. ## ref: https://docs.gitlab.com/runner/executors/kubernetes.html pollTimeout: 180 ## Set maximum build log size in kilobytes, by default set to 4096 (4MB) ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-section outputLimit: 4096 ## Distributed runners caching ## ref: https://gitlab.com/gitlab-org/gitlab-runner/blob/master/docs/configuration/autoscale.md#distributed-runners-caching ## ## If you want to use s3 based distributing caching: ## First of all you need to uncomment General settings and S3 settings sections. ## ## Create a secret 's3access' containing 'accesskey' \u0026amp; 'secretkey' ## ref: https://aws.amazon.com/blogs/security/wheres-my-secret-access-key/ ## ## $ kubectl create secret generic s3access \\ ## --from-literal=accesskey=\u0026quot;YourAccessKey\u0026quot; \\ ## --from-literal=secretkey=\u0026quot;YourSecretKey\u0026quot; ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ ## ## If you want to use gcs based distributing caching: ## First of all you need to uncomment General settings and GCS settings sections. ## ## Access using credentials file: ## Create a secret 'google-application-credentials' containing your application credentials file. ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runnerscachegcs-section ## You could configure ## $ kubectl create secret generic google-application-credentials \\ ## --from-file=gcs-application-credentials-file=./path-to-your-google-application-credentials-file.json ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ ## ## Access using access-id and private-key: ## Create a secret 'gcsaccess' containing 'gcs-access-id' \u0026amp; 'gcs-private-key'. ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html#the-runners-cache-gcs-section ## You could configure ## $ kubectl create secret generic gcsaccess \\ ## --from-literal=gcs-access-id=\u0026quot;YourAccessID\u0026quot; \\ ## --from-literal=gcs-private-key=\u0026quot;YourPrivateKey\u0026quot; ## ref: https://kubernetes.io/docs/concepts/configuration/secret/ cache: {} ## General settings # cacheType: s3 # cachePath: \u0026quot;gitlab_runner\u0026quot; # cacheShared: true ## S3 settings # s3ServerAddress: s3.amazonaws.com # s3BucketName: # s3BucketLocation: # s3CacheInsecure: false # secretName: s3access ## GCS settings # gcsBucketName: ## Use this line for access using access-id and private-key # secretName: gcsaccess ## Use this line for access using google-application-credentials file # secretName: google-application-credentials ## Build Container specific configuration ## builds: {} # cpuLimit: 200m # memoryLimit: 256Mi # cpuRequests: 100m # memoryRequests: 128Mi ## Service Container specific configuration ## services: {} # cpuLimit: 200m # memoryLimit: 256Mi # cpuRequests: 100m # memoryRequests: 128Mi ## Helper Container specific configuration ## helpers: {} # cpuLimit: 200m # memoryLimit: 256Mi # cpuRequests: 100m # memoryRequests: 128Mi # image: gitlab/gitlab-runner-helper:x86_64-latest ## Service Account to be used for runners ## # serviceAccountName: ## If Gitlab is not reachable through $CI_SERVER_URL ## # cloneUrl: ## Specify node labels for CI job pods assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## # nodeSelector: {} ## Specify pod labels for CI job pods ## # podLabels: {} ## Specify annotations for job pods, useful for annotations such as iam.amazonaws.com/role # podAnnotations: {} ## Configure environment variables that will be injected to the pods that are created while ## the build is running. These variables are passed as parameters, i.e. `--env \u0026quot;NAME=VALUE\u0026quot;`, ## to `gitlab-runner register` command. ## ## Note that `envVars` (see below) are only present in the runner pod, not the pods that are ## created for each build. ## ## ref: https://docs.gitlab.com/runner/commands/#gitlab-runner-register ## # env: # NAME: VALUE ## Configure securitycontext ## ref: http://kubernetes.io/docs/user-guide/security-context/ ## securityContext: fsGroup: 65533 runAsUser: 100 ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # memory: 256Mi # cpu: 200m # requests: # memory: 128Mi # cpu: 100m ## Affinity for pod assignment ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## affinity: {} ## Node labels for pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} # Example: The gitlab runner manager should not run on spot instances so you can assign # them to the regular worker nodes only. # node-role.kubernetes.io/worker: \u0026quot;true\u0026quot; ## List of node taints to tolerate (requires Kubernetes \u0026gt;= 1.6) ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] # Example: Regular worker nodes may have a taint, thus you need to tolerate the taint # when you assign the gitlab runner manager with nodeSelector or affinity to the nodes. # - key: \u0026quot;node-role.kubernetes.io/worker\u0026quot; # operator: \u0026quot;Exists\u0026quot; ## Configure environment variables that will be present when the registration command runs ## This provides further control over the registration process and the config.toml file ## ref: `gitlab-runner register --help` ## ref: https://docs.gitlab.com/runner/configuration/advanced-configuration.html ## # envVars: # - name: RUNNER_EXECUTOR # value: kubernetes ## list of hosts and IPs that will be injected into the pod's hosts file hostAliases: [] # Example: # - ip: \u0026quot;127.0.0.1\u0026quot; # hostnames: # - \u0026quot;foo.local\u0026quot; # - \u0026quot;bar.local\u0026quot; # - ip: \u0026quot;10.1.2.3\u0026quot; # hostnames: # - \u0026quot;foo.remote\u0026quot; # - \u0026quot;bar.remote\u0026quot; ## Annotations to be added to manager pod ## podAnnotations: {} # Example: # iam.amazonaws.com/role: \u0026lt;my_role_arn\u0026gt; ## Labels to be added to manager pod ## podLabels: {} # Example: # owner.team: \u0026lt;my_cool_team\u0026gt; ## HPA support for custom metrics: ## This section enables runners to autoscale based on defined custom metrics. ## In order to use this functionality, Need to enable a custom metrics API server by ## implementing \u0026quot;custom.metrics.k8s.io\u0026quot; using supported third party adapter ## Example: https://github.com/directxman12/k8s-prometheus-adapter ## #hpa: {} # minReplicas: 1 # maxReplicas: 10 # metrics: # - type: Pods # pods: # metricName: gitlab_runner_jobs # targetAverageValue: 400m 4.11.1.4 部署 chart ## 创建runner kubectl create ns gitlab-runner helm install gitlab-runner --namespace gitlab-runner ./gitlab-runner ## 更新 helm upgrade gitlab-runner --namespace gitlab-runner ./gitlab-runner 4.11.1.5 效果 gitlab\n\nkubernetes\n\n4.11.1.6 运行流水线测试 image: maven:3.6.3-jdk-8 before_script: - ls services: - name: mysql:latest alias: mysql-1 build: image: maven:3.6.3-jdk-8 stage: build tags: - k8s script: - ls - sleep 2 - echo \u0026quot;mvn clean \u0026quot; - sleep 10 deploy: stage: deploy tags: - k8s script: - echo \u0026quot;deploy\u0026quot; environment: name: production url: http://www.baidu.com 效果\n\n FAQ\n Q1 未创建 rbac\nERROR: Job failed (system failure): pods is forbidden: User \u0026quot;system:serviceaccount:gitlab-runner:default\u0026quot; cannot create resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;gitlab-runner\u0026quot; 4.11.1.7 应用发布集成 创建名称空间\nkubectl create ns cidevops 准备镜像凭据\nkubectl create secret docker-registry cidevops \\ --docker-server=registry.cn-beijing.aliyuncs.com \\ --docker-username=xxxx \\ --docker-password=xxxx \\ --docker-email=test@test.com -n cidevops 4.11.1.8 配置模板 jobs/deploy.yml\n.deploy-k8s: stage: deploy tags: - build script: - sed -i \u0026quot;s#__namespace__#${NAMESPACE}#g\u0026quot; deployment.yaml - sed -i \u0026quot;s#__appname__#${APP_NAME}#g\u0026quot; deployment.yaml - sed -i \u0026quot;s#__containerport__#${CONTAINER_PORT}#g\u0026quot; deployment.yaml - sed -i \u0026quot;s#__nodeport__#${NODE_PORT}#g\u0026quot; deployment.yaml - sed -i \u0026quot;s#__imagename__#${IMAGE_NAME}#g\u0026quot; deployment.yaml - kubectl apply -f deployment.yaml after_script: - sleep 10 - kubectl get pod -n $NAMESPACE template.yml\ndeploy_k8s: stage: deploy extends: .deploy-k8s rules: - if: \u0026quot; $RUN_DEPLOY == 'no' \u0026quot; when: never - if: \u0026quot; $MANUAL_BRANCH == 'master' \u0026quot; when: manual - when: always environment: name: $ENV_NAME url: $ENV_URL gitlab-ci.yml\n #部署k8s NAMESPACE: \u0026quot;$CI_PROJECT_NAMESPACE\u0026quot; APP_NAME: \u0026quot;$CI_PROJECT_NAME\u0026quot; CONTAINER_PORT: 8081 NODE_PORT: 30181 4.11.1.9 最终效果 \n\n5 Gitlab Variables Gitlab runner的环境变量分为：group/subgroup和project 变量，其中group变量是可迭代继承的，最多继承20个subgroup\n5.1 Gitlab 预定值的环境变量 6. GitLab runner examples  CI/CD example gitlab project example  7. GitLab Runner Extends  gitlab schedule:https://docs.gitlab.com/ee/api/pipeline_schedules.html  10 与其它CICD工具比较 10.1 GitLabCI VS Jenkins Jenkins 是一个广泛用于持续集成的可视化 web 自动化工具，jenkins 可以很好的支持各种语言的项目构建，也完全兼容ant、maven、gradle等多种第三方构建工具，同时跟svn、git能无缝集成，也支持直接与知名源代码托管网站，比如github、bitbucket直接集成，而且插件众多，在这么多年的技术积累之后，在国内大部分公司都有使用Jenkins。\ngitlab-CI是gitlab8.0之后自带的一个持续集成系统，中心思想是当每一次push到gitlab的时候，都会触发一次脚本执行，然后脚本的内容包括了测试，编译，部署等一系列自定义的内容。\ngitlab-CI的脚本执行，需要自定义安装对应gitlab-runner来执行，代码push之后，webhook检测到代码变化，就会触发gitlab-CI，分配到各个Runner来运行相应的脚本script。这些脚本有的是测试项目用的，有的是部署用的。\n10.1.1 差异点对比 分支的可配置性\n 使用GitLab CI，新创建的分支无需任何进一步配置即可立即使用CI管道中的已定义作业。 Jenkins 2 基于gitlab的多分支流水线可以实现。相对配置来说gitlab更加方便一些。  定时执行构建\n有时，根据时间触发作业或整个管道会有所帮助。例如，常规的夜间定时构建。\n 使用Jenkins 2可以立即使用。可以在应执行作业或管道的那一刻以cron式语法定义。 GitLab CI没有此功能。但是，可以通过一种变通办法来实现：通过WebAPI使用同一台或另一台服务器上的cronjob触发作业和管道。  尽管使用GitLab CI无法做到这一点，其实如果配置了提交代码即触发流水线，那么最后一次提交的构建在什么时候没有什么不同，反而减少未提交代码的定时构建资源浪费。\n拉取请求支持\n如果很好地集成了存储库管理器和CI / CD平台，您可以看到请求的当前构建状态。使用这种功能，可以避免将代码合并到不起作用或无法正确构建的主分支中。\n Jenkins没有与源代码管理系统进一步集成，需要管理员自行写代码或者插件实现。 GitLab与其CI平台紧密集成，可以方便查看每个打开和关闭拉动请求的运行和完成管道。  权限管理\n从存储库管理器继承的权限管理对于不想为每个服务分别设置每个用户的权限的大型开发人员或组织团体很有用。大多数情况下，两种情况下的权限都是相同的，因此默认情况下应将它们配置在一个位置。\n 由于GitLab与GitLabCI的深度整合，权限可以统一管理。 由于Jenkins 2没有内置的存储库管理器，因此它无法直接在存储库管理器和CI / CD平台之间合并权限。  存储库交互\n GitLab CI是Git存储库管理器GitLab的固定组件，因此在CI / CD流程和存储库功能之间提供了良好的交互。 Jenkins 2与存储库管理器都是松散耦合的，因此在选择版本控制系统时它非常灵活。此外，就像其前身一样，Jenkins 2强调了对插件的支持，以进一步扩展或改善软件的现有功能。  插件管理\n 扩展Jenkins的本机功能是通过插件完成的。插件的维护，保护和升级成本很高。 GitLab是开放式的，任何人都可以直接向代码库贡献更改，一旦合并，它将自动测试并维护每个更改。  10.1.2 优势与劣势 GitLabCI\n 轻量级，不需要复杂的安装手段。 配置简单，与gitlab可直接适配。 实时构建日志十分清晰，UI交互体验很好 使用 YAML 进行配置，任何人都可以很方便的使用。 没有统一的管理界面，无法统筹管理所有项目 配置依赖于代码仓库，耦合度没有Jenkins低  Jenkins\n 编译服务和代码仓库分离，耦合度低 插件丰富，支持语言众多。 有统一的web管理界面。 插件以及自身安装较为复杂。 体量较大，不是很适合小型团队。  10.1.3 实际应用  GitLabCI 有助于DevOps人员，例如敏捷开发中，开发与运维是同一个人，最便捷的开发方式。 JenkinsCI适合在多角色团队中，职责分明、配置与代码分离、插件丰富。  10.2 Gitlab VS Travis end  https://www.sitespeed.io/   ","description":"","id":44,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/gitlabcicd%E7%A0%94%E7%A9%B6%E8%AF%BE%E9%A2%98/"},{"content":"今天开始将在192.168.99.129这台主机上通过Jenkins:2.241版本为例进行Jenkins系列的深入学习与分享。至于Jenkins是干啥的，这些没用的话就不说了，如果你看到我的笔记了，证明你至少是需要Jenkins为你助力的。\n通过WAR包安装Jenkins 在jenkins官网下载地址下载2.241版本的war包,然后使用以下方式安装\n1 2  # 安装 nohup java -jar jenkins.war \u0026amp;   如何你想查看jenkins.war的配置选项，你可以执行以下命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  root@99-129:~# java -jar jenkins.war --help Running from: /root/jenkins.war webroot: $user.home/.jenkins Jenkins Automation Server Engine 2.241 Usage: java -jar jenkins.war [--option=value] [--option=value] Options: --webroot = folder where the WAR file is expanded into. Default is ${JENKINS_HOME}/war --pluginroot = folder where the plugin archives are expanded into. Default is ${JENKINS_HOME}/plugins (NOTE: this option does not change the directory where the plugin archives are stored) --extractedFilesFolder = folder where extracted files are to be located. Default is the temp folder --daemon = fork into background and run as daemon (Unix only) --logfile = redirect log messages to this file --enable-future-java = allows running with new Java versions which are not fully supported (class version 52 and above) --javaHome = Override the JAVA_HOME variable --toolsJar = The location of tools.jar. Default is JAVA_HOME/lib/tools.jar --config = load configuration properties from here. Default is ./winstone.properties --prefix = add this prefix to all URLs (eg http://localhost:8080/prefix/resource). Default is none --commonLibFolder = folder for additional jar files. Default is ./lib --extraLibFolder = folder for additional jar files to add to Jetty classloader --logThrowingLineNo = show the line no that logged the message (slow). Default is false --logThrowingThread = show the thread that logged the message. Default is false --debug = set the level of debug msgs (1-9). Default is 5 (INFO level) ... # 此处因为内容太多，省略   此时打开浏览器输入http://192.168.99.129:8080访问jenkins\n在红色框内找到管理员的初始密码\n1 2  root@99-129:~# cat ./.jenkins/secrets/initialAdminPassword ae44c519786b49b19eaef942715d7991   登录之后，选择插件进行安装，这里推荐安装建议安装的插件，如果出现插件安装失败，建议跳过安装，等登录上去之后，在进行安装插件\n然后就是配置一个管理员账号，登录之后的界面如下:\n这样通过war包安装的Jenkins就完成了。\n使用Docker安装Jenkins 1 2 3  mkdir ~/workspace/ cd $_ docker run -d -p 8080:8080 -v $(PWD)/jenkins3:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock -v ~/.m2:/root/.m2 --name jenkins jenkins/jenkins:2.241   在安装容器运行起来之后，我们通过以下命令获取Jenkins初始账号密码\n1 2  ☸️ devcluster🔥 kube-ops ~/workspaces/jenkins-kubenetes-231  🐳 👉 cat jenkins3/secrets/initialAdminPassword 84a43f93a3494c8aa6dc30a82b2711ed   接下来的步骤如上类似，安装插件，配置用户，然后登录即可。\n设置插件源 在安装成功之后，我们需要下载部分的插件，但是插件在国内下载十分慢，因此我们这里将使用清华的源替换默认的插件源，提高下载速度，我们按照以下的步骤进行配置。\n1  Manage Jenkins \u0026gt; Manage plugins \u0026gt; Advanced \u0026gt; Update Site   将文本框的内容替换为\n1  https://mirror.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json   这样就完成了插件源的替换，亲测下载pipeline插件以及它的依赖插件大概一分钟的时间，还可以接受。大家在安装遇到问题后可以找我咨询。微信devsecopser\n","description":"","id":45,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/jenkins-in-actions/01-jenkins%E5%AE%89%E8%A3%85%E7%AF%87/"},{"content":"显示正在启动,请稍后\u0026hellip; 在Jenkins的安装过程中我们经常会遇到无法正常的访问到Jenkins插件源造成Jenkins的安装过程一直卡着，处于显示正在启动,请稍后...此时我们只有更换一下插件源才有希望顺利的进入安装过程，在jenkins2.241版本中，更换插件源的方式为:\n1 2 3 4 5 6 7 8 9  vim ~/.jenkins/hudson.model.UpdateCenter.xml # 将\u0026lt;url\u0026gt;标签中的地址更换为:http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json,更改后为: cat ~/.jenkins/hudson.model.UpdateCenter.xml \u0026lt;?xml version=\u0026#39;1.1\u0026#39; encoding=\u0026#39;UTF-8\u0026#39;?\u0026gt; \u0026lt;sites\u0026gt; \u0026lt;site\u0026gt; \u0026lt;id\u0026gt;default\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;http://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\u0026lt;/url\u0026gt; \u0026lt;/site\u0026gt;   这样重启Jenkins即可进入插件的安装环节。\ninstallation of 73 plugins in 1 hr 26 min 在版本为Jenkins2.241的初始化安装时，选择已勾选的插件，包括插件之间的依赖关系一共是安装了73个，共耗时1h26min，例如以下安装插件的日志：\n1  2020-06-26 10:53:22.921+0000 [id=67]\tINFO\th.m.UpdateCenter$CompleteBatchJob#run: Completed installation of 73 plugins in 1 hr 26 min   这样的事情我觉得还是经历一次就好了，下次需要安装的时候该怎么办？建议将其的插件保存一个副本，下次在安装的时候将其plugins目录放在需要安装的Jenkins的数据目录下，如果是Docker容器安装的也可以将其挂载到容器中，这样安装过程变得就非常快了。\nLocalization: Chinese (Simplified) 为了解决插件下载缓慢的问题，目前有几种投机取巧的方法：\n拷贝插件 解决场景是避免Jenkins在安装过程中基础功能的插件下慢的问题，再一个是在Jenkins的迁移时候也是可以利用这样的方式进行安装。\n更换插件源 网上大部分人推荐使用清华的插件源来下载插件，但是在测试之后发现还是有些缓慢，但是确实比默认的已经很好了。\n解决下载的最后一公里 2019.11左右，Jenkins中文社区的国内镜像源发布了，具体的可以看Jenkins中文社区的博客，更多的背景信息这里就不在说明了，但是下载插件的速度不得不说基本上快无感知了，当选择了插件进行安装，刷新一下浏览器就显示安装完成了。首先在插件可用区搜索以下插件\n安装完成后会在Jenkins的首页的左下角显示中文社区的字样\n点击进去之后，就会看到中文社区的插件源，然后设置更新中心地址即可\n最后，我们在设置玩插件的地址之后，通过点击立即获取查看是否正常\n这样基本上就解决了后续因为插件安装带来的各种麻烦，如果被插件折磨的很痛苦，现在可以亲自实践一下试试效果啊。\n","description":"","id":46,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/jenkins-in-actions/02-jenkins%E5%AE%89%E8%A3%85%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"},{"content":"用户权限与管理 解决不同的职能部门的人使用通一个Jenkins实例的时候权限分配的问题，实现通过集中认证服务器OpenLDAP进行人员认证，通过权限匹配实现只能访问那些项目或者执行哪些任务。关于OpenLDAP的搭建请参考《轻目录访问协议的开源实现LDAP2.4》\n用户管理 Jenkins用户管理使用较为普遍的为两种:\n Jenkins内置的专有用户数据库 LDAP服务认证  通常使用内置的用户数据库需要我们手动的添加用户，在系统管理-\u0026gt;管理用户-\u0026gt;新建用户处添加\n另外一种则是通过连接远程的LDAP服务器做集中认证，在系统管理-\u0026gt;全局安全配置-\u0026gt;安全域处选择LDAP,然后点击高级配置相关信息，以下作为参考:\n最后通过TEST LDAP SETTING检查是否正常\n用户权限管理 用户的权限管理可以基于两种方式，一个是用户的角色，一个是基于文件夹进行权限划分，对应的插件分别是下图：\n在安装完成之后，打开系统管理-\u0026gt; 全局安全配置会发现授权策略多了两种，分别为:\n我们选择基于角色的策略的时候，会发现在系统管理里面多出一个manage and assign roles的未分类的功能模块，在里面可以完成基本的权限分配\n实现权限分配  将我们的项目复制成如下用于测试：  新建 3 个测试用户：test / develop / product  打开：系统管理 \u0026ndash;\u0026gt; 管理用户\n最终用户格式：\n配置权限：\n打开：系统管理 \u0026ndash;\u0026gt; Manage and Assign Roles\n我们主要使用上面两种。一个用户想要进行操作必须要有两种角色，一种是全局，一种是 Project：\n 创建角色：Manage Roles  分配角色：Assign Roles  说明：我们这三个用户其实代表着三个不同的属性，为了区分我给他定义了三种不同角色。这样以后就可以给每个角色授权不一样的权限。\n当然，我们这里就给了一个全部的只读权限，用户可以登录，并且修改自己的东西。\n创建项目角色：Manage Roles  给用户分配项目权限：Assign Roles  说明：我们给用户分配不同的项目和权限，便于测试对比。\n查看权限效果：  test 用户登录后项目：\ntest 用户项目权限：\ntest 用户权限说明：test 用户登录后能看到 TEST 开头的项目，包括文件夹，但是对于项目，test 用户都只具有执行权限，而没有修改和配置的权限。\ndevelop 用户登录后项目：\ndevelop 用户项目权限：\ndevelop 用户权限说明：可以看到，因为我们多配置了 Config 权限的原因，develop 相比于 test 用户对于分配给自己权限的项目多了修改配置权限。\nproduct 用户登录后项目：\nproduct 用户对于 TEST 项目权限：\nproduct 用户对于 PRODUCT 项目权限：\nproduct 用户授权说明：我们可以看到，PROCUDT 项目由于授权了 config 权限，所以用户能够修改，TEST 项目没用 config 权限，虽然同样是授权给了 product 用户，但是也是只有执行权限而已。\n至此，基本的权限管理大致完成！\n特别注意\n在我们设置用户权限的时候，默认已经包含了管理员角色：\n如果我们一不小心把这个勾去掉了，然后就炸了！\n最终的解决办法是：\n  停止 Jenkins。\n  备份 /data/jenkins/jenkins-data/config.xml 配置文件。\n  修改配置：\n  1 2 3 4  \u0026lt;useSecurity\u0026gt;true\u0026lt;/useSecurity\u0026gt; # 改为 \u0026lt;useSecurity\u0026gt;false\u0026lt;/useSecurity\u0026gt;   删除权限配置：建议文件拿来了使用 nodepad++ 类似的工具修改  删除： 标签及其内部内容。\n删除： 标签及其内部内容。\n启动 jenkins，此时不需要用户名密码，查看设置：  默认没有启动安全，我们需要重新配置我们之前的东西！\n","description":"","id":47,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/jenkins-in-actions/03-jenkins%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E4%B8%8E%E7%AE%A1%E7%90%86/"},{"content":"在jenkins的构建状态栏中显示构建的相关信息\n","description":"","id":48,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/jenkins-in-actions/jenkins%E6%98%BE%E7%A4%BA%E6%9E%84%E5%BB%BA%E6%8F%90%E4%BA%A4%E4%BF%A1%E6%81%AF/"},{"content":"kubernetes核心组件\n  etcd\n  kube\n    ","description":"","id":49,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/kubernetes-in-actions/kubernetes-roadmaps/"},{"content":"kubernetes背景 Kubernetes，又称为 k8s（首字母为 k、首字母与尾字母之间有 8 个字符、尾字母为 s，所以简称 k8s）或者简称为 \u0026ldquo;kube\u0026rdquo; ，是一种可自动实施 Linux 容器操作的开源平台。它可以帮助用户省去应用容器化过程的许多手动部署和扩展操作。也就是说，您可以将运行 Linux 容器的多组主机聚集在一起，由 Kubernetes 帮助您轻松高效地管理这些集群。而且，这些集群可跨公共云、私有云或混合云部署主机。Kubernetes 是理想的托管平台。\nKubernetes 最初由 Google 的工程师开发和设计。Google 是最早研发 Linux 容器技术的企业之一（组建了cgroups），曾公开分享介绍Google 如何将一切都运行于容器之中（这是 Google 云服务背后的技术）。Google 每周会启用超过 20 亿个容器——全都由内部平台 Borg 支撑。Borg 是 Kubernetes 的前身，多年来开发 Borg 的经验教训成了影响 Kubernetes 中许多技术的主要因素。\n红帽是第一批与 Google 合作研发 Kubernetes 的公司之一，作为 Kubernetes 上游项目的第二大贡献者，我们甚至在这个项目启动之前就已参与其中。2015 年，Google 将 Kubernetes 项目捐赠给新成立的云原生计算基金会。\nKuberneters(k8s)是谷歌使用了将近20年的一个云产品，是Borg的一个开源版本。Borg是谷歌的一个久负盛名的内部使用的大规模集群管理系统，它基于容器技术，目的是实现资源管理的自动化，以及跨多个数中心的资源利用率的最大化。十几年来，谷歌一直通过 Borg 系统管理着数量庞大的应用程序集群。由于谷歌员工都签署了保密协议，即便离职也不能泄露Borg的内部设计，所以外界一直无法了解关于它的更多信息。直到2015年4月，传闻许久的 Borg 论文伴随 Kuberneters 的高调宣传被谷歌首次公开，大家才得以了解它的更多内幕。正是由于站在 Borg 这个前辈的肩膀上，吸取了 Borg 过去十年间的经验与教训，所以 Kubernetes 一经开源就一鸣惊人，并迅速称霸了容器技术领域。\nKubernetes 拥有一个庞大且快速增长的生态系统。因为它是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 的服务、支持和工具广泛可用。\nKubernetes 源于希腊语，意为 \u0026ldquo;舵手\u0026rdquo; 或 \u0026ldquo;飞行员\u0026rdquo;。Google 在 2014 年开源了 Kubernetes 项目。Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验的基础上，结合了社区中最好的想法和实践。\nkubernetes时代 通过对比来看一下为什么我们需要kubernetes，为什么新时代需要kubernetes：\n传统部署时代： 早期，将单一的应用服务运行在物理服务器上，无法给服务器的应用程序进行资源的限制，导致物理服务器之间的资源负载使用不均衡，就导致了服务器上的应用程序的性能下降，物理服务器的维护成本变得很高。\n虚拟化部署时代： 作为解决方案，引入了虚拟化功能，它允许您在单个物理服务器的 CPU 上运行多个虚拟机VM。虚拟化功能允许应用程序在 VM 之间隔离，并提供安全级别，因为一个应用程序的信息不能被另一应用程序自由地访问。\n容器部署时代： 容器类似于 VM，但是它们具有轻量级的隔离属性，可以在应用程序之间共享操作系统OS。因此，容器被认为是目前最轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。由于它们与基础架构分离，因此可以跨云和 OS 分发进行移植。而在容器时代，Docker容器引擎是最流行的一个。\n容器因具有许多优势而变得流行起来。下面列出了容器的一些好处：\n 应用程序的构建和部署：与传统部署时代对比，容器镜像创建显得更简便性、效率更高。 持续开发、集成和部署：通过快速简单的回滚(由于镜像不可变性)，提供可靠且频繁的容器镜像构建和部署。 开发与运维分离：在build/deploy而不是在deploy时创建应用程序容器，从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序容器的运行状况和其他指标信息。 能保证开发、测试和生产的环境一致性：即使在便携式的计算机上也能与云上保持相同地运行。 应用服务可以运行在任何支持容器引擎的平台上。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分，并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  为什么需要 Kubernetes，它能做什么? 容器是打包和运行应用程序的最好的一种方式。在生产环境中，您需要管理运行应用程序的容器，并确保不会停机。例如，如果一个容器发生故障，则需要启动另一个容器。如果系统能够自动的处理这种行为，你觉得会不会是更方便。\n这就是 Kubernetes 的救援方法！Kubernetes 为您提供了一个可弹性运行分布式系统的框架。Kubernetes 会满足您的扩展要求、故障转移、部署模式等。例如，Kubernetes 可以轻松地实现金丝雀canary发布。\n 基于容器的应用部署、维护、滚动升级: 不断的将服务运行为按照用户定义服务运行的期望状态 通过service资源对象自实现负载均衡和服务发现: 当出现流量负载的时候，service会将流量按照一定的均衡算法调度到对应的资源组 实现跨机器和跨地域的集群调度: 可以按照用户定义的资源调度方式进行自动化的资源调度 根据资源负载情况实现服务自动伸缩: 获取服务运行负载指标，通过HPA实现服务的自动扩容与伸缩 实现多种类型服务负载，比如有状态和无状态类型服务: 支持例如nginx、filebeat等这种无状态以及MYSQL,Mongodb,Elasticsearch等有状态的集群服务 支持多种类型的存储插件，比如s3,ceph,nfs,glusterfs\u0026hellip; 插件机制保证扩展性：通过在不修改源代码的情况下增加类似如CNI、CRI、CSI、Device Plugin、CRD等扩展来解决越来越多的个性化需求 自身有安全防护和资源的准入机制：k8s的一套安全策略，通过用户或组与k8s上一些资源角色绑定从而实现在单一范围内有权限执行动作 实现多租户应用的支撑力:k8s本身具有多层资源隔离的条件例如集群本身、命令空间、节点、pod与容器 透明的服务注册和服务发现机制: 通过k8s的service资源对象与label资源对象实现基本的服务注册与发现 轻松实现服务版本迭代和金丝雀发布：因为k8s本身的特性，实现canary显得非常简单 实现服务在线扩容能力: k8s通过自身的resize功能结合一些存储卷实现不丢失原来数据的情况下进行在线扩容 多粒度细粒度的实现配额管理能力 支持自定义扩展（核心+外围扩展）API \u0026hellip;  Kubernetes 不是什么 Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。由于 Kubernetes 在容器级别而不是在硬件级别运行，因此它提供了 PaaS 产品共有的一些普遍适用的功能，例如部署、扩展、负载均衡、日志记录和监视。但是，Kubernetes 不是单一的，默认解决方案是可选和可插拔的。Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。\nKubernetes：\n  Kubernetes 不限制支持的应用程序类型。Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。\n  Kubernetes 不部署源代码，也不构建您的应用程序。持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。\n  Kubernetes 不提供应用程序级别的服务作为内置服务，例如中间件消息中间件、数据处理框架Spark、数据库mysql、缓存、集群存储系统Ceph。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制开放服务代理来访问。\n  Kubernetes 不提供日志记录、监控或警报解决方案。它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。\n  Kubernetes 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API，该声明性 API 可以由任意形式的声明性规范所构成。\n  Kubernetes 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。\n  此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。相比之下，Kubernetes 包含一组独立的、可组合的控制过程，这些过程连续地将当前状态驱动到所提供的所需状态。从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用且功能更强大、健壮、弹性和可扩展性。\n  支持的云厂商  华为云CCE 阿里云ACK 腾讯云TKE 谷歌云GCE  ","description":"","id":51,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/kubernetes%E4%B8%93%E8%BE%91-01kubernetes%E6%98%AF%E4%BB%80%E4%B9%88/"},{"content":"kubernetes专辑-02kubernetes炼气期之平台搭建 环境说明    功能名称 IP 配置     k8s-master 192.168.10.231 4c8g   k8s-node1 192.168.10.232 8c16g   K8s-node2 192.168.10.233 8c16g   k8s-node3 192.168.10.234 8c16g   k8s-node4 192.168.10.235 8c16g    环境初始化  更新环境  1 2  yum update -y yum install -y wget vim net-tools epel-release   关闭filewalld  1 2  systemctl disable firewalld systemctl stop firewalld   关闭selinux  1 2 3 4 5 6 7  sed -i \u0026#34;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#34; /etc/selinux/config sed -i \u0026#34;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#34; /etc/sysconfig/selinux if [ `getenforce` == \u0026#34;Enforcing\u0026#34; ];then setenforce 0 else echo \u0026#34;current selinux status...\u0026#34; `getenforce` fi   关闭swap  1 2  swapoff -a sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab   增加主机名解析  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  cat \u0026lt;\u0026lt; EOF /etc/host 192.168.10.231 dev-k8s-01.example.com 192.168.10.232 dev-k8s-02.example.com 192.168.10.233 dev-k8s-03.example.com 192.168.10.234 dev-k8s-04.example.com 192.168.10.235 dev-k8s-05.example.com EOF 6. 优化内核参数 ​```bash cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system   更新Yum源配置  1 2 3 4 5 6 7 8 9 10 11 12 13  mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.`date +%F`.backup wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo yum makecache fast cat \u0026lt;\u0026lt; EOF /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 EOF yum clean all yum makecache fast yum -y update   安装docker  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  yum -y install yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum install -y docker-ce-18.09.9-3.el7 mkdir /etc/docker -pv cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://********.mirror.aliyuncs.com\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;, \u0026#34;log-opts\u0026#34;: { \u0026#34;max-size\u0026#34;: \u0026#34;100m\u0026#34; }, \u0026#34;storage-driver\u0026#34;: \u0026#34;overlay2\u0026#34;, \u0026#34;storage-opts\u0026#34;: [ \u0026#34;overlay2.override_kernel_check=true\u0026#34; ] } EOF systemctl enable --now docker.service   安装初始化工具  1  yum install -y kubeadm kubelet   获取基础镜像  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  KUBE_VERSION=v1.16.0 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.3.15-0 CORE_DNS_VERSION=1.6.2 GCR_URL=k8s.gcr.io ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers images=(kube-proxy:${KUBE_VERSION} kube-scheduler:${KUBE_VERSION} kube-controller-manager:${KUBE_VERSION} kube-apiserver:${KUBE_VERSION} pause:${KUBE_PAUSE_VERSION} etcd:${ETCD_VERSION} coredns:${CORE_DNS_VERSION}) for imageName in ${images[@]} ; do docker pull $ALIYUN_URL/$imageName docker tag $ALIYUN_URL/$imageName $GCR_URL/$imageName docker rmi $ALIYUN_URL/$imageName done   以上10部建议在所有的节点上安装,在node节点上可以不用安装kubeadm\n部署集群 kubeadm初始化集群 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  [root@dev-k8s-01 ~]# sudo kubeadm init \\ \u0026gt; --apiserver-advertise-address 192.168.10.231 \\ \u0026gt; --kubernetes-version=v1.16.0 \\ \u0026gt; --pod-network-cidr=10.244.0.0/16 [init] Using Kubernetes version: v1.16.0 [preflight] Running pre-flight checks [WARNING Service-Kubelet]: kubelet service is not enabled, please run \u0026#39;systemctl enable kubelet.service\u0026#39; [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [dev-k8s-01.example.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.231] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [dev-k8s-01.example.com localhost] and IPs [192.168.10.231 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [dev-k8s-01.example.com localhost] and IPs [192.168.10.231 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 39.003840 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.16\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node dev-k8s-01.example.com as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node dev-k8s-01.example.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [kubelet-check] Initial timeout of 40s passed. [bootstrap-token] Using token: 9nwjok.ykyphybsveka8gev [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.10.231:6443 --token 9nwjok.ykyphybsveka8gev \\  --discovery-token-ca-cert-hash sha256:b92d7553a1da683a315ad2f4f5fcc855e2d630da0c7553467cdf2db3bd25a3ff   初始化kubectl配置文件 1 2 3  [root@dev-k8s-01 ~]# mkdir -p $HOME/.kube [root@dev-k8s-01 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@dev-k8s-01 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config   添加节点  添加192.168.10.232  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  [root@dev-k8s-05 ~]# kubeadm join 192.168.10.231:6443 --token 9pr3rj.0u8m510fai0op75h \\ --discovery-token-ca-cert-hash sha256:b86bdaaa0bed56e846adb0abc625cf29902dec9e3130d0ff7dae42ffb2e13349 [preflight] Running pre-flight checks [WARNING Service-Kubelet]: kubelet service is not enabled, please run \u0026#39;systemctl enable kubelet.service\u0026#39; [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [kubelet-start] Downloading configuration for the kubelet from the \u0026#34;kubelet-config-1.16\u0026#34; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. [root@dev-k8s-05 ~]# systemctl enable kubelet.service Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service. [root@dev-k8s-05 ~]#   如上所示依旧添加192.168.10.233节点\n验证集群状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  [root@dev-k8s-01 ~]# kubectl cluster-info Kubernetes master is running at https://192.168.10.231:6443 KubeDNS is running at https://192.168.10.231:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. ➜ ~ (☸ kubernetes-admin@kubernetes:default) kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME dev-k8s-01.example.com Ready master 14h v1.16.3 192.168.10.231 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.9 dev-k8s-02.example.com Ready \u0026lt;none\u0026gt; 14h v1.16.3 192.168.10.232 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.9 dev-k8s-03.example.com Ready \u0026lt;none\u0026gt; 14h v1.16.3 192.168.10.233 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.9 dev-k8s-04.example.com Ready \u0026lt;none\u0026gt; 14h v1.16.3 192.168.10.234 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-1062.4.1.el7.x86_64 docker://18.9.9 dev-k8s-05.example.com Ready \u0026lt;none\u0026gt; 13h v1.16.3 192.168.10.235 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.el7.x86_64 docker://18.9.9 ➜ ~ (☸ kubernetes-admin@kubernetes:default) kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-5644d7b6d9-96xm6 1/1 Running 0 14h 10.244.3.2 dev-k8s-04.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system coredns-5644d7b6d9-nkb9f 1/1 Running 0 14h 10.244.1.2 dev-k8s-02.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system etcd-dev-k8s-01.example.com 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-apiserver-dev-k8s-01.example.com 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-controller-manager-dev-k8s-01.example.com 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-bhtjc 1/1 Running 0 14h 192.168.10.232 dev-k8s-02.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-h2ltx 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-kh9k9 1/1 Running 0 14h 192.168.10.234 dev-k8s-04.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-lfh46 1/1 Running 0 14h 192.168.10.233 dev-k8s-03.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-proxy-pcm5d 1/1 Running 0 13h 192.168.10.235 dev-k8s-05.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-scheduler-dev-k8s-01.example.com 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   安装插件 安装flannel网络插件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  wget -O /opt/k8sworkspces/kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml FLANNEL_VERSION=v0.11.0 QUAY_URL=quay.io/coreos QINIU_URL=quay-mirror.qiniu.com/coreos images=(flannel:${FLANNEL_VERSION}-amd64 flannel:${FLANNEL_VERSION}-arm64 flannel:${FLANNEL_VERSION}-arm flannel:${FLANNEL_VERSION}-ppc64le flannel:${FLANNEL_VERSION}-s390x) for imageName in ${images[@]} ; do docker pull $QINIU_URL/$imageName docker tag $QINIU_URL/$imageName $QUAY_URL/$imageName docker rmi $QINIU_URL/$imageName done # 也可以只拉去你机器适配的架构版本`rpm -q centos-release` kubectl apply -f /opt/k8sworkspces/kube-flannel.yml #安装flannel ➜ ~ (☸ kubernetes-admin@kubernetes:default) kubectl get pods --all-namespaces -o wide | grep flannel kube-system kube-flannel-ds-amd64-9tnc7 1/1 Running 0 14h 192.168.10.234 dev-k8s-04.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-cjh4s 1/1 Running 0 14h 192.168.10.231 dev-k8s-01.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-fhlk4 1/1 Running 0 13h 192.168.10.235 dev-k8s-05.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-fnfpj 1/1 Running 0 14h 192.168.10.233 dev-k8s-03.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kube-system kube-flannel-ds-amd64-v5qtj 1/1 Running 0 14h 192.168.10.232 dev-k8s-02.example.com \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   krew krew 能够很方便的管理kubectl的插件包,包括安装卸载，查询升级\n安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  ( set -x; cd /opt/k8sworkspces/krew \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/download/v0.3.2/krew.{tar.gz,yaml}\u0026#34; \u0026amp;\u0026amp; tar zxvf krew.tar.gz \u0026amp;\u0026amp; ./krew-\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)_amd64\u0026#34; install \\  --manifest=krew.yaml --archive=krew.tar.gz ) export PATH=\u0026#34;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026#34; [root@dev-k8s-01 krew]# kubectl krew install ca-cert # 安装一个ca-caert的插件 [root@dev-k8s-01 krew]# kubectl ca-cert -----BEGIN CERTIFICATE----- MIICyDCCAbCgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl cm5ldGVzMB4XDTE5MTExNTA0MjEzOVoXDTI5MTExMjA0MjEzOVowFTETMBEGA1UE AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBANEi tPdWINQZfZqM4c/uaOzsBBByn0CLLLmMdiKF4Gpk9proDoR9eOMhQiiVLZ4tFFsb POTwq+MvHe4kEsunl/hBwNbXvGfbvnr+vX9ZsDfU5FT5O55Zryq5jgANDKFChKx9 R91QsbCeQKIWlc9AFdot8ig9LhYTfHJRfMeUBYl5Xzoof8YRMsJ0jOKLWca+oCfd doLKda9VpahU2AEmEFHuD6ctwBGFObadSktoOvr0Gfzo4cXRkjGXp4G1U8O1LLsU HiypNN4m7Romy4tIjPAxDAoDDyjA8OrbPlvJt8Oo0CHcAxFZDJCsKAG1s0nS7PJj vR2ULtIrHAm5QZa8BmMCAwEAAaMjMCEwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB /wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAKAi1Fg/2MlFxPbq9yaNkBhAV2ou /VbbuEJF1c92Tk24cuJV3vuYoTmWIGp1LYTLTW/xcfwFoanLRPBlBONoJRzXLIZD /mmuYMrTaKMwbCz2t4awqQyDb8A3RcgTrSfCWMs0uyvjPVgiJDfMlg0WDJ4kPb3Y SQv7UaaNa57gkEHB1PJy10n1E3gAcb6NVxvly7cHVaJlenZY6mkT40K8zVOXuM/G ausCNXEfEUXED2C8Ippj/sr1TgRlD8Gfi+Xp7XzHTeu5A+ac4YPmnoW8jurzo5z5 Q5TDBFRaOTyRgUxYt+PKv01S9tTiHgkxHoBzPQF7Z2TuRNKXoVQeXiUzW/s= -----END CERTIFICATE----- [root@dev-k8s-01 krew]# kubectl krew --help #查看krew的支持选项   ","description":"","id":52,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/kubernetes%E4%B8%93%E8%BE%91-02kubernetes%E7%82%BC%E6%B0%94%E6%9C%9F%E4%B9%8B%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/"},{"content":"k8s所有的配置文件以及资源的定义配置文件都是基于YAML格式的\nYAML的设计目标 使用 YAML 文件的优点：\n YAML 文件可读性较高，易于实现和使用。 可在现有的所有编程语言之间轻松移植。 与敏捷语言的原生数据结构相匹配。 YAML 文件具有一致模型，支持通用工具。 YAML 文件支持 One-pass 处理。 使用方便，因此您无需再将所有的参数添加到命令行中。 易于维护 – 可以将 YAML 文件添加到源控件中以跟踪更改。 灵活便捷 – 可以使用 YAML 创建更加复杂的结构（相对于使用命令行可以创建的结构）  YAML可以用来做什么  当数据能够容易的被读懂的时候，任何事情都会变得简单。\n  配置文件configuration files 日志文件log files 进程间消息传递interprocess messaging 跨语言数据共享cross-language data sharing 对象持久性object persistence 复杂数据结构debugging of complex data structures  与 JSON 和 XML 的关系   XML 是许多领域的优先采用格式。XML 最初设计为与标准通用标记语言 (SGML) 向后兼容，后者旨在支持结构化文档。因此，XML 存在许多设计上的约束。\n  JSON 的设计理念是简单性和通用性，并且易于生成和解析。JSON 格式的可读性低，但是这种格式的数据每一种现代编程环境都可以轻松处理。\n  YAML 的设计目标是提升可读性，提供更加完善的信息模型。YAML 的生成和解析更加复杂，因此可以将其视为 JSON 的自然超集。每个 JSON 文件都是一个有效的 YAML 文件。\n  在需要额外功能的情况下，可以轻松地从 JSON 迁移到 YAML。YAML 是从 XML 衍生而来。\nyaml工具   在线yaml格式校验\n  命令行校验\n1  python -c \u0026#39;import yaml, sys; yaml.safe_load(sys.stdin)\u0026#39; \u0026lt; cfg.yaml     Yaml高亮显示\n  在线获取yaml的值: shyaml\n  ","description":"","id":53,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/kubernetes%E4%B8%93%E8%BE%91-03kubernetes%E5%AE%9A%E4%B9%89%E6%B8%85%E5%8D%95yaml%E6%96%87%E4%BB%B6/"},{"content":"Nginx编译安装的前期准备 Nginx的编译安装一般用于需要加入一些扩展的第三方模块或者自研发的扩展功能包，所以在编译之前需要提前下载准备好这些扩展的包文件，比如我们编译的时候增加需要使用jemalloc优化nginx内存，使用openssl实现nginx的https数据加密，使用zlib库扩展nginx压缩的功能等，下面我们下载一些我们需要的基础依赖库的包文件:\n1 2 3 4 5 6 7 8 9 10 11 12 13  cd /usr/local/src wget http://nginx.org/download/nginx-1.14.2.tar.gz wget http://nginx.org/download/nginx-1.10.3.tar.gz wget https://www.zlib.net/fossils/zlib-1.2.8.tar.gz wget https://openssl.org/source/openssl-1.0.2d.tar.gz wget https://github.com/simplresty/ngx_devel_kit/archive/v0.3.0.tar.gz -O ngx_devel_kit.tar.gz wget https://github.com/jemalloc/jemalloc/releases/download/4.5.0/jemalloc-4.5.0.tar.bz2 wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz -O nginx_upstream_check_module.tar.gz wget https://github.com/openresty/lua-nginx-module/archive/v0.10.10.tar.gz -O lua-nginx-module.tar.gz tar xf jemalloc-4.5.0.tar.bz2 cd jemalloc-4.5.0 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install apt-get install -y lua5.1-0-dev sudo apt-get install libpcre3 libpcre3-dev # support pcre   jemalloc的安装也可以直接使用apt安装\n1  apt-get install -y libjemalloc-dev   如果在编译的过程中出现一些下面类似的错误，可以通过安装开发者工具包解决这个问题:\n1 2 3 4 5 6 7  checking for xsltproc... false checking for gcc... no checking for cc... no checking for cl.exe... no # ubuntu上安装开发者工具包 sudo apt install build-essential -y   编译安装Nginx1.18.0 首先我们准备一个nginx的运行用户www\n1 2 3  groupadd www \u0026amp;\u0026amp; useradd www -g www # 开始编译nginx ./configure --prefix=/webserver/nginx18 --user=www --group=www --with-pcre --with-zlib=/root/workspace/packages/nginx/zlib-1.2.8 --with-openssl=/root/workspace/packages/nginx/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/root/workspace/packages/nginx/ngx_devel_kit-0.3.0 --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/root/workspace/packages/nginx/nginx_upstream_check_module-0.3.0 \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install   编译完成后，在启动nginx\n1 2 3 4 5 6 7 8 9 10  /webserver/nginx18/sbin/nginx # 然后我们将nginx的work进程设置为2个，修改用户为www root@master:/webserver/nginx18/sbin# ps -fe |grep nginx root 50131 1 0 08:43 ? 00:00:00 nginx: master process ./nginx www 58005 50131 0 08:54 ? 00:00:00 nginx: worker process www 58006 50131 0 08:54 ? 00:00:00 nginx: worker process root@master:/webserver/nginx18/sbin# cp nginx /usr/local/bin/nginx root@master:/webserver/nginx18/sbin# nginx -t root@master:/webserver/nginx18/sbin# nginx -s reload   然后我们配置一个web.devopsman.cn的域名到192.168.99.128上，看一下服务运行和DNS解析后的效果:\n1 2 3 4 5 6 7 8 9 10  root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# curl -I web.devopsman.cn HTTP/1.1 200 OK Server: nginx/1.18.0 Date: Sat, 11 Jul 2020 13:03:29 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Sat, 11 Jul 2020 12:38:29 GMT Connection: keep-alive ETag: \u0026#34;5f09b2c5-264\u0026#34; Accept-Ranges: bytes   高亮Nginx的配置 nginx的源码包里面有配置vim编辑的nginx配置文件时内容格式高亮显示的配置文件，直接将其拷贝到.vim目录下即可\n1 2 3 4 5 6 7 8 9 10  root@master:~# cd /root/workspace/packages/nginx/nginx-1.18.0/contrib/ root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# ls -al total 24 drwxr-xr-x 4 www www 4096 Jul 11 08:02 . drwxr-xr-x 9 www www 4096 Jul 11 08:21 .. -rw-r--r-- 1 www www 1272 Apr 21 10:09 geo2nginx.pl -rw-r--r-- 1 www www 543 Apr 21 10:09 README drwxr-xr-x 2 www www 4096 Jul 11 08:02 unicode2nginx drwxr-xr-x 6 www www 4096 Apr 21 10:09 vim root@master:~/workspace/packages/nginx/nginx-1.18.0/contrib# cp -r vim/* ~/.vim/   拷贝进去之后，我们发现编辑nginx的配置文件变得高亮了\n","description":"","id":56,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/nginx%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E4%B8%93%E8%BE%91/02-nginx1.18.0%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"},{"content":"编译安装 NGINX1.10 相关依赖包的下载链接 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  cd /usr/local/src wget http://nginx.org/download/nginx-1.14.2.tar.gz wget http://nginx.org/download/nginx-1.10.3.tar.gz wget https://www.zlib.net/fossils/zlib-1.2.8.tar.gz wget https://openssl.org/source/openssl-1.0.2d.tar.gz wget https://github.com/simplresty/ngx_devel_kit/archive/v0.3.0.tar.gz -O ngx_devel_kit.tar.gz wget https://github.com/jemalloc/jemalloc/releases/download/4.5.0/jemalloc-4.5.0.tar.bz2 wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz -O nginx_upstream_check_module.tar.gz wget https://github.com/openresty/lua-nginx-module/archive/v0.10.10.tar.gz -O lua-nginx-module.tar.gz tar xf jemalloc-4.5.0.tar.bz2 cd jemalloc-4.5.0 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install apt-get install -y lua5.1-0-dev # apt-get install -y libjemalloc-dev 如果上面没有贬义安装jemalloc,可以使用apt进行安装 groupadd www useradd www   编译安装 Nginx 编译安装 1.10.3 1 2  tar xf nginx-1.10.3.tar.gz \u0026amp;\u0026amp; cd nginx-1.10.3 ./configure --prefix=/webserver/nginx --user=www --group=www --with-pcre --with-zlib=/usr/local/src/zlib-1.2.8 --with-openssl=/usr/local/src/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/usr/local/src/ngx_devel_kit --add-module=/usr/local/src/lua-nginx-module --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/usr/local/src/nginx_upstream_check_module \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install   编译安装 1.14.2 1 2  tar xf nginx-1.14.2.tar.gz \u0026amp;\u0026amp; cd nginx-1.14.2 ./configure --prefix=/webserver/nginx14 --user=www --group=www --with-pcre --with-zlib=/usr/local/src/zlib-1.2.8 --with-openssl=/usr/local/src/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/usr/local/src/ngx_devel_kit --add-module=/usr/local/src/lua-nginx-module --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/usr/local/src/nginx_upstream_check_module \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install   nginx 热升级  首先启动 nginx1.10.3  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  \u0026gt; root@cluster-node1:/webserver/nginx/sbin# ls nginx \u0026gt; root@cluster-node1:/webserver/nginx/sbin# pwd /webserver/nginx/sbin \u0026gt; root@cluster-node1:/webserver/nginx/sbin# ./nginx \u0026gt; root@cluster-node1:/webserver/nginx/sbin# ps -ef |grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx www 44683 44682 0 11:27 ? 00:00:00 nginx: worker process root 44685 6939 0 11:27 pts/1 00:00:00 grep --color=auto nginx \u0026gt; root@cluster-node1:/webserver/nginx/sbin# netstat -tunlp |grep nginx tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 44682/nginx \u0026gt; root@cluster-node1:/webserver/nginx/sbin# ./nginx -v nginx version: nginx/1.10.3 \u0026gt; root@cluster-node1:/webserver/nginx/sbin# ./nginx -V nginx version: nginx/1.10.3 built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11) built with OpenSSL 1.0.2d 9 Jul 2015 TLS SNI support enabled configure arguments: --prefix=/webserver/nginx --user=www --group=www --with-pcre --with-zlib=/usr/local/src/zlib-1.2.8 --with-openssl=/usr/local/src/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/usr/local/src/ngx_devel_kit --add-module=/usr/local/src/lua-nginx-module --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/usr/local/src/nginx_upstream_check_module \u0026gt; root@cluster-node1:/webserver/nginx/sbin# curl -I http://localhost HTTP/1.1 200 OK Server: nginx/1.10.3 Date: Thu, 01 Aug 2019 15:29:08 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Thu, 01 Aug 2019 15:12:52 GMT Connection: keep-alive ETag: \u0026#34;5d430174-264\u0026#34; Accept-Ranges: bytes   备份 nginx1.10.3 的二进制文件\n1 2 3 4 5  root@cluster-node1:/webserver/nginx/sbin# cp ./nginx ./nginx_1.10.3 root@cluster-node1:/webserver/nginx/sbin# ls -l total 24840 -rwxr-xr-x 1 root root 12715840 Aug 1 11:12 nginx -rwxr-xr-x 1 root root 12715840 Aug 1 11:30 nginx_1.10.3   复制 nginx 1.14.2 版本的 nginx 二进制文件到当前的目录下覆盖 nginx1.10.3 的二进制文件\n1 2 3 4 5  root@cluster-node1:/webserver/nginx/sbin# cp -f /webserver/nginx14/sbin/nginx ./ root@cluster-node1:/webserver/nginx/sbin# ls -l total 25680 -rwxr-xr-x 1 root root 13574704 Aug 1 11:31 nginx -rwxr-xr-x 1 root root 12715840 Aug 1 11:30 nginx_1.10.3   发送热更新信号\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  root@cluster-node1:/webserver/nginx/sbin# ps -ef | grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx www 44683 44682 0 11:27 ? 00:00:00 nginx: worker process root 44709 6939 0 11:33 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# kill -USR2 44682 # 发送一个平滑升级的信号给nginx master root@cluster-node1:/webserver/nginx/sbin# ps -ef | grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx # nginx.old www 44683 44682 0 11:27 ? 00:00:00 nginx: worker process root 44710 44682 0 11:34 ? 00:00:00 nginx: master process ./nginx # nginx.new www 44711 44710 0 11:34 ? 00:00:00 nginx: worker process root 44713 6939 0 11:34 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# curl -I http://localhost # 此时请求都打到nginx.new上了,nginx.old也不会在监听80/443断口了 HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Thu, 01 Aug 2019 15:34:41 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Thu, 01 Aug 2019 15:25:32 GMT Connection: keep-alive ETag: \u0026#34;5d43046c-264\u0026#34; Accept-Ranges: bytes   下线旧版本的 nginx 的 work 进程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  root@cluster-node1:/webserver/nginx/sbin# kill -WINCH 44682 root@cluster-node1:/webserver/nginx/sbin# ps -ef | grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx # nginx.old的处理流量请求的work进行全被下掉了，保留的nginx.old的master进程用于版本回退,如果要切回就的版本，只需使用旧的nginx二进制文件执行nginx -s reload即可. root 44710 44682 0 11:34 ? 00:00:00 nginx: master process ./nginx www 44711 44710 0 11:34 ? 00:00:00 nginx: worker process root 44757 6939 0 11:36 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# ./nginx -v nginx version: nginx/1.14.2 root@cluster-node1:/webserver/nginx/sbin# ./nginx -V nginx version: nginx/1.14.2 built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11) built with OpenSSL 1.0.2d 9 Jul 2015 TLS SNI support enabled configure arguments: --prefix=/webserver/nginx14 --user=www --group=www --with-pcre --with-zlib=/usr/local/src/zlib-1.2.8 --with-openssl=/usr/local/src/openssl-1.0.2d --with-http_gzip_static_module --with-http_realip_module --with-http_stub_status_module --add-module=/usr/local/src/ngx_devel_kit --add-module=/usr/local/src/lua-nginx-module --with-ld-opt=-ljemalloc --with-stream --with-http_ssl_module --add-module=/usr/local/src/nginx_upstream_check_module   在退出 nginx 的时候，nginx 旧版本的进程依旧存在，但是会在占用 80 端口，造成从新启动 nginx 失败，因此需要先杀掉 80 端口的旧版本的进程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  root@cluster-node1:/webserver/nginx/sbin# ./nginx -s quit root@cluster-node1:/webserver/nginx/sbin# ps -ef |grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx www 44783 44682 0 11:38 ? 00:00:00 nginx: worker process root 44785 6939 0 11:38 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# ./nginx -v nginx version: nginx/1.14.2 root@cluster-node1:/webserver/nginx/sbin# ps -ef |grep nginx root 44682 1 0 11:27 ? 00:00:00 nginx: master process ./nginx www 44783 44682 0 11:38 ? 00:00:00 nginx: worker process root 44788 6939 0 11:38 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# ./nginx -s reload nginx: [error] open() \u0026#34;/webserver/nginx14/logs/nginx.pid\u0026#34; failed (2: No such file or directory) root@cluster-node1:/webserver/nginx/sbin# ./nginx nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) nginx: [emerg] still could not bind() root@cluster-node1:/webserver/nginx/sbin# ./nginx -s stop nginx: [error] open() \u0026#34;/webserver/nginx14/logs/nginx.pid\u0026#34; failed (2: No such file or directory) root@cluster-node1:/webserver/nginx/sbin# ls -l /webserver/nginx14/logs/nginx.pid ls: cannot access \u0026#39;/webserver/nginx14/logs/nginx.pid\u0026#39;: No such file or directory root@cluster-node1:/webserver/nginx/sbin# pkill nginx root@cluster-node1:/webserver/nginx/sbin# ./nginx root@cluster-node1:/webserver/nginx/sbin# ps -ef |grep nginx root 44800 1 0 11:39 ? 00:00:00 nginx: master process ./nginx www 44801 44800 0 11:39 ? 00:00:00 nginx: worker process root 44803 6939 0 11:40 pts/1 00:00:00 grep --color=auto nginx root@cluster-node1:/webserver/nginx/sbin# ./nginx -v nginx version: nginx/1.14.2   nginx日志切割 1 2  kill -USR1 $NGINX_PID # 或者使用nginx的reopen选项 nginx -s reopen   ","description":"","id":57,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/nginx%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E4%B8%93%E8%BE%91/03-nginx%E7%9A%84%E7%83%AD%E5%8D%87%E7%BA%A7%E4%B8%8E%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2/"},{"content":"缓存服务器场景 配置一个缓存服务器，带来的优缺点，可以根据不同的场景适当选择：\n 当上游upstream端的服务器的响应内容发生了变化的时候，缓存服务器响应的内容在失效时间之前是不会变化的，因此要确认是否适用当前的场景 配置缓存代理服务器，可以提高网站的访问性能  配置实践 在http段配置缓存的相关参数，如缓存的存储位置、缓存级别、缓存空间、缓存存储最大多少，缓存有效时常\n1 2  # http proxy_cache_path /tmp/nginxcache levels=1:2 keys_zone=my_cache:10m max_size=10g inactive=60m use_temp_path=off;   配置缓存服务器\nupstream local { server 127.0.0.1:8089; } server { server_name proxy.devopsman.cn; listen 80; location / { proxy_set_header Host $host; proxy_set_header X-Real_IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;; proxy_cache my_cache; proxy_cache_key $host$uri$is_args$args; proxy_cache_valid 200 304 302 1d; proxy_pass http://local; } } ","description":"","id":58,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/nginx%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E4%B8%93%E8%BE%91/05-nginx%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%BC%93%E5%AD%98%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"content":"nginx问题记录   ./configure: error: ngx_http_lua_module requires the Lua library.\n1 2 3 4 5  checking for Lua library in /usr/ ... not found checking for LuaJIT library in /usr/local/ ... not found checking for LuaJIT library in /usr/ ... not found checking for LuaJIT library in /usr/ ... not found ./configure: error: ngx_http_lua_module requires the Lua library.   解决方案:\n1  apt-get install lua5.1-0-dev     --with-ld-opt=\u0026quot;-ljemalloc\u0026quot; ... not found\n1 2 3 4 5 6 7 8  checking for OS + Linux 4.4.0-142-generic x86_64 checking for C compiler ... found + using GNU C compiler + gcc version: 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11) checking for gcc -pipe switch ... found checking for --with-ld-opt=\u0026#34;-ljemalloc\u0026#34; ... not found ./configure: error: the invalid value in --with-ld-opt=\u0026#34;-ljemalloc\u0026#34;   解决办法:\n1 2 3  wget https://github.com/jemalloc/jemalloc/releases/download/4.5.0/jemalloc-4.5.0.tar.bz2 tar xf jemalloc-4.5.0.tar.bz2 cd jemalloc-4.5.0 \u0026amp;\u0026amp; ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install     ","description":"","id":59,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/nginx%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E4%B8%93%E8%BE%91/100-nginx%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"},{"content":"从苦逼到牛逼，详解DevOps工程师的打怪升级之路 从进入职场做运维到写点代码，慢慢的快四年了，就像玩消消乐一样，一关又一关的走了过来(你猜猜我的消消乐多少关了？哈哈)，在过五关斩六将(不断的背锅救火)之后，感觉自己的知识体系和运维体系变化的挺大的，学到了很多的新知识今天和大家分享分享。\n","description":"","id":60,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/%E4%BB%8E%E8%8B%A6%E9%80%BC%E5%88%B0%E7%89%9B%E9%80%BC%E8%AF%A6%E8%A7%A3devops%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E6%89%93%E6%80%AA%E5%8D%87%E7%BA%A7%E4%B9%8B%E8%B7%AF/"},{"content":"微信公众号文章链接 运维架构专辑  2020-06-20 OpenVPN高级进阶: 5个使用场景和问题集 2020-06-20 OpenVPN最难忘的一次”灾后重建“ Prometheus监控系列-部署篇  Python专辑  2020-02-25 当运维遇上了\u0026hellip;  Kubernetes  2002-03-22 写给孩子看的Kubernetes动画指南  2020-03-07 kubernetes的ingress控制器比较(traefik2.0.5安装指南) 2020-03-04 kubernetes深度探究Node和Pod的亲和性和反亲和性 2020-02-26 kubernetes安装方案大全 2019-12-18 kubernetes最常用的资源对象Deployment 2020-06-12 kubernetes炼气期之掌握kuebernetes背景 2020-06-12 kubernetes炼气期之k8s平台快速搭建   DevOps技术栈  2020-03-05 DevOps全开源端到端部署流水线视频   Travis  2020-06-05 GitHub+Travis+Mkdocs自动化构建文档库  Jenkins  2020-03-21 Jenkins流水线动画 2020-06-08 自定义Build History中显示构建信息 2020-03-01 自定义构建Jenkins镜像实战 2020-06-08 Jenkins在kubernetes上的初体验 2020-06-08 Jenkins动态Slave在k8s上的实践  Ansible  2020-02-16 Ansible书籍分享(ansible for devops/ansible for kubernetes)   GitLab  2020-02-16 Gitlab Runner系列-环境部署篇 2020-02-16 Gitlab Runner系列-持续集成篇 2020-02-16 Gitlab Runner系列-持续部署篇 2020-06-06 用GitLAB CICD pipeline template持续集成   Prometheus  2020-03-20 Prometheus监控系列-部署篇 2020-03-20 Prometheus监控系列-监控篇(consul) 2020-03-20 Awesome Alertmanager Rule  Develop Go Python  2020-02-21 Python原来如此美丽之Requeat \u0026amp; Parsel  linux  2020-02-28 企业级规范部署中央认证软件Openldap 2020-02-24 域名有效期监控 2020-03-14 Awk权威指南之终结篇 2020-03-11 Awk权威指南 2020-06-13 打造Mac下高颜值好用的终端环境  NoSQL Mongodb Elasticsearch  2020-03-28 Elasticsearch-5.2.2集群升级 ==待准备==  middleware Message Queue RabbitMQ  2020-04-10 RabbitMQ-3.6.6集群升级 ==待准备==  Kafka  2020-04-20 Kafka-2.11-0.10.1.1集群升级 ==待准备==  Redis  2020-03-30 Redis-3.2.8集群升级 ==待准备==  Service Discovery Consul Zookeeper  2020-04-30 zookeeper-3.4.10集群升级 ==待准备==  Etcd Database MySQL Container Runtime Docker  2020-03-15 Docker镜像分析工具之Dive介绍 2020-03-15 Docker镜像分析工具之Dive视频指南   2020-06-15 Lazydocker:专为\u0026quot;懒人\u0026quot;设计命令行可视化工具  tools  2020-03-13 IDE你要的永远激活在这里  ","description":"","id":61,"section":"posts","tags":null,"title":"","uri":"https://linuxermaster.github.io/en/posts/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E6%96%87%E7%AB%A0%E9%93%BE%E6%8E%A5/"}]